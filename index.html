<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"extrali.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Extrali">
<meta property="og:url" content="http://extrali.com/index.html">
<meta property="og:site_name" content="Extrali">
<meta property="article:author" content="黎达">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://extrali.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Extrali</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Navigationsleiste an/ausschalten">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Extrali</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Startseite</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archiv</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://extrali.com/2021/11/12/clickhouse/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="黎达">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Extrali">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/12/clickhouse/" class="post-title-link" itemprop="url">clickhouse</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>

              <time title="Erstellt: 2021-11-12 22:44:01" itemprop="dateCreated datePublished" datetime="2021-11-12T22:44:01+08:00">2021-11-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Bearbeitet am</span>
                <time title="Geändert am: 2021-11-14 17:32:06" itemprop="dateModified" datetime="2021-11-14T17:32:06+08:00">2021-11-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">in</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/clickhouse/" itemprop="url" rel="index"><span itemprop="name">clickhouse</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>refer: &lt;&lt;Clickhouse原理解析及应用实践&gt;&gt;</p>
</blockquote>
<h1 id="1-Clickhouse的前世今生"><a href="#1-Clickhouse的前世今生" class="headerlink" title="1. Clickhouse的前世今生"></a>1. Clickhouse的前世今生</h1><p>OLAP常见架构分类：</p>
<ol>
<li><p><strong>ROLAP</strong>（Relational OLAP, 关系型OLAP）。直接使用关系模型构建，数据模型常使用<strong>星型模型或则雪花模型</strong>。</p>
<ul>
<li><p>星型模是一种多维的数据关系，它由<strong>一个fact表</strong>和<strong>一组dimension表</strong>组成。每个dimension表都有一个维作为主键，所有这些dimension的主键组合成fact表的主键。强调的是对维度进行预处理，将多个维度集合到一个fact表，形成一个宽表。这也是我们在使用hive时，经常会看到一些大宽表的原因，大宽表一般都是fact表，包含了维度关联的主键和一些度量信息，而dimension表则是事实表里面维度的具体信息，使用时候一般通过join来组合数据，相对来说对OLAP的分析比较方便。</p>
<p><img src="https://i.loli.net/2021/11/12/3YL8i12oFf9ae7t.png" alt="image-20211112225815162"></p>
</li>
<li><p>当有一个或多个dimension表没有直接连接到fact表上，而是通过其他dimension表连接到fact表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各dimension表可能被扩展为小的fact表，形成一些局部的 “层次 “ 区域，这些被分解的表都连接到主dimension表而不是fact表。雪花模型更加符合数据库范式，减少数据冗余，但是在分析数据的时候，操作比较复杂，需要join的表比较多所以其性能并不一定比星型模型高。</p>
<p><img src="https://i.loli.net/2021/11/12/1cjWOKhoSzb6DM7.png" alt="image-20211112230731449"></p>
</li>
</ul>
</li>
<li><p><strong>MOLAP</strong>（Multidimensional OLAP，多维型OLAP）MOLAP使用多维数组的形式保存数据，其核心思想是借助预先聚合结果，使用空间换取时间的形式最终提升查询性能。<strong>缺点</strong>是维度预处理可能会导致数据的膨胀，如果数据立方体包括5个维度，那么维度的组合方式就是2^5^种。</p>
</li>
<li><p><strong>HOLAP</strong>（Hybrid OLAP，混合架构的OLAP），可以看成ROLAP和MOLAP两者的集成。</p>
</li>
</ol>
<h1 id="2-Clickhouse架构概述"><a href="#2-Clickhouse架构概述" class="headerlink" title="2. Clickhouse架构概述"></a>2. Clickhouse架构概述</h1><p>Clickhouse是一款<strong>MPP架构</strong>的<strong>列式存储数据库</strong></p>
<h2 id="2-1-Clickhouse核心特性"><a href="#2-1-Clickhouse核心特性" class="headerlink" title="2.1 Clickhouse核心特性"></a>2.1 Clickhouse核心特性</h2><ol>
<li><p><strong>列式存储与数据压缩</strong></p>
<p>按列存储和按行存储相比，前者可以有效减少查询时所需要扫描的数据量。假设一个数据表A用有50个字段A<del>1</del>A<del>50</del>，以及100行数据。现在需要查询前5个字段并进行数据分析，则可以使用如下SQL实现：</p>
<p><code>select A1, A2, A3, A4, A5 from A</code></p>
<p>如果按行存储，数据库首先会逐行扫描，并获取每行数据的所有50个字段，再从每一行数据种返回A<del>1</del>A<del>5</del>这5个字段。不难发现，尽管只需要前面5个字段，但由于数据是按行进行组织的，实际上还是扫描了所有的字段。如果数据按列存储，数据库就可以直接获取A<del>1</del>A<del>5</del>这5列的数据。</p>
<p>按列存储相比按行存储的另一个优势是对数据压缩的友好性。<strong>数据中的重复率越多，则压缩率越高；压缩率越高，则数据体量越小</strong>。数据按列组织，因为他们拥有相同的数据类型和现实语义，重复项的可能性自然就越高。</p>
</li>
<li><p><strong>向量化执行引擎</strong></p>
<p>为了实现向量化执行，需要利用CPU的<strong>SIMD</strong>指令。SIMD的全程是Single Instruction Mutiple Data，即<strong>使用单条指令操作多条数据</strong>。现代计算机系统概念种，它是通过数据并行以提升性能的一种实现方式，它的原理是在CPU寄存器层面实现数据的并行操作。</p>
</li>
<li><p><strong>关系模型与SQL查询</strong></p>
</li>
<li><p><strong>多样化的表引擎</strong></p>
</li>
<li><p><strong>多线程与分布式</strong></p>
<p>Clickhouse在大数据存取方面，既支持<strong>分区（纵向扩展，利用多线程原理）</strong>，也支持<strong>分片（横向扩展，利用分布式原理）</strong>。</p>
</li>
<li><p><strong>多主架构</strong></p>
<p>Clickhouse采用Multi-Master多主架构，集群中的每个角色对等，客户端访问任意一个节点都能得到同样的结果。</p>
</li>
<li><p><strong>在线查询</strong></p>
<p><strong>速度快</strong></p>
</li>
<li><p><strong>数据分片与分布式查询</strong></p>
<p>数据分片是将数据进行<strong>横向切分</strong>。Clickhouse支持分片，而分片依赖于集群，每个集群由1到多个分片组成，而<strong>每个分片则对应了Clickhouse的1个服务节点</strong>。分片的数量上限取决于节点数量（1个分片只能对应一个服务节点）。</p>
<p>Clickhouse提供了<strong>本地表（Local Table）</strong>与<strong>分布式表（Distributed Table）</strong>的概念。<strong>一张Local Table等同于一份数据的分片。而Distributed Table本身不存储任何数据，它是本地表的访问代理</strong>，其作用类似于分库中间件。借助分布式表，能够代理多个数据分片，从而实现分布式查询。</p>
</li>
</ol>
<h2 id="2-2-Clickhouse的架构设计"><a href="#2-2-Clickhouse的架构设计" class="headerlink" title="2.2 Clickhouse的架构设计"></a>2.2 Clickhouse的架构设计</h2><ol>
<li><p><strong>Column与Field</strong></p>
<p>Clickhouse按列存储数据，内存中的<strong>一列数据</strong>由一个<strong>Column对象</strong>表示。如果需要操作单个具体的值（也就是单列中的一行数据），则需要使用Field对象，<strong>Field对象代表一个单值</strong>。</p>
</li>
<li><p><strong>DataType</strong></p>
<p>数据的序列化和反序列化工作由DataType负责。</p>
</li>
<li><p><strong>Block与Block流</strong></p>
<p>Clickhouse内部的数据操作是面向<strong>Block对象</strong>进行的。虽然Column和Field组成了数据的基本映射单元，但对应到实际操作，它们还缺少了一些必要的信息，比如数据的类型及列的名称。Block对象的本质是由<strong>数据对象、数据类型和列名称</strong>组成的三元组，即Column、DataType及列名称字符串。Column提供了数据的读取功能，而DataType知道如何正反序列化。</p>
</li>
<li><p><strong>Table</strong></p>
<p>直接使用<strong>IStorage接口</strong>指代数据表。</p>
</li>
<li><p><strong>Parser与Interpreter</strong></p>
<p><strong>Parser分析器</strong>负责创建AST对象；而<strong>Interpreter解释器</strong>则负责解释AST，并进一步创建查询的执行管道。</p>
</li>
<li><p><strong>Functions与Aggregate Functions</strong></p>
<p>Clickhouse主要提供两类函数——<strong>普通函数</strong>和<strong>聚合函数</strong>。普通函数由IFunction接口定义。聚合函数由IAggregateFunction接口定义。</p>
</li>
<li><p><strong>Cluster与Replication</strong></p>
<p>Clickhouse的集群由<strong>分片（Shard）</strong>组成，而每个分片又通过副本（Replica）组成。</p>
<ul>
<li>ClickHouse的1个节点只能拥有1个分片，也就是说如果要实现1分片、1副本，则至少需要部署2个服务节点。</li>
<li>分片只是逻辑概念，其物理承载还是由副本承担的。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1分片、1副本的集群配置</span><br><span class="line"></span><br><span class="line">&lt;ch_cluster&gt;			&#x2F;&#x2F;集群名字</span><br><span class="line">	&lt;shard&gt;</span><br><span class="line">		&lt;replica&gt;</span><br><span class="line">			&lt;host&gt;10.37.129.6&lt;&#x2F;host&gt;</span><br><span class="line">			&lt;port&gt;9000&lt;&#x2F;port&gt;</span><br><span class="line">        &lt;&#x2F;replica&gt;</span><br><span class="line">        &lt;replica&gt;</span><br><span class="line">			&lt;host&gt;10.37.129.7&lt;&#x2F;host&gt;</span><br><span class="line">			&lt;port&gt;9000&lt;&#x2F;port&gt;</span><br><span class="line">        &lt;&#x2F;replica&gt;</span><br><span class="line">    &lt;&#x2F;shard&gt;</span><br><span class="line">&lt;&#x2F;ch_cluster&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h1 id="3-安装与部署"><a href="#3-安装与部署" class="headerlink" title="3. 安装与部署"></a>3. 安装与部署</h1><h2 id="3-1-目录结构"><a href="#3-1-目录结构" class="headerlink" title="3.1 目录结构"></a>3.1 目录结构</h2><p><strong>核心目录</strong>部分：</p>
<ol>
<li><strong>/etc/clickhouse-server</strong>：服务端的配置文件目录，包括全局配置config.xml和用户配置users.xml等。</li>
<li><strong>/var/lib/clickhouse</strong>：默认的数据存储目录（通常会修改默认路径配置，将数据保存到大容量磁盘挂载的路径）</li>
<li><strong>/var/log/clickhouse-server</strong>：默认保存日志的目录</li>
</ol>
<p>在/usr/bin路径下的可执行文件：</p>
<ol>
<li>clickhouse：主程序执行文件。</li>
<li>clickhouse-client：一个指向Clickhouse可执行文件的软链接，共客户端连接使用。</li>
<li>clickhouse-server：一个指向Clickhouse可执行文件的软链接，共服务端连接使用。</li>
<li>clickhouse-compressor：内置提供的压缩文具，可用于数据的正压反解。</li>
</ol>
<h2 id="3-2-客户端的访问接口"><a href="#3-2-客户端的访问接口" class="headerlink" title="3.2 客户端的访问接口"></a>3.2 客户端的访问接口</h2><p>ClickHouse的底层访问接口支持<strong>TCP和HTTP两种协议</strong>，其中TCP协议具有更好的性能，其默认端口为<strong>9000</strong>，主要用于集群间内部通信及CLI客户端；而HTTP协议则拥有更好的兼容性，可以通过REST服务的形式被广泛用于JAVA、Python等编程语言的客户端，其默认端口为<strong>8123</strong>.</p>
<ol>
<li><p><strong>CLI</strong></p>
<ul>
<li><p>交互式执行：<code>clickhouse-client</code></p>
</li>
<li><p>非交互式执行：<code>clickhouse-client --query</code>。如果需要执行多次查询，可以追加<code>--multiquery</code>参数，多条查询语句之间用分号分隔。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clickhouse-client --multiquery --query&#x3D;&quot;SELECT 1;SELECT 2;SELECT 3;&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>重要参数：</p>
<ol>
<li><code>--host/-h</code>：服务端的地址，默认值为localhost。</li>
<li><code>--port</code>：服务端的TCP端口，默认值为9000。</li>
<li><code>--user/-u</code>：登录的用户名。</li>
<li><code>--password</code>：登录的密码。</li>
<li><code>--database/-d</code>：登录的数据库，默认值为default。</li>
<li><code>--query/-q</code>：非交互方式。</li>
<li><code>--multiquery/-n</code></li>
<li><code>--time/-t</code>：在非交互式执行时，会打印每条SQL的执行时间。</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>JDBC</strong></p>
<p>使用下面的Maven依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;ru.yandex.clickhouse&lt;&#x2F;groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;clickhouse-jdbc&lt;&#x2F;artifactId&gt;</span><br><span class="line">	&lt;version&gt;0.2.4&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<ol>
<li><p><strong>标准形式</strong></p>
<ul>
<li><p><strong>JDBC Driver Class</strong>为<code>ru.yandex.clickhouse.ClickHouseDriver</code></p>
</li>
<li><p><strong>JDBC URL</strong>为<code>jdbc:clickhouse://&lt;host&gt;:&lt;port&gt;[/&lt;database&gt;]</code></p>
</li>
</ul>
</li>
<li><p><strong>高可用模式</strong></p>
<p>高可用模式允许设置多个host地址，每次会从可用的地址中随机选择一个进行连接，其URL声明格式如下：</p>
<p><code>jdbc:clickhouse://&lt;first-host&gt;:&lt;port&gt;,&lt;second-host&gt;:&lt;port&gt;[/&lt;database&gt;]</code></p>
<p>在高可用模式下，需要通过<code>BalancedClickhouseDataSource</code>对象获取连接。</p>
</li>
</ol>
</li>
</ol>
<h2 id="3-3-内置的实用工具"><a href="#3-3-内置的实用工具" class="headerlink" title="3.3 内置的实用工具"></a>3.3 内置的实用工具</h2><ol>
<li><p><strong>clickhouse-local</strong></p>
</li>
<li><p><strong>clickhouse-benchmark</strong></p>
<p>基准测试的小工具，可以自动运行SQL查询，并生成相应的运行指标报告。</p>
<p><code>echo &quot;select * from system.numbers limit 100&quot; | clickhouse-benchmark -i 5</code></p>
<p>核心参数：</p>
<ol>
<li><p><code>-i/--iterations</code>：SQL查询执行的次数，默认值为0.</p>
</li>
<li><p><code>-c/--concurrency</code>：同时执行查询的并发数，默认值为1.</p>
</li>
<li><p><code>-h/--host</code>：服务端地址，默认值为localhost。clickhouse-benchmark支持对比测试，此时需要通过此参数声明两个服务端的地址。</p>
<p><code>echo &quot;select * from system.numbers limit 100&quot; | clickhouse-benchmark -i 5 -h localhost -h localhost</code></p>
</li>
</ol>
</li>
</ol>
<h1 id="4-数据定义"><a href="#4-数据定义" class="headerlink" title="4. 数据定义"></a>4. 数据定义</h1><h2 id="4-1-数据类型"><a href="#4-1-数据类型" class="headerlink" title="4.1 数据类型"></a>4.1 数据类型</h2><p>Clickhouse的数据类型可以划分为<strong>基础类型、复合类型和特殊类型</strong>。</p>
<h3 id="4-1-1-基础类型"><a href="#4-1-1-基础类型" class="headerlink" title="4.1.1 基础类型"></a>4.1.1 基础类型</h3><h4 id="1-数值类型"><a href="#1-数值类型" class="headerlink" title="1. 数值类型"></a>1. 数值类型</h4><p>数值类型分为<strong>整形，浮点数和定点数</strong>。</p>
<ol>
<li><p><strong>Int</strong></p>
<p>在普遍观念中，常用<strong>Tinyint、Smallint、Int和Bigint</strong>指代整数的不同取值范围。而Clickhouse中直接使用<strong>Int8、Int16、Int32和Int64</strong>指代4种大小的Int类型。</p>
<table>
<thead>
<tr>
<th align="center">名称</th>
<th align="center">大小（字节）</th>
<th align="center">普遍观念</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Int8</td>
<td align="center">1</td>
<td align="center">Tinyint</td>
</tr>
<tr>
<td align="center">Int16</td>
<td align="center">2</td>
<td align="center">Smallint</td>
</tr>
<tr>
<td align="center">Int32</td>
<td align="center">4</td>
<td align="center">Int</td>
</tr>
<tr>
<td align="center">Int64</td>
<td align="center">8</td>
<td align="center">Bigint</td>
</tr>
</tbody></table>
<p>Clickhouse支持无符号的整数，使用前缀U表示。UInt8、UInt16、UInt32和UInt64.</p>
</li>
<li><p><strong>Float</strong></p>
<p>与整数类似，Clickhouse直接使用Float32和Float64代表单精度浮点数及双精度浮点数。</p>
<table>
<thead>
<tr>
<th align="center">名称</th>
<th align="center">大小（字节）</th>
<th align="center">有效精度（位数）</th>
<th align="center">普遍概念</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Float32</td>
<td align="center">4</td>
<td align="center">7</td>
<td align="center">Float</td>
</tr>
<tr>
<td align="center">Float64</td>
<td align="center">8</td>
<td align="center">16</td>
<td align="center">Double</td>
</tr>
</tbody></table>
<p>Float32从小数点后第8位起及Float64从小数点后第17位起，都会产生数据溢出。</p>
</li>
<li><p><strong>Decimal</strong></p>
<p>Clickhouse提供了Decimal32、Decimal64和Decimal128三种精度的定点数。可以通过两种形式声明：</p>
<ul>
<li>简写方式位Decimal32(S)、Decimal64(S)、Decimal128(S)；</li>
<li>原生方式为Decimal(P,S)</li>
</ul>
<p>其中P代表精度，决定总位数（整数部分+小数部分），取值范围是1~38；</p>
<p>S代表规模，决定小数位数，取值范围是0~P</p>
</li>
</ol>
<h4 id="2-字符串类型"><a href="#2-字符串类型" class="headerlink" title="2. 字符串类型"></a>2. 字符串类型</h4><ol>
<li><p><strong>String</strong></p>
<p>长度不限。完全替代了传统意义上数据库的Varchar、Text、Clob和Blob等字符串类型。</p>
</li>
<li><p><strong>FixedString</strong></p>
<p>定长字符串通过<strong>FixedString(N)</strong>声明，其中<strong>N表示字符串长度</strong>。但与Char不同的是，<strong>FixedString使用null字节填充末尾字符</strong>，而Char通常使用空格填充。</p>
</li>
<li><p><strong>UUID</strong></p>
<p>UUID共有32位，它的格式位8-4-4-4-12.如果一个UUID类型的字段在写入数据时没有被赋值，则会依照格式使用0填充。</p>
</li>
</ol>
<h4 id="3-时间类型"><a href="#3-时间类型" class="headerlink" title="3. 时间类型"></a>3. 时间类型</h4><p>时间类型分为<strong>DateTime、DateTime64和Date</strong>三类。Clickhouse目前没有时间戳类型，时间类型的最高精度为秒。</p>
<ol>
<li><p><strong>DateTime</strong></p>
<p>DateTime类型包含时、分、秒信息，精确到秒，支持使用字符串形式写入。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE Datetime_TEST (</span><br><span class="line">	c1 DateTime</span><br><span class="line">) ENGINE &#x3D; Memory</span><br><span class="line">----以字符串形式写入</span><br><span class="line">INSERT INTO Datetime_TEST VALUES(&#39;2019-06-22 00:00:00&#39;)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>DateTime64</strong></p>
<p>DateTime64可以记录亚秒，它在DateTime之上增加了精度的设置。</p>
</li>
<li><p><strong>Date</strong></p>
<p>Date类型不包含具体的时间信息，只精确到天，它同样支持字符串形式写入。</p>
</li>
</ol>
<h3 id="4-1-2-复合类型"><a href="#4-1-2-复合类型" class="headerlink" title="4.1.2 复合类型"></a>4.1.2 复合类型</h3><p>Clickhouse还提供了<strong>数组、元组、枚举和嵌套</strong>四类复合类型。</p>
<ol>
<li><p><strong>Array</strong></p>
<p>数组有两种定义方式，常规方式array(T)：</p>
<p><code>select array(1, 2)</code></p>
<p>简写方式[T]：</p>
<p><code>select [1, 2]</code></p>
</li>
<li><p><strong>Tuple</strong></p>
<p>元组类型由1~n个元素组成，<strong>每个元素之间允许设置不同的数据类型，且彼此之间不要求兼容</strong>。</p>
<p><strong>常规方式tuple(T)，简写方式(T)</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE Tuple_TEST (</span><br><span class="line">	c1 Tuple(String, Int8)</span><br><span class="line">) ENGINE &#x3D; Memory;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Enum</strong></p>
<p>Clickhouse提供了<strong>Enum8</strong>和<strong>Enum16</strong>两种枚举类型。枚举类型固定使用<strong>（String：Int）Key/Value键值对</strong>的形式定义数据，所以Enum8和Enum16分别会对应（String：Int8）和（String：Int16）.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE Enum_TEST (</span><br><span class="line">	c1 Enum8(&#39;ready&#39; &#x3D; 1, &#39;start&#39; &#x3D; 2, &#39;success&#39; &#x3D; 3, &#39;error&#39; &#x3D; 4)</span><br><span class="line">) ENGINE &#x3D; Memory</span><br></pre></td></tr></table></figure>

<p><strong>Key和Value是不允许重复的，要保证唯一性</strong>，其次Key和Value的值都不能为null，但Key允许是空字符串。在写入枚举数据的时候，只会用到Key字符串部分。</p>
<p><code>INSERT INTO Enum_TEST VALUES(&#39;ready&#39;);</code></p>
</li>
<li><p><strong>Nested</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE nested_test (</span><br><span class="line">	name String,</span><br><span class="line">	age  UInt8,</span><br><span class="line">	dept Nested(</span><br><span class="line">		id    UInt8,</span><br><span class="line">		name  String,</span><br><span class="line">	)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>嵌套类型本质是一种多维数组结构。<strong>嵌套字段中的每个字段都是一个数组</strong>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO nested_test VALUES (&#39;bruce&#39;, 30, [10000, 10001, 10002], [&#39;研发部&#39;, [技术部], [&#39;测试部&#39;]]);</span><br></pre></td></tr></table></figure>

<p>同一行数据内每个数组的元素个数必须相等。访问嵌套类型得数据需要使用点符号<code>dept.id</code></p>
</li>
</ol>
<h3 id="4-1-3-特殊类型"><a href="#4-1-3-特殊类型" class="headerlink" title="4.1.3 特殊类型"></a>4.1.3 特殊类型</h3><ol>
<li><p><strong>Nullable</strong></p>
<p>Nullable更像是一种辅助的修饰符，需要<strong>与基础数据类型一起搭配使用</strong>。它表示某个数据类型可以是Null值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE Null_TEST (</span><br><span class="line">	c1 String,</span><br><span class="line">	c2 Nullable(UInt8)</span><br><span class="line">) ENGINE &#x3D; TinyLog;</span><br></pre></td></tr></table></figure>

<p>应该慎用Nullable类型，不然会使查询和写入性能变慢。因为在正常情况下，每个列字段的数据会被存储在对应的<strong>[Column].bin</strong>文件种。如果一个列字段被Nullable类型修饰后，会额外生成一个<strong>[Column].null.bin</strong>文件专门保存它的Null值。这意味着在读取和写入数据时，需要一倍的额外文件操作。</p>
</li>
<li><p><strong>Domain</strong></p>
<p>域名类型分为<strong>IPV4</strong>和<strong>IPV6</strong>两类。</p>
</li>
</ol>
<h2 id="4-2-定义数据表"><a href="#4-2-定义数据表" class="headerlink" title="4.2 定义数据表"></a>4.2 定义数据表</h2><ol>
<li><p><strong>数据库</strong></p>
<p>数据库目前一共支持5种引擎：</p>
<ol>
<li><strong>Ordinary</strong>：默认引擎。</li>
<li><strong>Dictionary</strong>：字典引擎。</li>
<li><strong>Memory</strong>：内存引擎。</li>
<li><strong>Lazy</strong>：日志引擎，此类数据库下只能使用Log系列的表引擎。</li>
<li><strong>MySQL：</strong>MYSQL引擎，此类数据库下会自动拉去远端MySQL种的数据。</li>
</ol>
</li>
<li><p><strong>默认值表达式</strong></p>
<p>表字段支持三种默认值表达式的定义方式，分别是<strong>DEFAULT</strong>、<strong>MATERIALIZED</strong>和<strong>ALIAS</strong></p>
</li>
<li><p><strong>临时表</strong></p>
<p>创建临时表的方法是在普通表的基础上添加<strong>TEMPORARY</strong>关键字</p>
<ul>
<li>临时表的<strong>生命周期是会话绑定的</strong>，所以它<strong>只支持Memory表引擎</strong>，如果会话结束，数据表就会被销毁。</li>
<li>临时表不属于任何数据库。</li>
</ul>
</li>
<li><p><strong>分区表</strong></p>
<p>数据分区（<strong>partition</strong>）和数据分片（<strong>shard</strong>）是完全不同的两个概念。数据分区是针对本地数据而言的，是数据的一种纵向切分。而数据分片是一种横向切分。目前只有<strong>MergeTree</strong>家族系列的表引擎才支持数据分区。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE partition_v1 (</span><br><span class="line">	ID  	  String,</span><br><span class="line">	URL 	  String,</span><br><span class="line">	EventTime Date,</span><br><span class="line">) ENGINE &#x3D; MergeTree()</span><br><span class="line">PARTITION BY toYYYYMM(EventTime)</span><br><span class="line">ORDER BY ID</span><br><span class="line"></span><br><span class="line">INSERT INTO partition_v1 VALUES</span><br><span class="line">(&#39;A000&#39;, &#39;www.baidu.com&#39;, &#39;2019-05-01&#39;),</span><br><span class="line">(&#39;A001&#39;, &#39;www.baidu.com&#39;, &#39;2019-06-02&#39;);</span><br><span class="line"></span><br><span class="line">通过system.parts系统表，查询数据表的分区状态：</span><br><span class="line">SELECT table, partition, path from system.parts where table &#x3D; &#39;partition_v1&#39;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th align="center">table</th>
<th align="center">partition</th>
<th align="center">path</th>
</tr>
</thead>
<tbody><tr>
<td align="center">partition_v1</td>
<td align="center">201905</td>
<td align="center">/chbase/data/default/partition_v1/201905_1_1_0/</td>
</tr>
<tr>
<td align="center">partition_v1</td>
<td align="center">201906</td>
<td align="center">/chbase/data/default/partition_v1/201906_2_2_0/</td>
</tr>
</tbody></table>
<p>可以看到每个分区都对应一个独立的文件目录。分区键不应该使用粒度过细的数据字段。</p>
</li>
<li><p><strong>视图</strong></p>
<p>Clickhouse拥有<strong>普通</strong>和<strong>物化</strong>两种视图，其中<strong>物化视图拥有独立的存储</strong>，而<strong>普通视图只是一层简单的查询代理</strong>。创建普通视图的完整语法如下所示：</p>
<p><code>CREATE VIEW [IF NOT EXISTS] [db_name.]view_name AS SELECT ...</code></p>
<p>物化视图支持表引擎，数据保存形式由它的表引擎决定，创建物化视图的完整语法如下所示：</p>
<p><code>CREATE [MATERIALIZED] VIEW [IF NOT EXISTS] [db.]table_name [TO[db.name]] [ENGINE = engine] [POPULATE] AS SELECT ...</code></p>
<p>物化视图创建好之后，如果源表被写入新数据，那么物化视图也会同步更新。<strong>POPULATE</strong>修饰符决定了物化视图的初始化策略：如果使用了POPULATE修饰符，那么在创建视图的过程中，会连带源表种已存在的数据一并导入。</p>
</li>
</ol>
<h2 id="4-3-数据分区的基本操作"><a href="#4-3-数据分区的基本操作" class="headerlink" title="4.3 数据分区的基本操作"></a>4.3 数据分区的基本操作</h2><ol>
<li><p><strong>查询分区信息</strong></p>
<p><strong>parts系统表</strong>专门用于查询数据表的分区信息。</p>
</li>
<li><p><strong>卸载与装载分区</strong></p>
<p>表分区可以通过<strong>DETACH</strong>语句卸载，分区卸载后，它的物理数据并没有被删除，而是转移到了当前数据表目录的detached子目录下。而装载分区则是反向操作，它能将detached子目录下的某个分区重新装载回去。</p>
<p><code>ALTER TABLE tb_name DETACH(ATTACH) PARTITION partition_expr</code></p>
</li>
</ol>
<h2 id="4-4-分布式DDL执行"><a href="#4-4-分布式DDL执行" class="headerlink" title="4.4 分布式DDL执行"></a>4.4 分布式DDL执行</h2><p>将一条普通的DDL语句转换成分布式执行十分简单，只需加上<code>ON CLUSTER cluster_name</code>。例如，执行下面的语句后将会对ch_cluster集群内的所有节点广播这条DDL语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE partition_v3 ON CLUSTER ch_cluster(</span><br><span class="line">	ID        String,</span><br><span class="line">	URL       String,</span><br><span class="line">	EventTime Date</span><br><span class="line">) ENGINE &#x3D; MergeTree()</span><br><span class="line">PARTITION BY toYYYYMM(EventTime)</span><br><span class="line">ORDER BY ID</span><br></pre></td></tr></table></figure>

<h2 id="4-5-数据写入"><a href="#4-5-数据写入" class="headerlink" title="4.5 数据写入"></a>4.5 数据写入</h2><p>Clickhouse内部所有的数据操作都是面向<strong>Block数据块</strong>的，所以INSERT查询最终会将数据转换为Block数据块。也正因如此，INSERT语句在单个数据块的写入过程中是具有<strong>原子性</strong>的。在默认情况下，每个数据块最多写入1048576行数据（由<code>max_insert_block_size</code>参数控制）。也就是说，如果一条INSERT语句写入的数据少于<code>max_insert_block_size</code>行，那么这批数据的写入是具有原子性的，即要么全部成功，要么全部失败。需要注意的是，只有在Clickhouse服务端处理数据的时候才具有这种原子写入的特性，例如使用JDBC或HTTP接口时。</p>
<h1 id="5-数据字典"><a href="#5-数据字典" class="headerlink" title="5. 数据字典"></a>5. 数据字典</h1><p>TO DO</p>
<h1 id="6-MergeTree原理解析"><a href="#6-MergeTree原理解析" class="headerlink" title="6. MergeTree原理解析"></a>6. MergeTree原理解析</h1><p>只有合并树系列的表引擎才支持<strong>主键索引</strong>、<strong>数据分区</strong>、<strong>数据副本</strong>和<strong>数据采样</strong>这些特性，同时也只有此系列的表引擎支持ALTER相关操作。</p>
<p><img src="E:%5Cblog%5Csource%5Cassets%5C%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6.png" alt="未命名文件"></p>
<p>家族中其他表引擎则在MergeTree的基础上各有所长。如果给MergeTree系列的表引擎加上Replicated前缀，也会得到一组支持数据副本的表引擎。</p>
<h2 id="6-1-MergeTree的创建方式与存储结构"><a href="#6-1-MergeTree的创建方式与存储结构" class="headerlink" title="6.1 MergeTree的创建方式与存储结构"></a>6.1 MergeTree的创建方式与存储结构</h2><ol>
<li><p>MergeTree的创建方式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE [IF NOT EXISTS] [db_name.]table_name (</span><br><span class="line">	name1 [type] [DEFAULT|MATERIALIZED|ALIAS expr],</span><br><span class="line">	name2 [type] [DEFAULT|MATERIALIZED|ALIAS expr],</span><br><span class="line">	...</span><br><span class="line">) ENGINE &#x3D; MergeTree()</span><br><span class="line">[PARTITION BY expr]</span><br><span class="line">[ORDER BY expr]</span><br><span class="line">[PRIMARY KEY expr]</span><br><span class="line">[SAMPLE BY expr]</span><br><span class="line">[SETTINT name&#x3D;value,...]</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>PARTITION BY</strong>：分区键，用于指定表数据以何种标准进行分区。</li>
<li><strong>ORDER BY</strong>：排序键，用于指定在一个数据片段内，数据以何种标准排序。<strong>默认情况下主键（PRIMARY KEY）与排序键相同</strong>。</li>
<li><strong>PRIMARY KEY</strong>：主键，声明后会依照主键字段生成<strong>一级索引</strong>，用于加速表查询。与其他数据库不同，MergeTree主键允许数据存在重复数据（ReplacingMergeTree可以去重）。</li>
<li><strong>SAMPLE BY</strong>：抽样表达式，用于声明数据以何种标准进行采样。</li>
<li><strong>SEETINGS：index_granularity</strong>：index_granularity表示<strong>索引的粒度</strong>，<strong>默认值为8192</strong>。也就是说，在默认情况下，每间隔8192行数据才生成一条索引。</li>
</ul>
</li>
<li><p><strong>MergeTree的存储结构</strong></p>
<p>其完整存储结构如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">table_name</span><br><span class="line">	- partition_1</span><br><span class="line">		  &#x2F;&#x2F;基础文件</span><br><span class="line">		- checksums.txt </span><br><span class="line">		  columns.txt</span><br><span class="line">		  count.txt</span><br><span class="line">		  primary.idx</span><br><span class="line">		  [Column].bin</span><br><span class="line">		  [Column].mrk</span><br><span class="line">		  [Column].mrk2</span><br><span class="line">		  </span><br><span class="line">		  &#x2F;&#x2F;使用了分区键时才会生成</span><br><span class="line">		  partition.dat</span><br><span class="line">		  minmax_[Column].idx</span><br><span class="line">		  </span><br><span class="line">		  &#x2F;&#x2F;使用二级索引时才会生成</span><br><span class="line">		  skp_idx_[Column].idx</span><br><span class="line">		  skp_idx_[Column].mrk</span><br><span class="line">	  partition_2</span><br><span class="line">	  ...</span><br><span class="line">	  partition_n</span><br></pre></td></tr></table></figure>

<p>一张数据表的完整物理结构包括3个层级，依次时<strong>数据库表目录</strong>、<strong>分区目录</strong>及各<strong>分区下具体的数据文件</strong>。</p>
<ol>
<li><strong>partition</strong>：分区目录。属于相同分区的数据，最终会被合并到同一分区目录，而不同分区的数据，永远不会被合并在一起。</li>
<li><strong>checksums.txt</strong>：校验文件，使用二进制格式存储。它保存了余下各类文件（primary.idx、count.txt等）的size大小及size的哈希值。</li>
<li><strong>columns.txt</strong>：<strong>列信息文件</strong>。用明文格式存储，用于保存此数据分区下的列字段信息。</li>
<li><strong>count.txt</strong>：技术文件。用明文格式存储。用于<strong>记录当前数据分区下数据的总行数</strong>。</li>
<li><strong>primary.idx</strong>：<strong>一级索引文件</strong>。</li>
<li><strong>[Column].bin</strong>：数据文件，使用压缩格式存储，默认为LZ4压缩格式，用于存储某一列的数据。</li>
<li><strong>[Column].mrk</strong>：列字段标记文件。标记文件中保存了文件中数据的偏移量信息。</li>
<li><strong>[Column.mrk2]</strong>：如果使用了自适应大小的索引间隔，则标记文件会以.mrk2格式命名。</li>
<li><strong>partition.dat与minmax_[Column].idx</strong>：如果使用了分区键，partition.dat会保存当前分区表达式最终生成的值；而minmax索引用于记录当前分区下分区字段对应的原始数据的最小和最大值。例如EventTime字段对应的原始数据为2019-05-01、2019-05-05，分区表达式为PARTITION BY toYYYYMM(EventTime)，则partition.dat中保存的值将会是2019-05，而minmax索引中保存的值将会是2019-05-012019-05-05。</li>
<li><strong>skp_idx_[Column].idx与skp_idx_[Column].mrk</strong>：如果声明了二级索引，则会额外生成相应的二级索引与标记文件。</li>
</ol>
</li>
</ol>
<h2 id="6-2-数据分区"><a href="#6-2-数据分区" class="headerlink" title="6.2 数据分区"></a>6.2 数据分区</h2><ol>
<li><p>分区目录的<strong>命名规则：</strong></p>
<p>一个完整分区目录的命名公式如下表示：</p>
<p><code>PartitionID_MinBlockNum_MaxBlockNum_Level</code></p>
<p>其中对于<strong>MinBlockNum、MaxBlockNum</strong>，<strong>BlockNum</strong>是一个整型的自增长编号n，n从1开始，每当创建一个分区目录时，计数n就会累计加1。对于一个新的分区目录而言，MinBlockNum与MaxBlockNum取值一样。例如201905_1_1_0、201906_2_2_0以此类推。但是当分区目录发生合并时，MinBlockNum和MaxBlockNum会有另外的取值规则。</p>
<p><strong>Level</strong>：合并的层级，可以理解为<strong>某个分区被合并过的次数</strong>，或者是这个分区的年龄。对于一个新创建的分区目录而言，其初始值均为0。之后，如果相同分区发生合并动作，则在相应分区内计数加1。</p>
</li>
<li><p>分区目录的<strong>合并过程</strong>：</p>
<p>伴随着每一批数据的写入（一次INSERT语句），MergeTree都会产生一批新的分区目录。即便不同批次写入的数据属于相同分区，也会生成不同的分区目录。在之后的某个时刻（也可以手动执行<strong>optimize</strong>查询语句），Clickhouse会通过后台任务再将属于相同分区的多个目录合并成一个目录。</p>
<p>新目录名称的合并方式遵循如下规则：</p>
<ul>
<li><strong>MinBlockNum</strong>：取同一分区内所有目录中最小的MinBlockNum值；</li>
<li><strong>MaxBlockNum</strong>：取同一分区内所有目录中最大的MaxBlockNum值；</li>
<li><strong>Level</strong>：取同一分区内最大的Level值并加1。</li>
</ul>
</li>
</ol>
<h2 id="6-3-一级索引"><a href="#6-3-一级索引" class="headerlink" title="6.3 一级索引"></a>6.3 一级索引</h2><ol>
<li><p><strong>索引粒度</strong></p>
<p>MergeTree使用<strong>MarkRange</strong>表示一个具体的区间，并通过start和end表示其具体的范围。</p>
<p>如下图所示，以CounterID作为主键索引。</p>
<p><img src="E:%5Cblog%5Csource%5Cassets%5C19530395-616ae9043a42ce93.png" alt="img"></p>
<p>如果使用多个主键，例如<code>ORDER BY (CountID, EventDate)</code>，则索引文件如下所示：</p>
<p><img src="E:%5Cblog%5Csource%5Cassets%5C19530395-0ebd8d5439fdbb25.png" alt="img"></p>
</li>
<li><p>索引的查询过程</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://extrali.com/2021/08/29/Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="黎达">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Extrali">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/29/Spark/" class="post-title-link" itemprop="url">Spark</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>

              <time title="Erstellt: 2021-08-29 23:30:52" itemprop="dateCreated datePublished" datetime="2021-08-29T23:30:52+08:00">2021-08-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Bearbeitet am</span>
                <time title="Geändert am: 2021-09-05 10:43:16" itemprop="dateModified" datetime="2021-09-05T10:43:16+08:00">2021-09-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">in</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-Spark概述"><a href="#1-Spark概述" class="headerlink" title="1 Spark概述"></a>1 Spark概述</h1><h2 id="1-1-Spark是什么"><a href="#1-1-Spark是什么" class="headerlink" title="1.1 Spark是什么"></a>1.1 Spark是什么</h2><p>Spark 是一种基于内存的快速、通用、可扩展的大数据分析<strong>计算引擎</strong>。  </p>
<h2 id="1-2-Spark-and-Hadoop"><a href="#1-2-Spark-and-Hadoop" class="headerlink" title="1.2 Spark and Hadoop"></a>1.2 Spark and Hadoop</h2><p>在之前的学习中， Hadoop 的 MapReduce 是大家广为熟知的计算框架，那为什么咱们还要学习<strong>新的计算框架 Spark</strong> 呢，这里就不得不提到 Spark 和 Hadoop 的关系。  </p>
<p><strong>Hadoop</strong></p>
<ul>
<li>Hadoop 是由 java 语言编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架</li>
<li>作为 Hadoop 分布式文件系统， <strong>HDFS</strong> 处于 Hadoop 生态圈的最下层，存储着所有的数 据 ，支 持 着 Hadoop 的 所 有 服 务 。 它 的 理 论 基 础 源 于 Google 的TheGoogleFileSystem 这篇论文，它是 GFS 的开源实现。</li>
<li><strong>MapReduce</strong> 是一种编程模型， Hadoop 根据 Google 的 MapReduce 论文将其实现，作为 Hadoop 的分布式计算模型，是 Hadoop 的核心。基于这个框架，分布式并行程序的编写变得异常简单。综合了 HDFS 的分布式存储和 MapReduce 的分布式计<br>算， Hadoop 在处理海量数据时， 性能横向扩展变得非常容易。</li>
<li><strong>HBase</strong> 是对 Google 的 Bigtable 的开源实现，但又和 Bigtable 存在许多不同之处。HBase 是一个基于 HDFS 的分布式数据库，擅长实时地随机读/写超大规模数据集。它也是 Hadoop 非常重要的组件。  </li>
</ul>
<p><strong>Spark</strong></p>
<ul>
<li>Spark 是一种由 <strong>Scala</strong> 语言开发的快速、通用、可扩展的大数据分析引擎</li>
<li>Spark Core 中提供了 Spark 最基础与最核心的功能</li>
<li>Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用SQL 或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。</li>
<li>Spark Streaming 是 Spark 平台上针对实时数据进行<strong>流式计算</strong>的组件，提供了丰富的处理数据流的 API。  </li>
</ul>
<p>由上面的信息可以获知， Spark 出现的时间相对较晚，并且主要功能主要是用于数据计算，所以其实 Spark 一直被认为是 Hadoop 框架的升级版。  </p>
<h2 id="1-3-Spark-or-Hadoop"><a href="#1-3-Spark-or-Hadoop" class="headerlink" title="1.3 Spark or Hadoop"></a>1.3 Spark or Hadoop</h2><p>Hadoop 的 <strong>MR 框架和 Spark 框架</strong>都是数据处理框架，那么我们在使用时如何选择呢？  </p>
<ul>
<li>Hadoop MapReduce 由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景（如：机器学习、图挖掘算法、交互式数据挖掘算法）中存在诸多计算效率等问题。所以 Spark 应运而生， Spark 就是在传统的 MapReduce 计算框架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的 RDD 计算模型。</li>
<li>机器学习中 ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。 MR 这种模式不太合适，即使多 MR 串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘， MR 显然不擅长。而Spark 所基于的 scala 语言恰恰擅长函数的处理。    </li>
<li>Spark 是一个分布式数据快速分析项目。它的核心技术是<strong>弹性分布式数据集</strong>（Resilient Distributed Datasets），提供了比 MapReduce 丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法。  </li>
<li><strong>Spark 和Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘。</strong>  </li>
<li>Spark Task 的启动时间快。 Spark 采用 <strong>fork 线程</strong>的方式，而 Hadoop 采用创建新的进程的方式。</li>
<li><strong>Spark 只有在 shuffle 的时候将数据写入磁盘</strong>，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互</li>
<li>Spark 的缓存机制比 HDFS 的缓存机制高效。  </li>
</ul>
<p>经过上面的比较，我们可以看出在绝大多数的数据计算场景中， Spark 确实会比 MapReduce更有优势。但是 Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致 Job 执行失败，此时， MapReduce 其实是一个更好的选择，所以 Spark并不能完全替代 MR。  </p>
<h2 id="1-4-Spark核心模块"><a href="#1-4-Spark核心模块" class="headerlink" title="1.4 Spark核心模块"></a>1.4 Spark核心模块</h2><p><img src="https://i.loli.net/2021/08/30/9BIa3TMHukPe716.png" alt="image-20210830204726731"></p>
<ul>
<li><strong>Spark Core</strong><br>Spark Core 中提供了 Spark 最基础与最核心的功能， Spark 其他的功能如： Spark SQL，Spark Streaming， GraphX, MLlib 都是在 Spark Core 的基础上进行扩展的。</li>
<li><strong>Spark SQL</strong><br>Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用 SQL或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。</li>
<li><strong>Spark Streaming</strong><br>Spark Streaming 是 Spark 平台上针对<strong>实时数据进行流式计算</strong>的组件，提供了丰富的处理数据流的 API。</li>
<li><strong>Spark MLlib</strong><br>MLlib 是 Spark 提供的一个机器学习算法库。 MLlib 不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。</li>
<li>Spark GraphX<br>GraphX 是 Spark 面向图计算提供的框架与算法库。  </li>
</ul>
<h1 id="2-Spark运行架构"><a href="#2-Spark运行架构" class="headerlink" title="2 Spark运行架构"></a>2 Spark运行架构</h1><h2 id="2-1-运行架构"><a href="#2-1-运行架构" class="headerlink" title="2.1 运行架构"></a>2.1 运行架构</h2><p>Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 <strong>master-slave</strong> 的结构。<br>如下图所示，它展示了一个 Spark 执行时的基本结构。 图形中的 <strong>Driver 表示 master</strong>，负责管理整个集群中的作业任务调度。图形中的 <strong>Executor 则是 slave</strong>，负责实际执行任务。  </p>
<p><img src="https://i.loli.net/2021/08/31/tTvFgK7hp46ljuY.png" alt="image-20210831205527697"></p>
<h2 id="2-2-核心组件"><a href="#2-2-核心组件" class="headerlink" title="2.2 核心组件"></a>2.2 核心组件</h2><p>由上图可以看出，对于 Spark 框架有两个核心组件：  </p>
<h3 id="2-2-1-Driver"><a href="#2-2-1-Driver" class="headerlink" title="2.2.1 Driver"></a>2.2.1 Driver</h3><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。Driver 在 Spark 作业执行时主要负责：  </p>
<ul>
<li>将用户程序转化为作业（job）</li>
<li>在 Executor 之间调度任务(task)</li>
<li>跟踪 Executor 的执行情况</li>
<li>通过 UI 展示查询运行情况</li>
</ul>
<p>实际上，我们无法准确地描述 Driver 的定义，因为在整个的编程过程中没有看到任何有关Driver 的字眼。所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为Driver 类。  </p>
<h3 id="2-2-2-Executor"><a href="#2-2-2-Executor" class="headerlink" title="2.2.2 Executor"></a>2.2.2 Executor</h3><p>Spark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task） ，任务彼此之间相互独立。 Spark 应用启动时， Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有 Executor 节点发生了<br>故障或崩溃， Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。</p>
<p>Executor 有两个核心功能：</p>
<ul>
<li>负责运行组成 Spark 应用的任务，并将结果返回给Driver进程；</li>
<li>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。 RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。    </li>
</ul>
<h3 id="2-2-3-Master-amp-Worker"><a href="#2-2-3-Master-amp-Worker" class="headerlink" title="2.2.3 Master&amp;Worker"></a>2.2.3 Master&amp;Worker</h3><p>Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件： <strong>Master 和 Worker</strong>，这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 RM, 而Worker 呢，也是进程，一个 Worker 运行在集群中的一台服务器上，由 Master 分配资源对数据进行并行的处理和计算，类似于 Yarn 环境中NM。  </p>
<h3 id="2-2-4-Application-Master"><a href="#2-2-4-Application-Master" class="headerlink" title="2.2.4 Application Master"></a>2.2.4 Application Master</h3><p>​    Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包含 ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。说的简单点就是， ResourceManager（资源）和 Driver（计算）之间的解耦合靠的就是ApplicationMaster。  </p>
<h2 id="2-3-核心概念"><a href="#2-3-核心概念" class="headerlink" title="2.3 核心概念"></a>2.3 核心概念</h2><h3 id="2-3-1-Executor与Core"><a href="#2-3-1-Executor与Core" class="headerlink" title="2.3.1 Executor与Core"></a>2.3.1 Executor与Core</h3><p>Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 <strong>CPU 核（Core）数</strong><br><strong>量</strong>。  </p>
<p>应用程序相关启动参数如下：  </p>
<table>
<thead>
<tr>
<th align="center">名称</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">–num-executors</td>
<td align="center">配置 Executor 的数量</td>
</tr>
<tr>
<td align="center">–executor-memory</td>
<td align="center">配置每个 Executor 的内存大小</td>
</tr>
<tr>
<td align="center">–executor-cores</td>
<td align="center">配置每个 Executor 的虚拟 CPU core 数量</td>
</tr>
</tbody></table>
<h3 id="2-3-2-并行度（-Parallelism）"><a href="#2-3-2-并行度（-Parallelism）" class="headerlink" title="2.3.2 并行度（ Parallelism）"></a>2.3.2 并行度（ Parallelism）</h3><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，这里是<strong>并行</strong>，而不是并发。这里我们将整个集群并行执行任务的数量称之为并行度。那么一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。  </p>
<h3 id="2-3-3-有向无环图（DAG）"><a href="#2-3-3-有向无环图（DAG）" class="headerlink" title="2.3.3 有向无环图（DAG）"></a>2.3.3 有向无环图（DAG）</h3><p><img src="https://i.loli.net/2021/08/31/69awlsvqR7A2YSj.png" alt="image-20210831214147559"></p>
<p>​    大数据计算引擎框架我们根据使用方式的不同一般会分为<strong>四类</strong>，其中第一类就是Hadoop 所承载的 MapReduce,它将计算分为两个阶段，分别为 Map 阶段 和 Reduce 阶段。对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job的串联，以完成一个完整的算法，例如迭代计算。 由于这样的弊端，催生了支持 DAG 框架的产生。 因此，支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 Oozie 来说，大多还是批处理的任务。接下来就是以 Spark 为代表的第三代的计算引擎。第三代计算引擎的特点主要是 Job 内部的 DAG 支持（不跨越 Job），以及实时计算。</p>
<p>​    这里所谓的有向无环图，并不是真正意义的图形，而是<strong>由 Spark 程序直接映射成的数据流的高级抽象模型</strong>。简单理解就是<strong>将整个程序计算的执行过程用图形表示出来</strong>,这样更直观，更便于理解，可以用于表示程序的拓扑结构。  </p>
<p>​    <strong>DAG（Directed Acyclic Graph）有向无环图</strong>是由点和线组成的拓扑图形，该图形具有方向，不会闭环。  </p>
<h2 id="2-4-提交流程"><a href="#2-4-提交流程" class="headerlink" title="2.4 提交流程"></a>2.4 提交流程</h2><p>​    所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又有细微的区别，我们这里不进行详细的比较，但是因为国内工作中，将 Spark 引用部署到Yarn 环境中会更多一些，所以本课程中的提交流程是基于 Yarn 环境的。  </p>
<p><img src="https://i.loli.net/2021/08/31/X4iEUgTwRqnKQcl.png" alt="image-20210831214506334"></p>
<p>​    Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式： Client和 Cluster。 <strong>两种模式主要区别在于： Driver 程序的运行节点位置</strong>。  </p>
<h3 id="2-4-1-Yarn-Client模式"><a href="#2-4-1-Yarn-Client模式" class="headerlink" title="2.4.1 Yarn Client模式"></a>2.4.1 Yarn Client模式</h3><p>​    Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试。  </p>
<ul>
<li>Driver 在任务提交的本地机器上运行；</li>
<li>Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster；</li>
<li>ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster， 负责向 ResourceManager 申请 Executor 内存；</li>
<li>ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程 ；</li>
<li>Executor 进程启动后会向 Driver 反向注册， Executor 全部注册完成后 Driver 开始执行main 函数；</li>
<li>之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。  </li>
</ul>
<h3 id="2-4-2-Yarn-Cluster模式"><a href="#2-4-2-Yarn-Cluster模式" class="headerlink" title="2.4.2 Yarn Cluster模式"></a>2.4.2 Yarn Cluster模式</h3><p>​    Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于实际生产环境。  </p>
<ul>
<li>在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动ApplicationMaster；</li>
<li>随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver；</li>
<li>Driver 启动后向 ResourceManager 申请 Executor 内存， ResourceManager 接到ApplicationMaster 的资源申请后会分配container，然后在合适的 NodeManager 上启动Executor 进程；</li>
<li>Executor 进程启动后会向 Driver 反向注册， Executor 全部注册完成后 Driver 开始执行main 函数；</li>
<li>之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。  </li>
</ul>
<h1 id="3-Spark核心编程"><a href="#3-Spark核心编程" class="headerlink" title="3 Spark核心编程"></a>3 Spark核心编程</h1><p>​    Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p>
<ul>
<li><strong>RDD : 弹性分布式数据集</strong></li>
<li><strong>累加器</strong>：分布式共享<strong>只写</strong>变量</li>
<li><strong>广播变量</strong>：分布式共享<strong>只读</strong>变量</li>
</ul>
<h2 id="3-1-RDD"><a href="#3-1-RDD" class="headerlink" title="3.1 RDD"></a>3.1 RDD</h2><h3 id="3-1-1-什么是RDD？"><a href="#3-1-1-什么是RDD？" class="headerlink" title="3.1.1 什么是RDD？"></a>3.1.1 什么是RDD？</h3><p>RDD（<strong>Resilient Distributed Dataset</strong>）叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个<strong>弹性的、不可变、可分区、里面的元素可并行计算</strong>的集合。  </p>
<ul>
<li><p>弹性</p>
<ul>
<li><strong>存储的弹性：内存与磁盘的自动切换</strong>；</li>
<li><strong>容错的弹性：数据丢失可以自动恢复</strong>；</li>
<li><strong>计算的弹性：计算出错重试机制</strong>；</li>
<li><strong>分片的弹性：可根据需要重新分片</strong>。  </li>
</ul>
</li>
<li><p>分布式：数据存储在大数据集群不同节点上</p>
</li>
<li><p>数据集： <strong>RDD 封装了计算逻辑，并不保存数据</strong></p>
</li>
<li><p>数据抽象： RDD 是一个抽象类，需要子类具体实现</p>
</li>
<li><p>不可变： <strong>RDD 封装了计算逻辑，是不可以改变的</strong>，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑</p>
</li>
<li><p>可分区、并行计算  </p>
</li>
</ul>
<h3 id="3-1-2-核心属性"><a href="#3-1-2-核心属性" class="headerlink" title="3.1.2 核心属性"></a>3.1.2 核心属性</h3><p><img src="https://i.loli.net/2021/09/04/re84aGMuw3KUQvc.png" alt="image-20210904155949641"></p>
<ul>
<li><p><strong>分区列表</strong></p>
<p>RDD 数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。  </p>
<p><img src="https://i.loli.net/2021/09/04/2vIFZBsMfWpz6cV.png" alt="image-20210904160103418"></p>
</li>
<li><p><strong>分区计算函数</strong></p>
<p>Spark 在计算时，是使用分区函数对每一个分区进行计算  </p>
<p><img src="https://i.loli.net/2021/09/04/3qyAgEhCU9xLNZ1.png" alt="image-20210904160202543"></p>
</li>
<li><p><strong>RDD 之间的依赖关系</strong>  </p>
<p>RDD 是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个 RDD 建立依赖关系  </p>
<p><img src="https://i.loli.net/2021/09/04/b9RCaxPwINQME2j.png" alt="image-20210904160300386"></p>
</li>
<li><p><strong>分区器（可选）</strong>  </p>
<p>当数据为 KV 类型数据时，可以通过设定分区器自定义数据的分区  </p>
<p><img src="https://i.loli.net/2021/09/04/esdwbDLG4kN6oRy.png" alt="image-20210904160351926"></p>
</li>
<li><p><strong>首选位置（可选）</strong>  </p>
<p>计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算  </p>
<p><img src="https://i.loli.net/2021/09/04/e69tayM1jX2JCPc.png" alt="image-20210904160412938"></p>
</li>
</ul>
<h3 id="3-1-3-执行原理"><a href="#3-1-3-执行原理" class="headerlink" title="3.1.3 执行原理"></a>3.1.3 执行原理</h3><p>从计算的角度来讲，数据处理过程中需要<strong>计算资源（内存 &amp; CPU）</strong>和<strong>计算模型（逻辑）</strong>。执行时，需要将计算资源和计算模型进行协调和整合。  </p>
<p>Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。  </p>
<p>RDD 是 Spark 框架中用于数据处理的核心模型，接下来我们看看，在 Yarn 环境中， RDD的工作原理:  </p>
<ol>
<li><p>启动Yarn集群环境</p>
<p><img src="https://i.loli.net/2021/09/04/g5wFXiLAhtGj7Y6.png" alt="image-20210904160631603"></p>
</li>
<li><p>Spark 通过申请资源创建调度节点和计算节点  </p>
<p><img src="https://i.loli.net/2021/09/04/Swuben7O1aFt5zf.png" alt="image-20210904160646108"></p>
</li>
<li><p>Spark 框架根据需求将计算逻辑根据分区划分成不同的任务  </p>
<p><img src="https://i.loli.net/2021/09/04/3ma8lCtW2KMfjhH.png" alt="image-20210904160720057"></p>
</li>
<li><p>调度节点将任务根据计算节点状态发送到对应的计算节点进行计算  </p>
<p><img src="https://i.loli.net/2021/09/04/PGv9z5XcMD2ibwr.png" alt="image-20210904160733051"></p>
</li>
</ol>
<p>从以上流程可以看出 RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算，接下来我们就一起看看 Spark 框架中 RDD 是具体是如何进行数据处理的。  </p>
<h3 id="3-1-4-基础编程"><a href="#3-1-4-基础编程" class="headerlink" title="3.1.4 基础编程"></a>3.1.4 基础编程</h3><h4 id="3-1-4-1-RDD创建"><a href="#3-1-4-1-RDD创建" class="headerlink" title="3.1.4.1 RDD创建"></a>3.1.4.1 RDD创建</h4><p>在 Spark 中创建 RDD 的创建方式可以分为四种：  </p>
<ol>
<li><p><strong>从集合（内存）中创建 RDD</strong> </p>
<p> 从集合中创建 RDD， Spark 主要提供了两个方法： <strong>parallelize</strong> 和 <strong>makeRDD</strong>  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">val sparkConf &#x3D;</span><br><span class="line">new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;spark&quot;)</span><br><span class="line">val sparkContext &#x3D; new SparkContext(sparkConf)</span><br><span class="line">val rdd1 &#x3D; sparkContext.parallelize(</span><br><span class="line">List(1,2,3,4)</span><br><span class="line">)</span><br><span class="line">val rdd2 &#x3D; sparkContext.makeRDD(</span><br><span class="line">List(1,2,3,4)</span><br><span class="line">)</span><br><span class="line">rdd1.collect().foreach(println)</span><br><span class="line">rdd2.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure>

<p>从底层代码实现来讲， <strong>makeRDD 方法其实就是 parallelize 方法</strong>  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def makeRDD[T: ClassTag](</span><br><span class="line">    seq: Seq[T],</span><br><span class="line">    numSlices: Int &#x3D; defaultParallelism): RDD[T] &#x3D; withScope &#123;</span><br><span class="line">    parallelize(seq, numSlices)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>*<em>从外部存储（文件）创建 RDD  *</em></p>
<p>由外部存储系统的数据集创建 RDD 包括：本地的文件系统，所有 Hadoop 支持的数据集，比如 HDFS、 HBase 等。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val sparkConf &#x3D;</span><br><span class="line">	new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;spark&quot;)</span><br><span class="line">val sparkContext &#x3D; new SparkContext(sparkConf)</span><br><span class="line">val fileRDD: RDD[String] &#x3D; sparkContext.textFile(&quot;input&quot;)</span><br><span class="line">fileRDD.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure>
</li>
<li><p>*<em>从其他 RDD 创建  *</em></p>
<p>主要是通过一个 RDD 运算完后，再产生新的 RDD。 详情请参考后续章节  </p>
</li>
<li><p><strong>直接创建 RDD（new）</strong>  </p>
<p>使用 new 的方式直接构造 RDD，一般由 Spark 框架自身使用。  </p>
</li>
</ol>
<h4 id="3-1-4-2-RDD-并行度与分区"><a href="#3-1-4-2-RDD-并行度与分区" class="headerlink" title="3.1.4.2 RDD 并行度与分区"></a>3.1.4.2 RDD 并行度与分区</h4><p>默认情况下， Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而<strong>能够并行计算的任务数量我们称之为并行度</strong>。这个数量可以在构建 RDD 时指定。 记住，这里的<strong>并行执行的任务数量，并不是指的切分任务的数量</strong>，不要混淆了。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">val sparkConf &#x3D;</span><br><span class="line">	new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;spark&quot;)</span><br><span class="line">val sparkContext &#x3D; new SparkContext(sparkConf)</span><br><span class="line">val dataRDD: RDD[Int] &#x3D; sparkContext.makeRDD(</span><br><span class="line">    List(1,2,3,4),</span><br><span class="line">    4)</span><br><span class="line">val fileRDD: RDD[String] &#x3D; sparkContext.textFile(</span><br><span class="line">    &quot;input&quot;,</span><br><span class="line">    2)</span><br><span class="line">fileRDD.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure>

<ul>
<li><p>读取<strong>内存</strong>数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark 核心源码如下：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] &#x3D; &#123;</span><br><span class="line">    (0 until numSlices).iterator.map &#123; i &#x3D;&gt;</span><br><span class="line">    val start &#x3D; ((i * length) &#x2F; numSlices).toInt</span><br><span class="line">    val end &#x3D; (((i + 1) * length) &#x2F; numSlices).toInt</span><br><span class="line">    (start, end)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>读取文件数据时，数据是按照 Hadoop 文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异，具体 Spark 核心源码如下 :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">public InputSplit[] getSplits(JobConf job, int numSplits)</span><br><span class="line">throws IOException &#123;</span><br><span class="line">    long totalSize &#x3D; 0; &#x2F;&#x2F; compute total size</span><br><span class="line">    for (FileStatus file: files) &#123; &#x2F;&#x2F; check we have valid files</span><br><span class="line">    if (file.isDirectory()) &#123;</span><br><span class="line">    throw new IOException(&quot;Not a file: &quot;+ file.getPath());</span><br><span class="line">    &#125;</span><br><span class="line">    totalSize +&#x3D; file.getLen();</span><br><span class="line">    &#125;</span><br><span class="line">    long goalSize &#x3D; totalSize &#x2F; (numSplits &#x3D;&#x3D; 0 ? 1 : numSplits);</span><br><span class="line">    long minSize &#x3D; Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</span><br><span class="line">    FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);</span><br><span class="line">    ...</span><br><span class="line">    for (FileStatus file: files) &#123;</span><br><span class="line">    ...</span><br><span class="line">    if (isSplitable(fs, path)) &#123;</span><br><span class="line">    long blockSize &#x3D; file.getBlockSize();</span><br><span class="line">    long splitSize &#x3D; computeSplitSize(goalSize, minSize, blockSize);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected long computeSplitSize(long goalSize, long minSize,</span><br><span class="line">long blockSize) &#123;</span><br><span class="line">	return Math.max(minSize, Math.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="3-1-4-3-RDD转换算子"><a href="#3-1-4-3-RDD转换算子" class="headerlink" title="3.1.4.3 RDD转换算子"></a>3.1.4.3 RDD转换算子</h4><p>RDD 根据数据处理方式的不同将算子整体上分为 <strong>Value 类型</strong>、<strong>双 Value 类型</strong>和 <strong>Key-Value类型</strong>  </p>
<ol>
<li><p><strong>Value类型</strong></p>
<ol>
<li><p><strong>Map</strong></p>
<ul>
<li><p>函数签名</p>
<p><code>def map[U: ClassTag](f: T =&gt; U): RDD[U]</code></p>
</li>
<li><p>函数说明</p>
<p>将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">val dataRDD: RDD[Int] &#x3D; sparkContext.makeRDD(List(1,2,3,4))</span><br><span class="line">val dataRDD1: RDD[Int] &#x3D; dataRDD.map(</span><br><span class="line">    num &#x3D;&gt; &#123;</span><br><span class="line">    	num * 2</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">val dataRDD2: RDD[String] &#x3D; dataRDD1.map(</span><br><span class="line">    num &#x3D;&gt; &#123;</span><br><span class="line">    	&quot;&quot; + num</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>mapPartitions</strong></p>
<ul>
<li><p>函数签名  </p>
<p>`def mapPartitions[U: ClassTag](</p>
<pre><code>f: Iterator[T] =&gt; Iterator[U],
preservesPartitioning: Boolean = false): RDD[U]  `</code></pre></li>
<li><p>函数说明</p>
<p>将待处理的数据<strong>以分区为单位</strong>发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val dataRDD1: RDD[Int] &#x3D; dataRDD.mapPartitions(</span><br><span class="line">    datas &#x3D;&gt; &#123;</span><br><span class="line">    	datas.filter(_&#x3D;&#x3D;2)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><strong>map 和 mapPartitions 的区别？</strong>  </p>
<ol>
<li><p>数据处理角度</p>
<p>Map 算子是分区内一个数据一个数据的执行，类似于串行操作。而 mapPartitions 算子是<strong>以分区为单位</strong>进行批处理操作。  </p>
</li>
<li><p>功能的角度  </p>
<p>Map 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。MapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据。</p>
</li>
<li><p>性能的角度  </p>
<p>Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处理，所以性能较高。但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用 map 操作。  </p>
</li>
</ol>
</li>
<li><p>mapPartitionsWithIndex  </p>
<ul>
<li><p>函数签名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def mapPartitionsWithIndex[U: ClassTag](</span><br><span class="line">    f: (Int, Iterator[T]) &#x3D;&gt; Iterator[U],</span><br><span class="line">    preservesPartitioning: Boolean &#x3D; false): RDD[U]</span><br></pre></td></tr></table></figure>
</li>
<li><p>函数说明</p>
<p>将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时<strong>可以获取当前分区索引</strong>。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val dataRDD1 &#x3D; dataRDD.mapPartitionsWithIndex(</span><br><span class="line">    (index, datas) &#x3D;&gt; &#123;</span><br><span class="line">    	datas.map(index, _)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>flatMap</p>
<ul>
<li><p>函数签名  </p>
<p><code>def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U]</code></p>
</li>
<li><p>函数说明</p>
<p>将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射  </p>
</li>
</ul>
</li>
</ol>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://extrali.com/2021/08/29/Zookeeper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="黎达">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Extrali">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/29/Zookeeper/" class="post-title-link" itemprop="url">zookeeper</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              

              <time title="Erstellt: 2021-08-29 16:45:38 / Geändert am: 23:27:54" itemprop="dateCreated datePublished" datetime="2021-08-29T16:45:38+08:00">2021-08-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">in</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-Zookeeper入门"><a href="#1-Zookeeper入门" class="headerlink" title="1 Zookeeper入门"></a>1 Zookeeper入门</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><p>Zookeeper 是一个开源的分布式的，为分布式框架提供协调服务的 Apache 项目。  </p>
<p><img src="https://i.loli.net/2021/08/29/G8wMgyoR3hPKeCH.png" alt="image-20210829170800334"></p>
<h2 id="1-2-特点"><a href="#1-2-特点" class="headerlink" title="1.2 特点"></a>1.2 特点</h2><p><img src="https://i.loli.net/2021/08/29/CAsXa3iEpebLNhu.png" alt="image-20210829172306485"></p>
<h2 id="1-3-数据结构"><a href="#1-3-数据结构" class="headerlink" title="1.3 数据结构"></a>1.3 数据结构</h2><p>​    ZooKeeper 数据模型的结构与 Unix 文件系统很类似，整体上可以看作是一棵树，每个节点称做一个 ZNode。每一个 ZNode 默认能够存储 <strong>1MB</strong> 的数据，<strong>每个 ZNode 都可以通过其路径唯一标识</strong>。  </p>
<p><img src="https://i.loli.net/2021/08/29/op2xURLIHDZPsSG.png" alt="image-20210829172522549"></p>
<h2 id="1-4-应用场景"><a href="#1-4-应用场景" class="headerlink" title="1.4 应用场景"></a>1.4 应用场景</h2><p>提供的服务包括：<strong>统一命名服务</strong>、<strong>统一配置管理</strong>、<strong>统一集群管理</strong>、<strong>服务器节点动态上下线</strong>、<strong>软负载均衡</strong>等。  </p>
<p><img src="https://i.loli.net/2021/08/29/P5GeN16VgCwtaYO.png" alt="image-20210829172609144"></p>
<p><img src="https://i.loli.net/2021/08/29/IJ7U85RKxbXmB3Y.png" alt="image-20210829172904548"></p>
<p><img src="https://i.loli.net/2021/08/29/NcLZ2SxioJMsUYm.png" alt="image-20210829174416869"></p>
<p><img src="https://i.loli.net/2021/08/29/wlO1NQCRYSgUTGq.png" alt="image-20210829174440337"></p>
<h1 id="2-Zookeeper集群操作"><a href="#2-Zookeeper集群操作" class="headerlink" title="2 Zookeeper集群操作"></a>2 Zookeeper集群操作</h1><h2 id="2-1-集群操作"><a href="#2-1-集群操作" class="headerlink" title="2.1 集群操作"></a>2.1 集群操作</h2><h3 id="2-1-1-选举机制"><a href="#2-1-1-选举机制" class="headerlink" title="2.1.1 选举机制"></a>2.1.1 选举机制</h3><p><img src="https://i.loli.net/2021/08/29/gZdvFLCXUyKoWS3.png" alt="image-20210829211053805"></p>
<ol>
<li>服务器1启动， 发起一次选举。 服务器1投自己一票。 此时服务器1票数一票， 不够半数以上（ 3票） ， 选举无法完成， 服务器1状态保持为<strong>LOOKING</strong>；</li>
<li>服务器2启动， 再发起一次选举。 服务器1和2分别投自己一票并交换选票信息： 此时<strong>服务器1发现服务器2的myid比自己目前投票推举的（服务器1）大</strong>， <strong>更改选票为推举服务器2</strong>。 此时服务器1票数0票， 服务器2票数2票， 没有半数以上结果， 选举无法完成， 服务器1， 2状态保持LOOKING</li>
<li>服务器3启动， 发起一次选举。 此时服务器1和2都会更改选票为服务器3。 此次投票结果：服务器1为0票， 服务器2为0票， 服务器3为3票。 此时服务器3的票数已经超过半数， 服务器3当选Leader。 <strong>服务器1， 2更改状态为FOLLOWING</strong>， <strong>服务器3更改状态为LEADING</strong>。</li>
<li>服务器4启动， 发起一次选举。 <strong>此时服务器1， 2， 3已经不是LOOKING状态， 不会更改选票信息</strong>。 交换选票信息结果：服务器3为3票， 服务器4为1票。 此时服务器4服从多数， 更改选票信息为服务器3， 并更改状态为FOLLOWING；</li>
<li>服务器5启动， 同4一样当小弟。  </li>
</ol>
<p><strong>非第一次启动：</strong></p>
<ol>
<li><p>当ZooKeeper集群中的一台服务器出现以下两种情况之一时， 就会开始进入Leader选举：</p>
<ul>
<li>服务器初始化启动。</li>
<li>服务器运行期间无法和Leader保持连接。  </li>
</ul>
</li>
<li><p>而当一台机器进入Leader选举流程时，当前集群也可能会处于以下两种状态：</p>
<ul>
<li><p>集群中本来就已经存在一个Leader。<br>对于第一种已经存在Leader的情况，机器试图去选举Leader时，会被告知当前服务器的Leader信息，对于该机器来说，仅仅需要和Leader机器建立连接，并进行状态同步即可。  </p>
</li>
<li><p>集群中确实不存在Leader。</p>
<p>假设ZooKeeper由5台服务器组成， SID分别为1、 2、 3、 4、 5， ZXID分别为8、 8、 8、 7、 7，并且此时SID为3的服务器是Leader。某一时刻，3和5服务器出现故障，因此开始进行Leader选举。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">						（EPOCH， ZXID， SID ）</span><br><span class="line">SID为1、 2、 4的机器投票情况： （1， 8， 1） （1， 8， 2） （1， 7， 4）</span><br></pre></td></tr></table></figure>

<p>选举Leader规则：</p>
<ol>
<li>EPOCH大的直接胜出 ；</li>
<li><strong>EPOCH相同，事务id大的胜出</strong> ；</li>
<li>事务id相同，服务器id大的胜出  。</li>
</ol>
</li>
</ul>
</li>
</ol>
<h2 id="2-2-客户端命令行操作"><a href="#2-2-客户端命令行操作" class="headerlink" title="2.2 客户端命令行操作"></a>2.2 客户端命令行操作</h2><h3 id="2-2-1-命令行语法"><a href="#2-2-1-命令行语法" class="headerlink" title="2.2.1 命令行语法"></a>2.2.1 命令行语法</h3><table>
<thead>
<tr>
<th>命令基本语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>help</td>
<td>显示所有操作命令</td>
</tr>
<tr>
<td>ls path</td>
<td>使用 ls 命令来查看当前 znode 的子节点 [可监听] <br />-w  监听子节点变化 <br />-s   附加次级信息</td>
</tr>
<tr>
<td>create</td>
<td>普通创建 <br />-s 含有序列 <br />-e 临时（重启或者超时消失）</td>
</tr>
<tr>
<td>get path</td>
<td>获得节点的值 [可监听] <br />-w 监听节点内容变化 <br />-s 附加次级信息</td>
</tr>
<tr>
<td>set</td>
<td>设置节点的具体值</td>
</tr>
<tr>
<td>stat</td>
<td>查看节点状态</td>
</tr>
<tr>
<td>delete</td>
<td>删除节点</td>
</tr>
<tr>
<td>deleteall</td>
<td>递归删除节点</td>
</tr>
</tbody></table>
<h3 id="2-2-2-znode节点信息"><a href="#2-2-2-znode节点信息" class="headerlink" title="2.2.2 znode节点信息"></a>2.2.2 znode节点信息</h3><p><strong>查看当前节点详细信息</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop102:2181(CONNECTED) 5] ls -s &#x2F;</span><br><span class="line">[zookeeper]cZxid &#x3D; 0x0</span><br><span class="line">ctime &#x3D; Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid &#x3D; 0x0</span><br><span class="line">mtime &#x3D; Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid &#x3D; 0x0</span><br><span class="line">cversion &#x3D; -1</span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 0</span><br><span class="line">numChildren &#x3D; 1</span><br></pre></td></tr></table></figure>

<ol>
<li><strong>czxid： 创建节点的事务 zxid</strong><br>每次修改 ZooKeeper 状态都会产生一个 ZooKeeper 事务 ID。事务 ID 是 ZooKeeper 中所有修改总的次序。每次修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1 在 zxid2 之前发生。  </li>
<li><strong>ctime： znode 被创建的毫秒数</strong>（从 1970 年开始）  </li>
<li>mzxid： <strong>znode 最后更新的事务 zxid</strong>  </li>
<li>mtime： znode 最后修改的毫秒数（从 1970 年开始） </li>
<li>pZxid： <strong>znode 最后更新的子节点 zxid</strong>  </li>
<li>cversion： znode 子节点变化号， znode 子节点修改次数  </li>
<li><strong>dataversion： znode 数据变化号</strong>  </li>
<li>aclVersion： znode 访问控制列表的变化号</li>
<li>ephemeralOwner： 如果是临时节点，这个是 znode 拥有者的 session id。如果不是临时节点则是 0。  </li>
<li><strong>dataLength： znode 的数据长度</strong></li>
<li><strong>numChildren： znode 子节点数量</strong>  </li>
</ol>
<h3 id="2-2-3-节点类型（持久-短暂-有序号-无序号）"><a href="#2-2-3-节点类型（持久-短暂-有序号-无序号）" class="headerlink" title="2.2.3 节点类型（持久/短暂/有序号/无序号）"></a>2.2.3 节点类型（持久/短暂/有序号/无序号）</h3><p><img src="https://i.loli.net/2021/08/29/D5g3AMzpsLGv9uk.png" alt="image-20210829220856006"></p>
<h3 id="2-2-4-监听器原理"><a href="#2-2-4-监听器原理" class="headerlink" title="2.2.4 监听器原理"></a>2.2.4 监听器原理</h3><p>客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、节点删除、子目录节点增加删除）时， ZooKeeper 会通知客户端。<strong>监听机制保证 ZooKeeper 保存的任何的数据的任何改变都能快速的响应到监听了该节点的应用程序</strong>。  </p>
<p><img src="https://i.loli.net/2021/08/29/pwxWdQe7vcOkNuX.png" alt="image-20210829221239963"></p>
<p>在hadoop103再多次修改/sanguo的值， hadoop104上不会再收到监听。因为<strong>注册一次，只能监听一次。想再次监听，需要再次注册。</strong>  </p>
<h1 id="3-Zookeeper分布式锁案例"><a href="#3-Zookeeper分布式锁案例" class="headerlink" title="3 Zookeeper分布式锁案例"></a>3 Zookeeper分布式锁案例</h1><p>什么叫做分布式锁呢？<br>比如说”进程 1”在使用该资源的时候，会先去获得锁， “进程 1”获得锁以后会对该资源保持独占，这样其他进程就无法访问该资源， “进程 1”用完该资源以后就将锁释放掉，让其他进程来获得锁，那么通过这个锁机制，我们就能保证了分布式系统中多个进程能够有序的访问该临界资源。那么我们把这个<strong>分布式环境</strong>下的这个锁叫作分布式锁  </p>
<p><img src="https://i.loli.net/2021/08/29/qlWVpOb1FDa284B.png" alt="image-20210829223733257"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">package com.hust.zookeeper.lock;</span><br><span class="line"></span><br><span class="line">import org.apache.zookeeper.*;</span><br><span class="line">import org.apache.zookeeper.data.Stat;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.Collections;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.concurrent.CountDownLatch;</span><br><span class="line"></span><br><span class="line">public class DistributedLock &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; zookeeper server 列表</span><br><span class="line">    private String connectString &#x3D;</span><br><span class="line">            &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;;</span><br><span class="line">    &#x2F;&#x2F; 超时时间</span><br><span class="line">    private int sessionTimeout &#x3D; 2000;</span><br><span class="line">    private ZooKeeper zk;</span><br><span class="line">    private String rootNode &#x3D; &quot;locks&quot;;</span><br><span class="line">    private String subNode &#x3D; &quot;seq-&quot;;</span><br><span class="line">    &#x2F;&#x2F; 当前 client 等待的子节点</span><br><span class="line">    private String waitPath;</span><br><span class="line">    &#x2F;&#x2F;ZooKeeper 连接</span><br><span class="line">    private CountDownLatch connectLatch &#x3D; new CountDownLatch(1);</span><br><span class="line">    &#x2F;&#x2F;ZooKeeper 节点等待</span><br><span class="line">    private CountDownLatch waitLatch &#x3D; new CountDownLatch(1);</span><br><span class="line">    &#x2F;&#x2F; 当前 client 创建的子节点</span><br><span class="line">    private String currentNode;</span><br><span class="line"></span><br><span class="line">    public DistributedLock() throws IOException, InterruptedException, KeeperException &#123;</span><br><span class="line">        zk &#x3D; new ZooKeeper(connectString, sessionTimeout, new</span><br><span class="line">                Watcher() &#123;</span><br><span class="line">                    @Override</span><br><span class="line">                    public void process(WatchedEvent event) &#123;</span><br><span class="line">						&#x2F;&#x2F; 连接建立时, 打开 latch, 唤醒 wait 在该 latch 上的线程</span><br><span class="line">                        if (event.getState() &#x3D;&#x3D;</span><br><span class="line">                                Event.KeeperState.SyncConnected) &#123;</span><br><span class="line">                            connectLatch.countDown();</span><br><span class="line">                        &#125;</span><br><span class="line">						&#x2F;&#x2F; 发生了 waitPath 的删除事件</span><br><span class="line">                        if (event.getType() &#x3D;&#x3D;</span><br><span class="line">                                Event.EventType.NodeDeleted &amp;&amp; event.getPath().equals(waitPath))</span><br><span class="line">                        &#123;</span><br><span class="line">                            waitLatch.countDown();</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        &#x2F;&#x2F; 等待连接建立</span><br><span class="line">        connectLatch.await();</span><br><span class="line">		&#x2F;&#x2F;获取根节点状态</span><br><span class="line">        Stat stat &#x3D; zk.exists(&quot;&#x2F;&quot; + rootNode, false);</span><br><span class="line">		&#x2F;&#x2F;如果根节点不存在，则创建根节点，根节点类型为永久节点</span><br><span class="line">        if (stat &#x3D;&#x3D; null) &#123;</span><br><span class="line">            System.out.println(&quot;根节点不存在&quot;);</span><br><span class="line">            zk.create(&quot;&#x2F;&quot; + rootNode, new byte[0],</span><br><span class="line">                    ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 加锁方法</span><br><span class="line">    public void zkLock() &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">			&#x2F;&#x2F;在根节点下创建临时顺序节点，返回值为创建的节点路径</span><br><span class="line">            currentNode &#x3D; zk.create(&quot;&#x2F;&quot; + rootNode + &quot;&#x2F;&quot; + subNode,</span><br><span class="line">                    null, ZooDefs.Ids.OPEN_ACL_UNSAFE,</span><br><span class="line">                    CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line">			&#x2F;&#x2F; wait 一小会, 让结果更清晰一些</span><br><span class="line">            Thread.sleep(10);</span><br><span class="line">			&#x2F;&#x2F; 注意, 没有必要监听&quot;&#x2F;locks&quot;的子节点的变化情况</span><br><span class="line">            List&lt;String&gt; childrenNodes &#x3D; zk.getChildren(&quot;&#x2F;&quot; +</span><br><span class="line">                    rootNode, false);</span><br><span class="line">			&#x2F;&#x2F; 列表中只有一个子节点, 那肯定就是 currentNode , 说明client 获得锁</span><br><span class="line">            if (childrenNodes.size() &#x3D;&#x3D; 1) &#123;</span><br><span class="line">                return;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">				&#x2F;&#x2F;对根节点下的所有临时顺序节点进行从小到大排序</span><br><span class="line">                Collections.sort(childrenNodes);</span><br><span class="line">				&#x2F;&#x2F;当前节点名称</span><br><span class="line">                String thisNode &#x3D; currentNode.substring((&quot;&#x2F;&quot; +</span><br><span class="line">                        rootNode + &quot;&#x2F;&quot;).length());</span><br><span class="line">				&#x2F;&#x2F;获取当前节点的位置</span><br><span class="line">                int index &#x3D; childrenNodes.indexOf(thisNode);</span><br><span class="line">                if (index &#x3D;&#x3D; -1) &#123;</span><br><span class="line">                    System.out.println(&quot;数据异常&quot;);</span><br><span class="line">                &#125; else if (index &#x3D;&#x3D; 0) &#123;</span><br><span class="line">					&#x2F;&#x2F; index &#x3D;&#x3D; 0, 说明 thisNode 在列表中最小, 当前client 获得锁</span><br><span class="line">                    return;</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">					&#x2F;&#x2F; 获得排名比 currentNode 前 1 位的节点</span><br><span class="line">                    this.waitPath &#x3D; &quot;&#x2F;&quot; + rootNode + &quot;&#x2F;&quot; +</span><br><span class="line">                            childrenNodes.get(index - 1);</span><br><span class="line">					&#x2F;&#x2F; 在 waitPath 上注册监听器, 当 waitPath 被删除时,zookeeper 会回调监听器的 process 方法</span><br><span class="line">                    zk.getData(waitPath, true, new Stat());</span><br><span class="line">					&#x2F;&#x2F;进入等待锁状态</span><br><span class="line">                    waitLatch.await();</span><br><span class="line">                    return;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; catch (KeeperException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; catch (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 解锁方法</span><br><span class="line">    public void zkUnlock() &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            zk.delete(this.currentNode, -1);</span><br><span class="line">        &#125; catch (InterruptedException | KeeperException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-1-Curator-框架实现分布式锁案例"><a href="#3-1-Curator-框架实现分布式锁案例" class="headerlink" title="3.1 Curator 框架实现分布式锁案例"></a>3.1 Curator 框架实现分布式锁案例</h2><ol>
<li><p>原生的 Java API 开发存在的问题</p>
<ol>
<li><strong>会话连接是异步的，需要自己去处理</strong>。比如使用 CountDownLatch</li>
<li>Watch 需要重复注册，不然就不能生效</li>
<li>开发的复杂性还是比较高的</li>
<li>不支持多节点删除和创建。需要自己去递归  </li>
</ol>
</li>
<li><p>Curator 是一个专门解决<strong>分布式锁的框架</strong>，解决了原生 Java API 开发分布式遇到的问题。<br>详情请查看官方文档： <a href="https://curator.apache.org/index.html" target="_blank" rel="noopener">https://curator.apache.org/index.html</a>  </p>
</li>
</ol>
<h1 id="4-面试重点"><a href="#4-面试重点" class="headerlink" title="4 面试重点"></a>4 面试重点</h1><h2 id="4-1-选举机制"><a href="#4-1-选举机制" class="headerlink" title="4.1 选举机制"></a>4.1 选举机制</h2><p><strong>半数机制，超过半数的投票通过，即通过</strong>。</p>
<ol>
<li>第一次启动选举规则：<br>投票过半数时， 服务器 id 大的胜出</li>
<li>第二次启动选举规则：<ol>
<li>EPOCH 大的直接胜出</li>
<li>EPOCH 相同，事务 id 大的胜出</li>
<li>事务 id 相同，服务器 id 大的胜出  </li>
</ol>
</li>
</ol>
<h2 id="4-2-生产集群安装多少zk合适"><a href="#4-2-生产集群安装多少zk合适" class="headerlink" title="4.2 生产集群安装多少zk合适"></a>4.2 生产集群安装多少zk合适</h2><p>安装<strong>奇数台</strong>。<br>生产经验：</p>
<ul>
<li>10 台服务器： 3 台 zk；</li>
<li>20 台服务器： 5 台 zk；</li>
<li>100 台服务器： 11 台 zk；</li>
<li>200 台服务器： 11 台 zk<br>服务器台数多：好处，提高可靠性；坏处：提高通信延时  </li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://extrali.com/2021/08/28/Hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="黎达">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Extrali">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/28/Hive/" class="post-title-link" itemprop="url">Hive</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>

              <time title="Erstellt: 2021-08-28 16:33:48" itemprop="dateCreated datePublished" datetime="2021-08-28T16:33:48+08:00">2021-08-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Bearbeitet am</span>
                <time title="Geändert am: 2021-08-29 10:01:02" itemprop="dateModified" datetime="2021-08-29T10:01:02+08:00">2021-08-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">in</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-Hive基础概念"><a href="#1-Hive基础概念" class="headerlink" title="1 Hive基础概念"></a>1 Hive基础概念</h1><h2 id="1-1-什么是Hive"><a href="#1-1-什么是Hive" class="headerlink" title="1.1 什么是Hive"></a>1.1 什么是Hive</h2><ol>
<li><p>hive简介</p>
<ul>
<li>Hive：由 Facebook 开源用于解决海量<strong>结构化</strong>日志的数据统计工具。</li>
<li>Hive 是基于 Hadoop 的一个数据仓库工具，可以将<strong>结构化的数据文件映射为一张表</strong>，并提供<strong>类 SQL</strong> 查询功能。  </li>
</ul>
</li>
<li><p>hive本质</p>
<p>将 HQL 转化成 MapReduce 程序 </p>
<ul>
<li>Hive 处理的数据存储在 HDFS </li>
<li>Hive 分析数据底层的实现是 MapReduce </li>
<li>执行程序运行在 Yarn 上 </li>
</ul>
</li>
</ol>
<h2 id="1-2-Hive优缺点"><a href="#1-2-Hive优缺点" class="headerlink" title="1.2 Hive优缺点"></a>1.2 Hive优缺点</h2><h3 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h3><ol>
<li>操作接口采用<strong>类 SQL</strong> 语法，提供快速开发的能力（简单、容易上手）。</li>
<li>避免了去写 MapReduce，减少开发人员的学习成本。</li>
<li>Hive 的执行延迟比较高，因此 Hive 常用于数据分析，对实时性要求不高的场合。</li>
<li>Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。</li>
<li>Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。  </li>
</ol>
<h3 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h3><ol>
<li>Hive 的 HQL 表达能力有限 <ul>
<li>迭代式算法无法表达。</li>
<li>数据挖掘方面不擅长， 由于 MapReduce 数据处理流程的限制，效率更高的算法却无法实现。  </li>
</ul>
</li>
<li>Hive的效率比较低<ul>
<li>Hive 自动生成的 MapReduce 作业，通常情况下不够智能化  </li>
<li>Hive 调优比较困难，粒度较粗  </li>
</ul>
</li>
</ol>
<h2 id="1-3-Hive架构"><a href="#1-3-Hive架构" class="headerlink" title="1.3 Hive架构"></a>1.3 Hive架构</h2><p><img src="https://i.loli.net/2021/08/28/R8zTNq4bIrVpcmu.png" alt="image-20210828164005216"></p>
<ol>
<li><p>用户接口：client</p>
<p>CLI（command-line interface）、 JDBC/ODBC(jdbc 访问 hive)、 WEBUI（浏览器访问 hive）  </p>
</li>
<li><p>元数据：Metastore</p>
<p>元数据包括：表名、表所属的数据库（默认是 default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；  </p>
<p>*<em>默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore  *</em></p>
</li>
<li><p>Hadoop </p>
<p>使用 HDFS 进行存储，使用 MapReduce 进行计算。  </p>
</li>
<li><p>驱动器：Driver</p>
<ul>
<li>解析器（SQL Parser）：将 SQL 字符串转换成抽象语法树 AST，这一步一般都用第三方工具库完成，比如 antlr；对 AST 进行语法分析，比如表是否存在、字段是否存在、 SQL语义是否有误。</li>
<li>编译器（Physical Plan）：将 AST 编译生成逻辑执行计划。</li>
<li>优化器（Query Optimizer）：对逻辑执行计划进行优化。</li>
<li>执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来说， 就是 MR/Spark。  </li>
</ul>
</li>
</ol>
<h1 id="2-Hive数据类型"><a href="#2-Hive数据类型" class="headerlink" title="2 Hive数据类型"></a>2 Hive数据类型</h1><h2 id="2-1-基本数据类型"><a href="#2-1-基本数据类型" class="headerlink" title="2.1 基本数据类型"></a>2.1 基本数据类型</h2><table>
<thead>
<tr>
<th>Hive 数据类型</th>
<th>Java 数据类型</th>
<th>长度</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte 有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte 有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte 有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte 有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型， true 或者 false</td>
<td>TRUE FALSE</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字 符集。可以使用单引号或者双 引号。</td>
<td>‘ now is the time ’ “for all good men”</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td>时间类型</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BINARY</td>
<td>字节数组</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>对于 Hive 的 String 类型相当于数据库的 varchar 类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储 2GB 的字符数。 </p>
<h2 id="2-2-集合数据类型"><a href="#2-2-集合数据类型" class="headerlink" title="2.2 集合数据类型"></a>2.2 集合数据类型</h2><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和 c 语言中的 struct 类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是 STRUCT{first STRING, last STRING},那么第 1 个元素可以通过字段.first 来 引用。</td>
<td>struct() 例 如 struct&lt;street:string, city:string&gt;</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP 是一组键-值对元组集合，使用数组表示法可以 访问数据。例如，如果某个列的数据类型是 MAP，其中键 -&gt;值对是’ first’ -&gt;’ John’和’ last’ -&gt;’ Doe’，那么可以 通过字段名[‘last’ ]获取最后一个元素</td>
<td>map() 例如 map&lt;string, int&gt;</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些 变量称为数组的元素，每个数组元素都有一个编号，编号从 零开始。例如，数组值为[‘John’ , ‘Doe’ ]，那么第 2 个 元素可以通过数组名[1]进行引用。</td>
<td>Array() 例如 array<string></td>
</tr>
</tbody></table>
<p>Hive 有三种复杂数据类型 ARRAY、 MAP 和 STRUCT。 ARRAY 和 MAP 与 Java 中的 Array和 Map 类似，而 STRUCT 与 C 语言中的 Struct 类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。  </p>
<ol>
<li><p>案例实操</p>
<ol>
<li><p>假设某表有如下一行， 我们用 JSON 格式来表示其数据结构。在 Hive 下访问的格式为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;name&quot;: &quot;songsong&quot;,</span><br><span class="line">    &quot;friends&quot;: [&quot;bingbing&quot; , &quot;lili&quot;] , &#x2F;&#x2F;列表 Array,</span><br><span class="line">    &quot;children&quot;: &#123; &#x2F;&#x2F;键值 Map,</span><br><span class="line">        &quot;xiao song&quot;: 18 ,</span><br><span class="line">        &quot;xiaoxiao song&quot;: 19</span><br><span class="line">    &#125;</span><br><span class="line">    &quot;address&quot;: &#123; &#x2F;&#x2F;结构 Struct,</span><br><span class="line">        &quot;street&quot;: &quot;hui long guan&quot;,</span><br><span class="line">        &quot;city&quot;: &quot;beijing&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>基于上述数据结构， 我们在 Hive 里创建对应的表， 并导入数据。  </p>
<p>创建本地测试文件 test.txt </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure>

<p>注意： MAP， STRUCT 和 ARRAY 里的元素间关系都可以用同一个字符表示， 这里用“_”。  </p>
</li>
<li><p>Hive 上创建测试表 test  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table test(</span><br><span class="line">    name string,</span><br><span class="line">    friends array&lt;string&gt;,</span><br><span class="line">    children map&lt;string, int&gt;,</span><br><span class="line">    address struct&lt;street:string, city:string&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;,&#39;</span><br><span class="line">collection items terminated by &#39;_&#39;</span><br><span class="line">map keys terminated by &#39;:&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure>

<p>字段解释：</p>
<p>row format delimited fields terminated by ‘,’             – 列分隔符 </p>
<p>collection items terminated by ‘_’                                 –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p>
<p>map keys terminated by ‘:’                                             – MAP 中的 key 与 value 的分隔符<br>lines terminated by ‘\n’;                                                  – 行分隔符</p>
</li>
<li><p>导入文本数据到测试表</p>
<p><code>load data local inpath &#39;/opt/module/hive/datas/test.txt&#39; into table test;</code></p>
</li>
<li><p>访问三种集合列里的数据，以下分别是 ARRAY， MAP， STRUCT 的访问方式 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select friends[1],children[&#39;xiao song&#39;],address.city from</span><br><span class="line">test where name&#x3D;&quot;songsong&quot;;</span><br><span class="line">OK</span><br><span class="line">_c0 _c1 city</span><br><span class="line">lili 18 beijing</span><br><span class="line">Time taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h2 id="2-3-类型转化"><a href="#2-3-类型转化" class="headerlink" title="2.3 类型转化"></a>2.3 类型转化</h2><p>Hive 的原子数据类型是可以进行<strong>隐式转换</strong>的，类似于 Java 的类型转换，例如某表达式使用 INT 类型， TINYINT 会自动转换为 INT 类型，但是 Hive 不会进行反向转化，例如，某表达式使用 TINYINT 类型， INT 不会自动转换为 TINYINT 类型，它会返回错误，除非使用 CAST<br>操作。  </p>
<ol>
<li><p>隐式类型转化规则</p>
<ul>
<li>任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成INT， INT 可以转换成 BIGINT。</li>
<li>所有整数类型、 FLOAT 和 STRING 类型都可以隐式地转换成 DOUBLE。</li>
<li>TINYINT、 SMALLINT、 INT 都可以转换为 FLOAT。</li>
<li>BOOLEAN 类型不可以转换为任何其它的类型。  </li>
</ul>
</li>
<li><p>可以使用 <strong>CAST</strong> 操作显示进行数据类型转换  </p>
<p>例如 CAST(‘1’ AS INT)将把字符串’1’ 转换成整数 1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。  </p>
</li>
</ol>
<h1 id="3-DDL数据定义"><a href="#3-DDL数据定义" class="headerlink" title="3 DDL数据定义"></a>3 DDL数据定义</h1><h2 id="3-1-创建表"><a href="#3-1-创建表" class="headerlink" title="3.1 创建表"></a>3.1 创建表</h2><ol>
<li><p>建表语法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name</span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">[COMMENT table_comment]</span><br><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">[CLUSTERED BY (col_name, col_name, ...)</span><br><span class="line">[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</span><br><span class="line">[ROW FORMAT row_format]</span><br><span class="line">[STORED AS file_format]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name&#x3D;property_value, ...)]</span><br><span class="line">[AS select_statement]</span><br></pre></td></tr></table></figure>
</li>
<li><p>字段解释说明</p>
<ul>
<li><p><strong>EXTERNAL</strong> 关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION） ， 在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。  </p>
</li>
<li><p><strong>PARTITIONED BY</strong> 创建分区表  </p>
</li>
<li><p><strong>CLUSTERED BY</strong> 创建分桶表  </p>
</li>
<li><p>ROW FORMAT DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]<br>[MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]<br>| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,<br>property_name=property_value, …)]  </p>
<p><strong>SerDe 是 Serialize/Deserilize 的简称， hive 使用 Serde 进行行对象的序列与反序列化。</strong>  </p>
</li>
<li><p>STORED AS 指定存储文件类型<br>常用的存储文件类型： SEQUENCEFILE（二进制序列文件）、 TEXTFILE（文本）、 RCFILE（列式存储格式文件）<br>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</p>
</li>
<li><p>LOCATION ：指定表在 HDFS 上的存储位置。</p>
</li>
<li><p>AS：后跟查询语句， 根据查询结果创建表。</p>
</li>
<li><p>LIKE 允许用户复制现有的表结构，但是不复制数据。    </p>
</li>
</ul>
</li>
</ol>
<h1 id="4-DML数据操作"><a href="#4-DML数据操作" class="headerlink" title="4. DML数据操作"></a>4. DML数据操作</h1><h2 id="4-1-数据导入"><a href="#4-1-数据导入" class="headerlink" title="4.1 数据导入"></a>4.1 数据导入</h2><h3 id="4-1-1-向表中装载数据（Load）"><a href="#4-1-1-向表中装载数据（Load）" class="headerlink" title="4.1.1 向表中装载数据（Load）"></a>4.1.1 向表中装载数据（Load）</h3><ol>
<li><p>语法</p>
<p><code>load data [local] inpath &#39;数据的 path&#39; [overwrite] into table student [partition (partcol1=val1,…)];</code></p>
</li>
</ol>
<h3 id="4-1-2-创建表时通过-Location-指定加载数据路径"><a href="#4-1-2-创建表时通过-Location-指定加载数据路径" class="headerlink" title="4.1.2 创建表时通过 Location 指定加载数据路径"></a>4.1.2 创建表时通过 Location 指定加载数据路径</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create external table if not exists student5(</span><br><span class="line">id int, name string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br><span class="line">location &#39;&#x2F;student;</span><br></pre></td></tr></table></figure>

<h3 id="4-1-3-Import-数据到指定-Hive-表中"><a href="#4-1-3-Import-数据到指定-Hive-表中" class="headerlink" title="4.1.3 Import 数据到指定 Hive 表中"></a>4.1.3 Import 数据到指定 Hive 表中</h3><p><code>import table student2 from &#39;/user/hive/warehouse/export/student&#39;;</code></p>
<h2 id="4-2-数据导出"><a href="#4-2-数据导出" class="headerlink" title="4.2 数据导出"></a>4.2 数据导出</h2><h3 id="4-2-1-Insert导出"><a href="#4-2-1-Insert导出" class="headerlink" title="4.2.1 Insert导出"></a>4.2.1 Insert导出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory</span><br><span class="line">&#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;data&#x2F;export&#x2F;student1&#39;</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;</span><br><span class="line">select * from student;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-2-Hive-Shell命令导出"><a href="#4-2-2-Hive-Shell命令导出" class="headerlink" title="4.2.2 Hive Shell命令导出"></a>4.2.2 Hive Shell命令导出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin&#x2F;hive -e &#39;select * from default.student;&#39; &gt;</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;hive&#x2F;data&#x2F;export&#x2F;student4.txt;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-3-Export导出到HDFS上"><a href="#4-2-3-Export导出到HDFS上" class="headerlink" title="4.2.3 Export导出到HDFS上"></a>4.2.3 Export导出到HDFS上</h3><p><code>(defahiveult)&gt; export table default.student to &#39;/user/hive/warehouse/export/student&#39;;</code></p>
<h1 id="5-查询"><a href="#5-查询" class="headerlink" title="5. 查询"></a>5. 查询</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT [ALL | DISTINCT] select_expr, select_expr, ...</span><br><span class="line">FROM table_reference</span><br><span class="line">[WHERE where_condition]</span><br><span class="line">[GROUP BY col_list]</span><br><span class="line">[ORDER BY col_list]</span><br><span class="line">[CLUSTER BY col_list</span><br><span class="line">| [DISTRIBUTE BY col_list] [SORT BY col_list]</span><br><span class="line">]</span><br><span class="line">[LIMIT number]</span><br></pre></td></tr></table></figure>

<h2 id="5-1-每个-Reduce-内部排序（Sort-By）"><a href="#5-1-每个-Reduce-内部排序（Sort-By）" class="headerlink" title="5.1 每个 Reduce 内部排序（Sort By）"></a>5.1 每个 Reduce 内部排序（Sort By）</h2><p>Sort By： 对于大规模的数据集 order by 的效率非常低。在很多情况下， 并不需要全局排序， 此时可以使用 sort by。<br><strong>Sort by 为每个 reducer 产生一个排序文件。 每个 Reducer 内部进行排序， 对全局结果集来说不是排序。</strong>  </p>
<h2 id="5-2-分区-Distribute-by"><a href="#5-2-分区-Distribute-by" class="headerlink" title="5.2 分区(Distribute by)"></a>5.2 分区(Distribute by)</h2><p>Distribute By： 在有些情况下， 我们需要<strong>控制某个特定行应该到哪个 reducer</strong>， 通常是为了进行后续的聚集操作。 distribute by 子句可以做这件事。 distribute by 类似 MR 中 partition（自定义分区） ，进行分区，结合 sort by 使用。  </p>
<ol>
<li><p>案例实操</p>
<ul>
<li><p>先按照部门编号分区，再按照员工编号降序排序。 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces&#x3D;3;</span><br><span class="line">hive (default)&gt; insert overwrite local directory</span><br><span class="line">&#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;distribute-result&#39; </span><br><span class="line">select * from emp </span><br><span class="line">distribute by deptno </span><br><span class="line">sort by empno desc;</span><br></pre></td></tr></table></figure>

<ul>
<li>distribute by 的分区规则是根据分区字段的 hash 码与 reduce 的个数进行模除后，余数相同的分到一个区。</li>
<li>Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前。  </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="5-3-Cluster-By"><a href="#5-3-Cluster-By" class="headerlink" title="5.3 Cluster By"></a>5.3 Cluster By</h2><p>当 distribute by 和 sorts by 字段相同时，可以使用 cluster by 方式。<br>cluster by 除了具有 distribute by 的功能外还兼具 sort by 的功能。但是排序只能是升序排序， 不能指定排序规则为 ASC 或者 DESC。  </p>
<ul>
<li><p>以下两种写法等价 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line">hive (default)&gt; select * from emp distribute by deptno sort by deptno;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h1 id="6-分区表和分桶表"><a href="#6-分区表和分桶表" class="headerlink" title="6 分区表和分桶表"></a>6 分区表和分桶表</h1><h2 id="6-1-分区表"><a href="#6-1-分区表" class="headerlink" title="6.1 分区表"></a>6.1 分区表</h2><p>分区表实际上就是对应一个 HDFS 文件系统上的独立的<strong>文件夹</strong>，该文件夹下是该分区所有的数据文件。 Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率<br>会提高很多。  </p>
<h3 id="6-1-1-分区表基本操作"><a href="#6-1-1-分区表基本操作" class="headerlink" title="6.1.1 分区表基本操作"></a>6.1.1 分区表基本操作</h3><ol>
<li><p>引入分区表（需要根据日期对日志进行管理, 通过部门信息模拟）  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dept_20200401.log</span><br><span class="line">dept_20200402.log</span><br><span class="line">dept_20200403.log</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建分区表语法 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition(</span><br><span class="line">deptno int, dname string, loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (day string)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br></pre></td></tr></table></figure>

<p>注意：<strong>分区字段不能是表中已经存在的数据</strong>，可以将分区字段看作表的伪列。</p>
</li>
<li><p>加载数据到分区表中</p>
<ol>
<li><p>数据准备</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">dept_20200401.log</span><br><span class="line">10 ACCOUNTING 1700</span><br><span class="line">20 RESEARCH 1800</span><br><span class="line"></span><br><span class="line">dept_20200402.log</span><br><span class="line">30 SALES 1900</span><br><span class="line">40 OPERATIONS 1700</span><br><span class="line"></span><br><span class="line">dept_20200403.log</span><br><span class="line">50 TEST 2000</span><br><span class="line">60 DEV 1900</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath</span><br><span class="line">&#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;dept_20200401.log&#39; into table dept_partition</span><br><span class="line">partition(day&#x3D;&#39;20200401&#39;);</span><br><span class="line">hive (default)&gt; load data local inpath</span><br><span class="line">&#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;dept_20200402.log&#39; into table dept_partition</span><br><span class="line">partition(day&#x3D;&#39;20200402&#39;);</span><br><span class="line">hive (default)&gt; load data local inpath</span><br><span class="line">&#39;&#x2F;opt&#x2F;module&#x2F;hive&#x2F;datas&#x2F;dept_20200403.log&#39; into table dept_partition</span><br><span class="line">partition(day&#x3D;&#39;20200403&#39;);</span><br></pre></td></tr></table></figure>

<p>注意：分区表加载数据时，必须指定分区  </p>
</li>
<li><p>查询分区中的数据</p>
<p><code>hive (default)&gt; select * from dept_partition where day=&#39;20200401&#39;;</code></p>
</li>
</ol>
</li>
</ol>
<h2 id="6-2-分桶表"><a href="#6-2-分桶表" class="headerlink" title="6.2 分桶表"></a>6.2 分桶表</h2><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。 对于一张表或者分区， Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。  </p>
<p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。<br><strong>分区针对的是数据的存储路径；分桶针对的是数据文件。</strong>  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create table stu_buck(id int, name string)</span><br><span class="line">clustered by(id)</span><br><span class="line">into 4 buckets</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure>

<p>*<em>Hive 的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中  *</em></p>
<h2 id="6-3-抽样调查"><a href="#6-3-抽样调查" class="headerlink" title="6.3 抽样调查"></a>6.3 抽样调查</h2><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。 Hive 可以通过对表进行抽样来满足这个需求。<br>语法: <code>TABLESAMPLE(BUCKET x OUT OF y)</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">查询表 stu_buck 中的数据。</span><br><span class="line">hive (default)&gt; select * from stu_buck tablesample(bucket 1 out of 4 on id);</span><br></pre></td></tr></table></figure>

<h1 id="7-函数"><a href="#7-函数" class="headerlink" title="7 函数"></a>7 函数</h1><h2 id="7-1-系统内置函数"><a href="#7-1-系统内置函数" class="headerlink" title="7.1 系统内置函数"></a>7.1 系统内置函数</h2><ol>
<li><p>查看系统内置函数</p>
<p><code>hive&gt; show functions;</code></p>
</li>
<li><p>显示自带函数的用法</p>
<p><code>hive&gt; desc function upper;</code></p>
</li>
<li><p>详细显示自带函数的用法</p>
<p><code>hive&gt; desc function extended upper;</code></p>
</li>
</ol>
<h2 id="7-2-常用内置函数"><a href="#7-2-常用内置函数" class="headerlink" title="7.2 常用内置函数"></a>7.2 常用内置函数</h2><h3 id="7-2-1-空字段赋值"><a href="#7-2-1-空字段赋值" class="headerlink" title="7.2.1 空字段赋值"></a>7.2.1 空字段赋值</h3><ol>
<li><p>函数说明</p>
<p><strong>NVL</strong>： 给值为 NULL 的数据赋值， 它的格式是 NVL( value， default_value)。它的功能是如果 value 为 NULL， 则 NVL 函数返回 default_value 的值， 否则返回 value 的值， 如果两个参数都为 NULL ， 则返回 NULL。  </p>
</li>
</ol>
<h3 id="7-2-2-CASE-WHEN-THEN-ELSE-END"><a href="#7-2-2-CASE-WHEN-THEN-ELSE-END" class="headerlink" title="7.2.2 CASE WHEN THEN ELSE END"></a>7.2.2 CASE WHEN THEN ELSE END</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">dept_id,</span><br><span class="line">sum(case sex when &#39;男&#39; then 1 else 0 end) male_count,</span><br><span class="line">sum(case sex when &#39;女&#39; then 1 else 0 end) female_count</span><br><span class="line">from emp_sex</span><br><span class="line">group by dept_id;</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://extrali.com/2021/08/24/Kafka/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="黎达">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Extrali">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/24/Kafka/" class="post-title-link" itemprop="url">Kafka</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>

              <time title="Erstellt: 2021-08-24 23:32:08" itemprop="dateCreated datePublished" datetime="2021-08-24T23:32:08+08:00">2021-08-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Bearbeitet am</span>
                <time title="Geändert am: 2021-08-29 16:33:18" itemprop="dateModified" datetime="2021-08-29T16:33:18+08:00">2021-08-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">in</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-Kafka概述"><a href="#1-Kafka概述" class="headerlink" title="1 Kafka概述"></a>1 Kafka概述</h1><h2 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h2><p>Kafka 是一个<strong>分布式</strong>的基于<strong>发布/订阅模式</strong>的<strong>消息队列</strong>（Message Queue） ， 主要应用于<strong>大数据实时</strong>处理领域。  </p>
<h2 id="1-2-消息队列"><a href="#1-2-消息队列" class="headerlink" title="1.2 消息队列"></a>1.2 消息队列</h2><h3 id="1-2-1-传统消息队列的应用场景"><a href="#1-2-1-传统消息队列的应用场景" class="headerlink" title="1.2.1 传统消息队列的应用场景"></a>1.2.1 传统消息队列的应用场景</h3><p><img src="https://i.loli.net/2021/08/29/bhaXuFQUj8poE4A.png" alt="image-20210829100457579">、</p>
<p><strong>使用消息队列好处</strong></p>
<ol>
<li><p><strong>解耦</strong>  </p>
<p>允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p>
</li>
<li><p><strong>可恢复性</strong><br>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。  </p>
</li>
<li><p><strong>缓冲</strong>  </p>
<p>有助于控制和优化数据流经过系统的速度， 解决生产消息和消费消息的处理速度不一致的情况。 </p>
</li>
<li><p><strong>灵活性 &amp; 峰值处理能力</strong><br>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。  </p>
</li>
<li><p>异步通信</p>
<p>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。  </p>
</li>
</ol>
<h3 id="1-2-2-消息队列的两种模式"><a href="#1-2-2-消息队列的两种模式" class="headerlink" title="1.2.2 消息队列的两种模式"></a>1.2.2 消息队列的两种模式</h3><ol>
<li><p><strong>点对点模式</strong>（一对一，消费者主动拉取数据，消息收到后消息清除）  </p>
<p>消息生产者生产消息发送到Queue中， 然后消息消费者从Queue中取出并且消费消息。消息被消费以后， queue 中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue 支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。  </p>
<p><img src="https://i.loli.net/2021/08/29/vKHwqUcN1F4riJn.png" alt="image-20210829100905235"></p>
</li>
<li><p><strong>发布/订阅模式</strong>（一对多，消费者消费数据之后不会清除消息 ）</p>
<p>消息生产者（发布）将消息发布到 topic 中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到 topic 的消息会被所有订阅者消费。  </p>
<p><img src="https://i.loli.net/2021/08/29/fXHZ8NrcVTFghRm.png" alt="image-20210829101011829"></p>
</li>
</ol>
<h2 id="1-3-Kafka基础架构"><a href="#1-3-Kafka基础架构" class="headerlink" title="1.3 Kafka基础架构"></a>1.3 Kafka基础架构</h2><p><img src="https://i.loli.net/2021/08/29/BwG1mf6K9zP7A2H.png" alt="image-20210829101042834"></p>
<ol>
<li><strong>Producer</strong> ： 消息生产者，就是向 kafka broker 发消息的客户端；</li>
<li><strong>Consumer</strong> ： 消息消费者，向 kafka broker 取消息的客户端；</li>
<li><strong>Consumer Group （CG）</strong>： 消费者组，由多个 consumer 组成。 消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。 所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li>
<li><strong>Broker</strong> ： 一台 kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker可以容纳多个 topic。 </li>
<li><strong>Topic</strong> ： 可以理解为一个队列， 生产者和消费者面向的都是一个 topic；</li>
<li><strong>Partition</strong>： 为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，<strong>一个 topic 可以分为多个 partition</strong>，每个 partition 是一个有序的队列； </li>
<li><strong>Replica</strong>： 副本，为保证集群中的某个节点发生故障时， 该节点上的 partition 数据不丢失，且 kafka 仍然能够继续工作， kafka 提供了副本机制，一个 topic 的每个分区都有若干个副本，一个 <strong>leader</strong> 和若干个 <strong>follower</strong>。  </li>
<li><strong>leader</strong>： 每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 leader。</li>
<li><strong>follower</strong>： 每个分区多个副本中的“从”，实时从 leader 中同步数据，保持和 leader 数据的同步。 leader 发生故障时，某个 follower 会成为新的 follower。  </li>
</ol>
<h1 id="2-Kafka架构深入"><a href="#2-Kafka架构深入" class="headerlink" title="2 Kafka架构深入"></a>2 Kafka架构深入</h1><h2 id="2-1-Kafka工作流程及文件存储机制"><a href="#2-1-Kafka工作流程及文件存储机制" class="headerlink" title="2.1 Kafka工作流程及文件存储机制"></a>2.1 Kafka工作流程及文件存储机制</h2><p><img src="https://i.loli.net/2021/08/29/1B7DeiXFnAdz94R.png" alt="image-20210829101614958"></p>
<p>Kafka 中消息是以 topic 进行分类的， 生产者生产消息，消费者消费消息，都是面向 topic的。  </p>
<p>topic 是逻辑上的概念，而 partition 是物理上的概念，<strong>每个 partition 对应于一个 log 文件</strong>，该 log 文件中存储的就是 producer 生产的数据。 Producer 生产的数据会被不断追加到该log 文件末端，且每条数据都有自己的 offset。 消费者组中的每个消费者， 都会实时记录自己消费到了哪个 offset，以便出错恢复时，从上次的位置继续消费。  </p>
<p><img src="https://i.loli.net/2021/08/29/nim9obKMrXIlBu5.png" alt="image-20210829101808522"></p>
<p>由于生产者生产的消息会不断追加到 log 文件末尾， 为防止 log 文件过大导致数据定位效率低下， Kafka 采取了分片和索引机制，将每个 partition 分为多个 <strong>segment</strong>。 每个 segment对应两个文件——“.index”文件和“.log”文件。 这些文件位于一个文件夹下， 该文件夹的命名规则为： topic 名称+分区序号。例如， first 这个 topic 有三个分区，则其对应的文件夹为 first-0,first-1,first-2。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">00000000000000000000.index</span><br><span class="line">00000000000000000000.log</span><br><span class="line">00000000000000170410.index</span><br><span class="line">00000000000000170410.log</span><br><span class="line">00000000000000239430.index</span><br><span class="line">00000000000000239430.log</span><br></pre></td></tr></table></figure>

<p>index 和 log 文件以<strong>当前 segment 的第一条消息的 offset 命名</strong>。下图为 index 文件和 log文件的结构示意图。  </p>
<p><img src="https://i.loli.net/2021/08/29/CjRaA2wIP4OzFUm.png" alt="image-20210829101927517"></p>
<p>“.index”文件存储大量的索引信息，“.log”文件存储大量的数据，<strong>索引文件中的元数据指向对应数据文件中 message 的物理偏移地址</strong>。  </p>
<h2 id="2-2-Kafka生产者"><a href="#2-2-Kafka生产者" class="headerlink" title="2.2 Kafka生产者"></a>2.2 Kafka生产者</h2><h3 id="2-2-1-分区策略"><a href="#2-2-1-分区策略" class="headerlink" title="2.2.1 分区策略"></a>2.2.1 分区策略</h3><ol>
<li><p>分区的原因</p>
<ul>
<li><strong>方便在集群中扩展</strong>，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic又可以有多个 Partition 组成，因此整个集群就可以适应任意大小的数据了；</li>
<li>可以<strong>提高并发</strong>，因为可以以 Partition 为单位读写了。  </li>
</ul>
</li>
<li><p>分区的原因</p>
<p>我们需要将 producer 发送的数据封装成一个 <strong>ProducerRecord</strong> 对象。  </p>
<p><img src="https://i.loli.net/2021/08/29/XYTHAUvksoMIVat.png" alt="image-20210829102228983"></p>
<ul>
<li>指明 partition 的情况下，直接将指明的值直接作为 partiton 值；  </li>
<li>没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition数进行取余得到 partition 值；</li>
<li>既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition值，也就是常说的 <strong>round-robin</strong> 算法。  </li>
</ul>
</li>
</ol>
<h3 id="2-2-2-数据可靠性保证"><a href="#2-2-2-数据可靠性保证" class="headerlink" title="2.2.2 数据可靠性保证"></a>2.2.2 数据可靠性保证</h3><p>为保证 producer 发送的数据，能可靠的发送到指定的 topic， <strong>topic</strong> 的每个 partition 收到producer 发送的数据后， 都需要向 producer 发送 <strong>ack</strong>（acknowledgement 确认收到） ，<strong>如果producer 收到 ack， 就会进行下一轮的发送，否则重新发送数据</strong>。  </p>
<p><img src="https://i.loli.net/2021/08/29/TasCyYfQ5uBELIJ.png" alt="image-20210829102436502"></p>
<ol>
<li><p>副本数据同步策略</p>
<table>
<thead>
<tr>
<th>方案</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>半数以上完成同步， 就发 送 ack</td>
<td>延迟低</td>
<td>选举新的 leader 时， 容忍 n 台 节点的故障，需要 2n+1 个副 本</td>
</tr>
<tr>
<td>全部完成同步，才发送 ack</td>
<td>选举新的 leader 时， 容忍 n 台 节点的故障，需要 n+1 个副 本</td>
<td>延迟高</td>
</tr>
</tbody></table>
<p>Kafka 选择了第二种方案，原因如下：<br>1.同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1个副本，而 Kafka 的每个分区都有大量的数据， 第一种方案会造成大量数据的冗余。<br>2.虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小。  </p>
</li>
<li><p><strong>ISR</strong></p>
<p>采用第二种方案之后，设想以下情景： leader 收到数据，所有 follower 都开始同步数据，但有一个 follower，因为某种故障，迟迟不能与 leader 进行同步，那 leader 就要一直等下去，直到它完成同步，才能发送 ack。这个问题怎么解决呢？ </p>
<p>Leader 维护了一个动态的 <strong>in-sync replica set (ISR)</strong>，意为和 leader 保持同步的 follower 集合。当 ISR 中的 follower 完成数据的同步之后， leader 就会给 follower 发送 ack。如果 follower长 时 间 未 向 leader 同 步 数 据 ， 则 该 follower 将 被 踢 出 ISR ， 该 时 间 阈 值 由replica.lag.time.max.ms 参数设定。 Leader 发生故障之后，就会从 ISR 中选举新的 leader。  </p>
</li>
<li><p><strong>ack 应答机制</strong><br>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等 ISR 中的 follower 全部接收成功。所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。  </p>
<p><strong>acks 参数配置：</strong><br>acks：<br><strong>0</strong>： producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟， broker 一接收到还没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据；<br><strong>1</strong>： producer 等待 broker 的 ack， partition 的 leader 落盘成功后返回 ack，如果在 follower同步成功之前 leader 故障，那么将会丢失数据；  </p>
<p><strong>-1（all）</strong> ： producer 等待 broker 的 ack， partition 的 leader 和 follower 全部落盘成功后才返回 ack。但是如果在 follower 同步完成后， broker 发送 ack 之前， leader 发生故障，那么会造成数据重复。  </p>
<p><img src="https://i.loli.net/2021/08/29/j5esYdzm1DOBcxg.png" alt="image-20210829103122680"></p>
</li>
<li><p><strong>故障处理细节</strong></p>
<p><img src="https://i.loli.net/2021/08/29/dZvJMmHUViKA2hN.png" alt="image-20210829103213045"></p>
<p><strong>LEO：指的是每个副本最大的 offset；<br>HW：指的是消费者能见到的最大的 offset， ISR 队列中最小的 LEO。</strong>    </p>
<ol>
<li><p><strong>follower 故障</strong><br>follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后， follower 会读取本地磁盘<strong>记录的上次的 HW</strong>，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。<strong>等该 follower 的 LEO 大于等于该 Partition 的 HW</strong>，即 follower 追上 leader 之后，就可以重新加入 ISR 了。  </p>
</li>
<li><p><strong>leader 故障</strong><br>leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的 数据一致性， <strong>其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉</strong>，然后从新的 leader同步数据。  </p>
</li>
</ol>
<p>*<em>注意： 这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。  *</em></p>
</li>
</ol>
<h3 id="2-2-3-Exactly-Once-语义"><a href="#2-2-3-Exactly-Once-语义" class="headerlink" title="2.2.3 Exactly Once 语义"></a>2.2.3 Exactly Once 语义</h3><p>将服务器的 ACK 级别设置为-1，可以保证 Producer 到 Server 之间不会丢失数据，即 <strong>AtLeast Once</strong> 语义。相对的，将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被发送一次，即 <strong>At Most Once</strong> 语义。  </p>
<p>At Least Once 可以保证数据不丢失，但是不能保证数据不重复；相对的， At Most Once可以保证数据不重复，但是不能保证数据不丢失。 但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即 <strong>Exactly Once</strong> 语义。 在 0.11 版本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。  </p>
<p>0.11 版本的 Kafka，引入了一项重大特性：<strong>幂等性</strong>。所谓的幂等性就是指 Producer 不论向 Server 发送多少次重复数据， Server 端都只会持久化一条。幂等性结合 At Least Once 语义，就构成了 Kafka 的 Exactly Once 语义。即： </p>
<p>​                                                                <strong>At Least Once + 幂等性 = Exactly Once</strong></p>
<p>要启用幂等性，只需要将 Producer 的参数中 <strong>enable.idompotence</strong> 设置为 true 即可。 Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而Broker 端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时， Broker 只会持久化一条。    </p>
<p>但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以<strong>幂等性无法保证跨分区跨会话的 Exactly Once</strong>。  </p>
<h2 id="2-3-Kafka消费者"><a href="#2-3-Kafka消费者" class="headerlink" title="2.3 Kafka消费者"></a>2.3 Kafka消费者</h2><h3 id="2-3-1-消费方式"><a href="#2-3-1-消费方式" class="headerlink" title="2.3.1 消费方式"></a>2.3.1 消费方式</h3><p>consumer 采用 <strong>pull（拉）</strong> 模式从 broker 中读取数据。<br>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 <strong>pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息</strong>。 </p>
<p><strong>pull 模式不足之处是，如果 kafka 没有数据，消费者可能会陷入循环中， 一直返回空数据</strong>。 针对这一点， Kafka 的消费者在消费数据时会传入一个时长参数 timeout，如果当前没有数据可供消费， consumer 会等待一段时间之后再返回，这段时长即为 timeout。  </p>
<h3 id="2-3-2-分区分配策略"><a href="#2-3-2-分区分配策略" class="headerlink" title="2.3.2 分区分配策略"></a>2.3.2 分区分配策略</h3><p>一个 consumer group 中有多个 consumer，一个 topic 有多个 partition，所以必然会涉及到 partition 的分配问题，即确定那个 partition 由哪个 consumer 来消费。<br>Kafka 有两种分配策略，一是 RoundRobin，一是 Range。 </p>
<h3 id="2-3-3-offset的维护"><a href="#2-3-3-offset的维护" class="headerlink" title="2.3.3 offset的维护"></a>2.3.3 offset的维护</h3><p>由于 consumer 在消费过程中可能会出现断电宕机等故障， consumer 恢复后，需要从故障前的位置的继续消费，所以 <strong>consumer 需要实时记录自己消费到了哪个 offset</strong>，以便故障恢复后继续消费。  </p>
<p>Kafka 0.9 版本之前， consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始，<strong>consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中</strong>，该 topic 为__consumer_offsets。  </p>
<h2 id="2-4-Kafka高效读写数据"><a href="#2-4-Kafka高效读写数据" class="headerlink" title="2.4 Kafka高效读写数据"></a>2.4 Kafka高效读写数据</h2><ol>
<li><p><strong>顺序写磁盘</strong></p>
<p>Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。 官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。  </p>
</li>
<li><p><strong>零拷贝</strong></p>
</li>
</ol>
<h2 id="2-5-Zookeeper在Kafka中的作用"><a href="#2-5-Zookeeper在Kafka中的作用" class="headerlink" title="2.5 Zookeeper在Kafka中的作用"></a>2.5 Zookeeper在Kafka中的作用</h2><p>Kafka 集群中有一个 broker 会被选举为 <strong>Controller</strong>，<strong>负责管理集群 broker 的上下线，所有 topic 的分区副本分配和 leader 选举等工作</strong>。<br>Controller 的管理工作都是依赖于 Zookeeper 的。</p>
<h2 id="2-6-Kafka事务"><a href="#2-6-Kafka事务" class="headerlink" title="2.6 Kafka事务"></a>2.6 Kafka事务</h2><p>Kafka 从 0.11 版本开始引入了事务支持。<strong>事务可以保证 Kafka 在 Exactly Once 语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</strong>  </p>
<h3 id="2-6-1-Producer事务"><a href="#2-6-1-Producer事务" class="headerlink" title="2.6.1 Producer事务"></a>2.6.1 Producer事务</h3><p>为了实现跨分区跨会话的事务，需要引入一个全局唯一的 Transaction ID，并将 Producer获得的PID 和Transaction ID 绑定。这样当Producer 重启后就可以通过正在进行的 TransactionID 获得原来的 PID。  </p>
<p>为了管理 Transaction， Kafka 引入了一个新的组件 Transaction Coordinator。 Producer 就是通过和 Transaction Coordinator 交互获得 Transaction ID 对应的任务状态。 TransactionCoordinator 还负责将事务所有写入 Kafka 的一个内部 Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。  </p>
<h3 id="2-6-2-Consumer事务"><a href="#2-6-2-Consumer事务" class="headerlink" title="2.6.2 Consumer事务"></a>2.6.2 Consumer事务</h3><p>上述事务机制主要是从 Producer 方面考虑，对于 Consumer 而言，事务的保证就会相对较弱，尤其是无法保证 Commit 的信息被精确消费。这是由于 Consumer 可以通过 offset 访问任意信息，而且不同的 Segment File 生命周期不同，同一事务的消息可能会出现重启后被删除的情况。  </p>
<h1 id="3-Kafka-API"><a href="#3-Kafka-API" class="headerlink" title="3 Kafka API"></a>3 Kafka API</h1><h2 id="3-1-Producer-API"><a href="#3-1-Producer-API" class="headerlink" title="3.1 Producer API"></a>3.1 Producer API</h2><h3 id="3-1-1-消息发送流程"><a href="#3-1-1-消息发送流程" class="headerlink" title="3.1.1 消息发送流程"></a>3.1.1 消息发送流程</h3><p>Kafka 的 Producer 发送消息采用的是<strong>异步发送</strong>的方式。在消息发送的过程中，涉及到了两个线程——<strong>main 线程和 Sender 线程</strong>，以及一个线程共享变量——<strong>RecordAccumulator</strong>。main 线程将消息发送给 RecordAccumulator， Sender 线程不断从RecordAccumulator 中拉取消息发送到 Kafka broker。  </p>
<p><img src="C:%5CUsers%5Cextra%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210829153604369.png" alt="image-20210829153604369"></p>
<p><strong>相关参数：</strong><br>batch.size： 只有数据积累到 batch.size 之后， sender 才会发送数据。<br>linger.ms： 如果数据迟迟未达到 batch.size， sender 等待 linger.time 之后就会发送数据。  </p>
<h3 id="3-1-2-异步发送API"><a href="#3-1-2-异步发送API" class="headerlink" title="3.1.2 异步发送API"></a>3.1.2 异步发送API</h3><ol>
<li><p><strong>导入依赖</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.kafka&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;kafka-clients&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;0.11.0.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写代码</p>
<p>需要用到的类：<br><strong>KafkaProducer</strong>：需要创建一个生产者对象，用来发送数据<br><strong>ProducerConfig</strong>：获取所需的一系列配置参数<br><strong>ProducerRecord</strong>：每条数据都要封装成一个 ProducerRecord 对象  </p>
<ol>
<li><p><strong>不带回调函数的 API</strong>  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line">import org.apache.kafka.clients.producer.*;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line">public class CustomProducer &#123;</span><br><span class="line">    public static void main(String[] args) throws ExecutionException, InterruptedException &#123;</span><br><span class="line">    </span><br><span class="line">        Properties props &#x3D; new Properties();</span><br><span class="line">        &#x2F;&#x2F;kafka 集群， broker-list</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);</span><br><span class="line">        props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line">        &#x2F;&#x2F;重试次数</span><br><span class="line">        props.put(&quot;retries&quot;, 1);</span><br><span class="line">        &#x2F;&#x2F;批次大小</span><br><span class="line">        props.put(&quot;batch.size&quot;, 16384);</span><br><span class="line">        &#x2F;&#x2F;等待时间</span><br><span class="line">        props.put(&quot;linger.ms&quot;, 1);</span><br><span class="line">        &#x2F;&#x2F;RecordAccumulator 缓冲区大小</span><br><span class="line">        props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line">        props.put(&quot;key.serializer&quot;,</span><br><span class="line">        &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        props.put(&quot;value.serializer&quot;,</span><br><span class="line">        &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        Producer&lt;String, String&gt; producer &#x3D; new KafkaProducer&lt;&gt;(props);</span><br><span class="line">        for (int i &#x3D; 0; i &lt; 100; i++) &#123;</span><br><span class="line">            producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;,</span><br><span class="line">            Integer.toString(i), Integer.toString(i)));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>*<em>带回调函数的 API  *</em></p>
<p><strong>回调函数会在 producer 收到 ack 时调用，为异步调用</strong>， 该方法有两个参数，分别是RecordMetadata 和 Exception，如果 Exception 为 null，说明消息发送成功，如果Exception 不为 null，说明消息发送失败。<br>注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">for (int i &#x3D; 0; i &lt; 100; i++) &#123;</span><br><span class="line">    producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;,</span><br><span class="line">    Integer.toString(i), Integer.toString(i)), new Callback() &#123;</span><br><span class="line">        &#x2F;&#x2F;回调函数， 该方法会在 Producer 收到 ack 时调用，为异步调用</span><br><span class="line">        @Override</span><br><span class="line">        public void onCompletion(RecordMetadata metadata, Exception exception) &#123;</span><br><span class="line">            if (exception &#x3D;&#x3D; null) &#123;</span><br><span class="line">                System.out.println(&quot;success-&gt;&quot; +metadata.offset());</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                exception.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h2 id="3-2-Consumer-API"><a href="#3-2-Consumer-API" class="headerlink" title="3.2 Consumer API"></a>3.2 Consumer API</h2><p>Consumer 消费数据时的可靠性是很容易保证的，因为数据在 Kafka 中是持久化的，故不用担心数据丢失问题。<br>由于 consumer 在消费过程中可能会出现断电宕机等故障， consumer 恢复后，需要从故障前的位置的继续消费，所以 consumer 需要实时记录自己消费到了哪个 offset，以便故障恢复后继续消费。<br>所以 <strong>offset 的维护是 Consumer 消费数据是必须考虑的问题。</strong></p>
<h3 id="3-2-1-自动提交offset"><a href="#3-2-1-自动提交offset" class="headerlink" title="3.2.1 自动提交offset"></a>3.2.1 自动提交offset</h3><ol>
<li><p>编写代码</p>
<p>需要用到的类：<br><strong>KafkaConsumer</strong>： 需要创建一个消费者对象，用来消费数据<br><strong>ConsumerConfig</strong>： 获取所需的一系列配置参数<br><strong>ConsuemrRecord</strong>： 每条数据都要封装成一个 ConsumerRecord 对象<br>为了使我们能够专注于自己的业务逻辑， Kafka 提供了自动提交 offset 的功能。自动提交 offset 的相关参数：<br><strong>enable.auto.commit</strong>： 是否开启自动提交 offset 功能<br><strong>auto.commit.interval.ms</strong>： 自动提交 offset 的时间间隔  </p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line">import org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line">public class CustomConsumer &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">    </span><br><span class="line">        Properties props &#x3D; new Properties();</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);</span><br><span class="line">        props.put(&quot;group.id&quot;, &quot;test&quot;);</span><br><span class="line">        props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);</span><br><span class="line">        props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);</span><br><span class="line">        props.put(&quot;key.deserializer&quot;,</span><br><span class="line">        &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        props.put(&quot;value.deserializer&quot;,</span><br><span class="line">        &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        </span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer &#x3D; new KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(&quot;first&quot;));</span><br><span class="line">        while (true) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records &#x3D; consumer.poll(100);</span><br><span class="line">            for (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">            	System.out.printf(&quot;offset &#x3D; %d, key &#x3D; %s, value &#x3D; %s%n&quot;, record.offset(), record.key(), record.value());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-2-手动提交offset"><a href="#3-2-2-手动提交offset" class="headerlink" title="3.2.2 手动提交offset"></a>3.2.2 手动提交offset</h3><p>虽然自动提交 offset 十分简介便利，但由于其是基于时间提交的， 开发人员难以把握offset 提交的时机。因此 Kafka 还提供了手动提交 offset 的 API。手动提交 offset 的方法有两种：分别是 <strong>commitSync（同步提交）</strong> 和 <strong>commitAsync（异步提交）</strong> 。两者的相同点是，都会将本次 poll 的一批数据最高的偏移量提交；不同点是，commitSync 阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而 commitAsync 则没有失败重试机制，故有可能提交失败。  </p>
<ul>
<li><p><strong>异步提交offset</strong></p>
<p>虽然同步提交 offset 更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会收到很大的影响。因此更多的情况下，会选用异步提交 offset 的方式。<br>以下为异步提交 offset 的示例：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka.consumer;</span><br><span class="line"></span><br><span class="line">import org.apache.kafka.clients.consumer.*;</span><br><span class="line">import org.apache.kafka.common.TopicPartition;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line">public class CustomConsumer &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">    </span><br><span class="line">        Properties props &#x3D; new Properties();</span><br><span class="line">        &#x2F;&#x2F;Kafka 集群</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);</span><br><span class="line">        &#x2F;&#x2F;消费者组，只要 group.id 相同，就属于同一个消费者组</span><br><span class="line">        props.put(&quot;group.id&quot;, &quot;test&quot;);</span><br><span class="line">        &#x2F;&#x2F;关闭自动提交 offset</span><br><span class="line">        props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);</span><br><span class="line">        props.put(&quot;key.deserializer&quot;,</span><br><span class="line">        &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        props.put(&quot;value.deserializer&quot;,</span><br><span class="line">        &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer &#x3D; new KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(&quot;first&quot;));&#x2F;&#x2F;消费者订阅主题</span><br><span class="line">        while (true) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records &#x3D; consumer.poll(100);&#x2F;&#x2F;消费者拉取数据</span><br><span class="line">            for (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(&quot;offset &#x3D; %d, key &#x3D; %s, value &#x3D; %s%n&quot;, record.offset(), record.key(), record.value());</span><br><span class="line">            &#125;</span><br><span class="line">            &#x2F;&#x2F;异步提交</span><br><span class="line">            consumer.commitAsync(new OffsetCommitCallback() &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) &#123;</span><br><span class="line">                    if (exception !&#x3D; null) &#123;</span><br><span class="line">                    	System.err.println(&quot;Commit failed for&quot; + offsets);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="3-3-自定义Interceptor"><a href="#3-3-自定义Interceptor" class="headerlink" title="3.3 自定义Interceptor"></a>3.3 自定义Interceptor</h2><h3 id="3-3-1-拦截器原理"><a href="#3-3-1-拦截器原理" class="headerlink" title="3.3.1 拦截器原理"></a>3.3.1 拦截器原理</h3><p>Producer 拦截器(interceptor)是在 Kafka 0.10 版本被引入的，主要用于<strong>实现 clients 端的定制化控制逻辑。</strong><br>对于 producer 而言， interceptor 使得用户<strong>在消息发送前</strong>以及 <strong>producer 回调逻辑前</strong>有机会对消息做一些定制化需求，比如修改消息等。同时， producer 允许用户指定多个 interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。 Intercetpor 的实现接口是<strong>org.apache.kafka.clients.producer.ProducerInterceptor</strong>，其定义的方法包括：  </p>
<p>（1） <strong>configure(configs)</strong>：<br>获取配置信息和初始化数据时调用。<br>（2） <strong>onSend(ProducerRecord)</strong>：<br>该方法封装进 KafkaProducer.send 方法中，即它运行在用户主线程中。 Producer 确保在消息被序列化以及计算分区前调用该方法。 用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的 topic 和分区， 否则会影响目标分区的计算。<br>（3） <strong>onAcknowledgement(RecordMetadata, Exception)</strong>：<br>该方法会在消息从 RecordAccumulator 成功发送到 Kafka Broker 之后，或者在发送过程中失败时调用。 并且通常都是在 producer 回调逻辑触发之前。 onAcknowledgement 运行在producer 的 IO 线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢 producer 的消息发送效率。<br>（4） <strong>close</strong>：<br>关闭 interceptor，主要用于执行一些资源清理工作。</p>
<p>如前所述， interceptor 可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个 interceptor，则 producer 将按照指定顺序调用它们，并仅仅是捕获每个 interceptor 可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。  </p>
<h3 id="3-3-2-拦截器案例"><a href="#3-3-2-拦截器案例" class="headerlink" title="3.3.2 拦截器案例"></a>3.3.2 拦截器案例</h3><ol>
<li><p><strong>需求：</strong></p>
<p>实现一个简单的双 interceptor 组成的拦截链。第一个 interceptor 会在消息发送前将时间戳信息加到消息 value 的最前部；第二个 interceptor 会在消息发送后更新成功发送消息数或失败发送消息数。</p>
<p><img src="https://i.loli.net/2021/08/29/4N1kchtIQYpRix5.png" alt="image-20210829160513448"></p>
</li>
<li><p><strong>案例实操</strong></p>
<ol>
<li><p>增加时间戳拦截器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka.interceptor;</span><br><span class="line"></span><br><span class="line">import java.util.Map;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line">public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void configure(Map&lt;String, ?&gt; configs) &#123;</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123;</span><br><span class="line">        &#x2F;&#x2F; 创建一个新的 record，把时间戳写入消息体的最前部</span><br><span class="line">        return new ProducerRecord(record.topic(),record.partition(), record.timestamp(), record.key(),System.currentTimeMillis() + &quot;,&quot; + record.value().toString());</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123;</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public void close() &#123;</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>统计发送消息成功和发送失败消息数，并在 producer 关闭时打印这两个计数器 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka.interceptor;</span><br><span class="line"></span><br><span class="line">import java.util.Map;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line">public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt;&#123;</span><br><span class="line"></span><br><span class="line">    private int errorCounter &#x3D; 0;</span><br><span class="line">    private int successCounter &#x3D; 0;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public void configure(Map&lt;String, ?&gt; configs) &#123;</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123;</span><br><span class="line">    	return record;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123;</span><br><span class="line">        &#x2F;&#x2F; 统计成功和失败的次数</span><br><span class="line">        if (exception &#x3D;&#x3D; null) &#123;</span><br><span class="line">        successCounter++;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">        errorCounter++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public void close() &#123;</span><br><span class="line">        &#x2F;&#x2F; 保存结果</span><br><span class="line">        System.out.println(&quot;Successful sent: &quot; + successCounter);</span><br><span class="line">        System.out.println(&quot;Failed sent: &quot; + errorCounter);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Producer主程序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.kafka.interceptor;</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.Producer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line">public class InterceptorProducer &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 1 设置配置信息</span><br><span class="line">        Properties props &#x3D; new Properties();</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);</span><br><span class="line">        props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line">        props.put(&quot;retries&quot;, 3);</span><br><span class="line">        props.put(&quot;batch.size&quot;, 16384);</span><br><span class="line">        props.put(&quot;linger.ms&quot;, 1);</span><br><span class="line">        props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line">        props.put(&quot;key.serializer&quot;,</span><br><span class="line">        &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        props.put(&quot;value.serializer&quot;,</span><br><span class="line">        &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F; 2 构建拦截链</span><br><span class="line">        List&lt;String&gt; interceptors &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line">        interceptors.add(&quot;com.atguigu.kafka.interceptor.TimeInterceptor&quot;);</span><br><span class="line">        interceptors.add(&quot;com.atguigu.kafka.interceptor.CounterInterceptor&quot;);</span><br><span class="line">        props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);</span><br><span class="line">        String topic &#x3D; &quot;first&quot;;</span><br><span class="line">        Producer&lt;String, String&gt; producer &#x3D; new KafkaProducer&lt;&gt;(props);</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F; 3 发送消息</span><br><span class="line">        for (int i &#x3D; 0; i &lt; 10; i++) &#123;</span><br><span class="line">            ProducerRecord&lt;String, String&gt; record &#x3D; new ProducerRecord&lt;&gt;(topic, &quot;message&quot; + i);</span><br><span class="line">            producer.send(record);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F; 4 一定要关闭 producer，这样才会调用 interceptor 的 close 方法</span><br><span class="line">        producer.close();</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<p>在 kafka 上启动消费者， 然后运行客户端 java 程序。 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first</span><br><span class="line"></span><br><span class="line">1501904047034,message0</span><br><span class="line">1501904047225,message1</span><br><span class="line">1501904047230,message2</span><br><span class="line">1501904047234,message3</span><br><span class="line">1501904047236,message4</span><br><span class="line">1501904047240,message5</span><br><span class="line">1501904047243,message6</span><br><span class="line">1501904047246,message7</span><br><span class="line">1501904047249,message8</span><br><span class="line">1501904047252,message9</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://extrali.com/2021/04/02/%E5%A4%A7%E6%95%B0%E6%8D%AELinux%E5%BC%80%E5%8F%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="黎达">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Extrali">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/04/02/%E5%A4%A7%E6%95%B0%E6%8D%AELinux%E5%BC%80%E5%8F%91/" class="post-title-link" itemprop="url">大数据Linux开发</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>

              <time title="Erstellt: 2021-04-02 16:07:35" itemprop="dateCreated datePublished" datetime="2021-04-02T16:07:35+08:00">2021-04-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Bearbeitet am</span>
                <time title="Geändert am: 2021-05-19 22:55:05" itemprop="dateModified" datetime="2021-05-19T22:55:05+08:00">2021-05-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">in</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-虚拟机的三种网络配置方式"><a href="#1-虚拟机的三种网络配置方式" class="headerlink" title="1.虚拟机的三种网络配置方式"></a>1.虚拟机的三种网络配置方式</h1><ol>
<li><strong>桥接模式</strong></li>
<li><strong>NAT模式</strong></li>
<li><strong>主机模式</strong></li>
</ol>
<p><img src="https://i.loli.net/2021/05/09/CdsxetMYWaTNLOk.png" alt=""></p>
<h1 id="2-Linux的文件目录"><a href="#2-Linux的文件目录" class="headerlink" title="2.Linux的文件目录"></a>2.Linux的文件目录</h1><p>linux 的文件系统是采用级层式的<strong>树状目录结构</strong>，在此结构中的最上层是<strong>根目录”/“</strong>，然后在此目录下再创建其他的目录。  </p>
<ul>
<li>在 Linux 世界里，<strong>一切皆文件</strong>（即使是一个硬件设备，也是使用文本来标志）  </li>
</ul>
<p><img src="https://i.loli.net/2021/05/09/zZqnRByStk1c6dl.png" alt="image-20210509144732100"></p>
<p><strong>具体的目录结构：</strong></p>
<p><img src="https://i.loli.net/2021/05/09/DG7Q8ZNV43wFa6z.png" alt="image-20210509144858005"></p>
<p><img src="https://i.loli.net/2021/05/09/ca7YICtRSjiJuL9.png" alt="image-20210509145035968"></p>
<p><img src="https://i.loli.net/2021/05/09/GSEyH62VDoembkv.png" alt="image-20210509145242110"></p>
<p><img src="https://i.loli.net/2021/05/09/Jr23vF459znRWVY.png" alt="image-20210509145312029"></p>
<p><img src="https://i.loli.net/2021/05/09/zVjF9ySi4QoZeBa.png" alt="image-20210509145332052"></p>
<h1 id="3-远程登陆到Linux服务器"><a href="#3-远程登陆到Linux服务器" class="headerlink" title="3.远程登陆到Linux服务器"></a>3.远程登陆到Linux服务器</h1><p><strong>启用服务器的SSH服务：service sshd start</strong></p>
<p><strong>设置SSH服务开机自动启动：chkconfig –level 2345 sshd on</strong></p>
<h1 id="4-Vi和Vim编辑器"><a href="#4-Vi和Vim编辑器" class="headerlink" title="4.Vi和Vim编辑器"></a>4.Vi和Vim编辑器</h1><h2 id="4-1-Vim的三种模式切换"><a href="#4-1-Vim的三种模式切换" class="headerlink" title="4.1 Vim的三种模式切换"></a>4.1 Vim的三种模式切换</h2><p><img src="https://i.loli.net/2021/05/09/Uj4x6PzmZQs8ctv.png" alt="image-20210509152159810"></p>
<h2 id="4-2-Vim的快捷键使用"><a href="#4-2-Vim的快捷键使用" class="headerlink" title="4.2 Vim的快捷键使用"></a>4.2 Vim的快捷键使用</h2><p><strong>快捷键需要在一般模式下使用</strong></p>
<ol>
<li>拷贝当前行 yy , 拷贝当前行向下的 5 行 5yy，并粘贴 p</li>
<li>删除当前行 dd , 删除当前行向下的 5 行 5dd  </li>
<li>在文件中查找某个单词 [<strong>命令模式下 /关键字</strong> ， 回车 查找 , 输入 <strong>n</strong> 就是查找下一个 ]  </li>
<li>设置文件的行号，取消文件的行号[<strong>命令模式</strong>下 : <strong>set nu</strong> 和 :<strong>set nonu</strong>]  </li>
<li>编辑 /etc/profile 文件，使用快捷键到底文档的<strong>最末行[G]</strong>和<strong>最首行[gg]</strong>  </li>
<li>在一个文件中输入 “hello” ,然后又<strong>撤销这个动作</strong> <strong>u</strong>  </li>
</ol>
<h1 id="5-开机、重启和用户登陆注销"><a href="#5-开机、重启和用户登陆注销" class="headerlink" title="5.开机、重启和用户登陆注销"></a>5.开机、重启和用户登陆注销</h1><h2 id="5-1-开机-amp-重启指令"><a href="#5-1-开机-amp-重启指令" class="headerlink" title="5.1 开机&amp;重启指令"></a>5.1 开机&amp;重启指令</h2><ul>
<li><strong>shutdown -h now [立刻关机]</strong></li>
<li>shutdown -h 1 “1 分钟，关机.” [1 分钟后，关机]</li>
<li><strong>shutdown -r now [立刻重启]</strong></li>
<li>shutdown -r 2 “2 分钟后，重启”</li>
<li>halt 【立刻关机】</li>
<li><strong>reboot 【立刻重启】</strong></li>
</ul>
<p>在重启和关机前，通常需要先执行<strong>sync</strong> [把内存的数据，写入磁盘]  </p>
<p>Linux系统中为了提高磁盘的读写效率，对磁盘采取了 “<strong>预读迟写</strong>”操作方式。当用户保存文件时，Linux核心并不一定立即将保存数据写入物理磁盘中，而是将数据保存在缓冲区中，等缓冲区满时再写入磁盘，这种方式可以极大的提高磁盘写入数据的效率。但是，也带来了安全隐患，如果数据还未写入磁盘时，系统掉电或者其他严重问题出现，则将导致数据丢失。使用sync指令可以立即将缓冲区的数据写入磁盘。</p>
<h2 id="5-2-用户登录和注销"><a href="#5-2-用户登录和注销" class="headerlink" title="5.2 用户登录和注销"></a>5.2 用户登录和注销</h2><ol>
<li>登录时尽量少用 root 帐号登录，因为它是系统管理员，最大的权限，避免操作失误。可以利用普通用户登录，登录后再用” su - 用户名’命令来切换成系统管理员身份.</li>
<li>在提示符下输入 <strong>logout 即可注销用户</strong>  </li>
</ol>
<h1 id="6-用户管理"><a href="#6-用户管理" class="headerlink" title="6.用户管理"></a>6.用户管理</h1><p><img src="https://i.loli.net/2021/05/09/XZ7fWwTivHk5u12.png" alt="image-20210509161526595"></p>
<h2 id="6-1添加用户"><a href="#6-1添加用户" class="headerlink" title="6.1添加用户"></a>6.1添加用户</h2><ul>
<li><strong>基本语法</strong><ul>
<li><code>useradd 用户名</code></li>
</ul>
</li>
<li>使用细节<ul>
<li>当创建用户成功后，会自动的创建和用户同名的家目录 【/home/xiaoming】</li>
<li>也可以通过 useradd -d 指定目录 新的用户名 jack，给新创建的用户指定家目录  </li>
</ul>
</li>
</ul>
<h2 id="6-2-指定-修改密码"><a href="#6-2-指定-修改密码" class="headerlink" title="6.2 指定/修改密码"></a>6.2 指定/修改密码</h2><ul>
<li><strong>基本语法</strong><ul>
<li><code>passwd 用户名   //如果没有带用户名则是给当前登录的用户修改密码</code></li>
</ul>
</li>
</ul>
<h2 id="6-3-删除用户"><a href="#6-3-删除用户" class="headerlink" title="6.3 删除用户"></a>6.3 删除用户</h2><ul>
<li><strong>基本语法</strong><ul>
<li><code>userdel 用户名</code></li>
</ul>
</li>
<li><strong>使用细节</strong><ul>
<li>删除用户 xiaoming，但是要<strong>保留家目录</strong> <strong>userdel 用户名</strong> //userdel xiaoming</li>
<li><strong>删除用户以及用户主目录</strong> // <strong>userdel – r xiaoming</strong> 【小心使用】  </li>
</ul>
</li>
</ul>
<h2 id="6-4-查询用户信息"><a href="#6-4-查询用户信息" class="headerlink" title="6.4 查询用户信息"></a>6.4 查询用户信息</h2><ul>
<li><strong>基本语法</strong><ul>
<li><code>id 用户名</code></li>
</ul>
</li>
</ul>
<h2 id="6-5-切换用户"><a href="#6-5-切换用户" class="headerlink" title="6.5 切换用户"></a>6.5 切换用户</h2><p>*<em>在操作 Linux 中，如果当前用户的权限不够，可以通过 su - 指令，切换到高权限用户，比如root  *</em></p>
<ul>
<li><p><strong>基本语法</strong></p>
</li>
<li><p><code>su - 切换用户名</code></p>
</li>
<li><p><strong>细节说明</strong></p>
<ul>
<li>从权限高的用户切换到权限低的用户，不需要输入密码，反之需要。</li>
<li><strong>当需要返回到原来用户时，使用 exit 指令</strong></li>
<li>如果 su – 没有带用户名，则默认切换到 root 用户  </li>
</ul>
</li>
<li><p><strong>注意</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">su 用户名称   （功能描述：切换用户，只能获得用户的执行权限，不能获得环境变量）</span><br><span class="line">su - 用户名称		（功能描述：切换到用户并获得该用户的环境变量及执行权限）</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]#su tangseng</span><br><span class="line">[root@hadoop101 ~]#echo $PATH</span><br><span class="line">&#x2F;usr&#x2F;lib64&#x2F;qt-3.3&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;sbin:&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;root&#x2F;bin</span><br><span class="line">[root@hadoop101 ~]#exit</span><br><span class="line">[root@hadoop101 ~]#su - tangseng</span><br><span class="line">[root@hadoop101 ~]#echo $PATH</span><br><span class="line">&#x2F;usr&#x2F;lib64&#x2F;qt-3.3&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;bin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;sbin:&#x2F;sbin:&#x2F;home&#x2F;tangseng&#x2F;bin</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="6-6-用户组"><a href="#6-6-用户组" class="headerlink" title="6.6 用户组"></a>6.6 用户组</h2><p><strong>新增组</strong></p>
<ul>
<li><strong>基本语法</strong>      <code>groupadd 组名</code></li>
</ul>
<p><strong>添加用户时指定特定的组</strong></p>
<ul>
<li><strong>基本语法</strong>      <code>useradd -g 用户组 用户名</code></li>
</ul>
<p><strong>删除组</strong></p>
<ul>
<li><strong>基本语法</strong>      <code>groupdel 组名</code></li>
</ul>
<p><strong>修改用户的组</strong></p>
<ul>
<li><strong>基本语法</strong>      <code>usermod -g 新的组名 用户名</code></li>
</ul>
<h2 id="6-7-sudo-设置普通用户具有root权限"><a href="#6-7-sudo-设置普通用户具有root权限" class="headerlink" title="6.7 sudo 设置普通用户具有root权限"></a>6.7 sudo 设置普通用户具有root权限</h2><p><strong>修改配置文件</strong></p>
<p><code>vim /etc/sudoers</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">修改 &#x2F;etc&#x2F;sudoers 文件，找到下面一行(91行)，在root下面添加一行，如下所示：</span><br><span class="line">## Allow root to run any commands anywhere</span><br><span class="line">root    ALL&#x3D;(ALL)     ALL</span><br><span class="line">atguigu   ALL&#x3D;(ALL)     ALL</span><br><span class="line">或者配置成采用sudo命令时，不需要输入密码</span><br><span class="line">## Allow root to run any commands anywhere</span><br><span class="line">root      ALL&#x3D;(ALL)     ALL</span><br><span class="line">atguigu   ALL&#x3D;(ALL)     NOPASSWD:ALL</span><br></pre></td></tr></table></figure>

<h1 id="7-网络设置和系统管理操作"><a href="#7-网络设置和系统管理操作" class="headerlink" title="7.网络设置和系统管理操作"></a>7.网络设置和系统管理操作</h1><h2 id="7-1-配置网络IP地址"><a href="#7-1-配置网络IP地址" class="headerlink" title="7.1 配置网络IP地址"></a>7.1 配置网络IP地址</h2><p><strong><code>ifconfig</code>指令可以用来查看所有网络接口的配置信息</strong></p>
<h3 id="7-1-1-修改IP地址"><a href="#7-1-1-修改IP地址" class="headerlink" title="7.1.1 修改IP地址"></a>7.1.1 <strong>修改IP地址</strong></h3><ol>
<li><p><code>vim /etc/sysconfig/network-scripts/ifcfg-ens33</code></p>
<p><strong>修改成如下图所示：</strong></p>
</li>
</ol>
<p><img src="https://i.loli.net/2021/05/09/7mkyhRbrwzG3FH5.png" alt="image-20210509165453752"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DEVICE&#x3D;eth0                #接口名（设备,网卡）</span><br><span class="line">HWADDR&#x3D;00:0C:2x:6x:0x:xx   #MAC地址 </span><br><span class="line">TYPE&#x3D;Ethernet               #网络类型（通常是Ethemet）</span><br><span class="line">UUID&#x3D;926a57ba-92c6-4231-bacb-f27e5e6a9f44  #随机id</span><br><span class="line">#系统启动的时候网络接口是否有效（yes&#x2F;no）</span><br><span class="line">ONBOOT&#x3D;yes                </span><br><span class="line"># IP的配置方法[none|static|bootp|dhcp]（引导时不使用协议|静态分配IP|BOOTP协议|DHCP协议）</span><br><span class="line">BOOTPROTO&#x3D;static      </span><br><span class="line">#IP地址</span><br><span class="line">IPADDR&#x3D;192.168.1.101   </span><br><span class="line">#网关  </span><br><span class="line">GATEWAY&#x3D;192.168.1.2      </span><br><span class="line">#域名解析器</span><br><span class="line">DNS1&#x3D;192.168.1.2</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>执行<code>service network restart</code>重启网络</li>
<li>如果报错，reboot</li>
</ol>
<h2 id="7-2-配置主机名"><a href="#7-2-配置主机名" class="headerlink" title="7.2 配置主机名"></a>7.2 配置主机名</h2><p><code>hostname</code><strong>显示系统的主机名</strong></p>
<h3 id="7-2-1-修改主机名称"><a href="#7-2-1-修改主机名称" class="headerlink" title="7.2.1 修改主机名称"></a>7.2.1 修改主机名称</h3><p><code>vim /etc/sysconfig/network</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">文件中内容</span><br><span class="line">NETWORKING&#x3D;yes</span><br><span class="line">NETWORKING_IPV6&#x3D;no</span><br><span class="line">HOSTNAME&#x3D; hadoop100</span><br></pre></td></tr></table></figure>

<p>然后``vim /etc/hosts`  添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">192.168.1.100 hadoop100</span><br><span class="line">192.168.1.101 hadoop101</span><br><span class="line">192.168.1.102 hadoop102</span><br><span class="line">192.168.1.103 hadoop103</span><br><span class="line">192.168.1.104 hadoop104</span><br><span class="line">192.168.1.105 hadoop105</span><br><span class="line">192.168.1.106 hadoop106</span><br><span class="line">192.168.1.107 hadoop107</span><br><span class="line">192.168.1.108 hadoop108</span><br></pre></td></tr></table></figure>

<p>最后修改window10的主机映射文件（<strong>hosts文件</strong>）</p>
<ol>
<li>进入C:\Windows\System32\drivers\etc路径</li>
<li>拷贝hosts文件到桌面</li>
<li>打开桌面hosts文件并添加如下内容</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">192.168.1.100 hadoop100</span><br><span class="line">192.168.1.101 hadoop101</span><br><span class="line">192.168.1.102 hadoop102</span><br><span class="line">192.168.1.103 hadoop103</span><br><span class="line">192.168.1.104 hadoop104</span><br><span class="line">192.168.1.105 hadoop105</span><br><span class="line">192.168.1.106 hadoop106</span><br><span class="line">192.168.1.107 hadoop107</span><br><span class="line">192.168.1.108 hadoop108</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>将桌面hosts文件覆盖C:\Windows\System32\drivers\etc路径hosts文件</li>
</ol>
<p><strong>亲测上面的方法不好用，直接使用指令<code>hostnamectl set-hostname [主机名]</code>即可</strong></p>
<h2 id="7-3关闭防火墙"><a href="#7-3关闭防火墙" class="headerlink" title="7.3关闭防火墙"></a>7.3关闭防火墙</h2><h3 id="7-3-1-service后台服务管理"><a href="#7-3-1-service后台服务管理" class="headerlink" title="7.3.1 service后台服务管理"></a>7.3.1 service后台服务管理</h3><ol>
<li><strong>基本语法</strong><ul>
<li>service 服务名 start         （功能描述：开启服务）</li>
<li>service 服务名 stop         （功能描述：关闭服务）</li>
<li>service 服务名 restart       （功能描述：重新启动服务）</li>
<li>service 服务名 status        （功能描述：查看服务状态）</li>
</ul>
</li>
</ol>
<h3 id="7-3-2-chkconfig设置后台服务的自启配置"><a href="#7-3-2-chkconfig设置后台服务的自启配置" class="headerlink" title="7.3.2 chkconfig设置后台服务的自启配置"></a>7.3.2 chkconfig设置后台服务的自启配置</h3><ol>
<li><strong>基本语法</strong><ul>
<li><code>chkconfig</code>                                <strong>查看服务器的所有自启配置</strong></li>
<li><code>chkconfig 服务名 off</code>           <strong>关掉指定服务的自动启动</strong></li>
<li><code>chkconfig 服务名 on</code>           <strong>开启指定服务的自动启动</strong></li>
<li><code>chkconfig 服务名 --list</code>           <strong>查看服务开机启动状态</strong></li>
</ul>
</li>
</ol>
<h3 id="7-3-3-进程运行级别"><a href="#7-3-3-进程运行级别" class="headerlink" title="7.3.3 进程运行级别"></a>7.3.3 进程运行级别</h3><p><img src="https://i.loli.net/2021/05/10/wJy6RtjpvWUaho5.png" alt="image-20210510210208615"></p>
<h3 id="7-3-4-关闭防火墙"><a href="#7-3-4-关闭防火墙" class="headerlink" title="7.3.4 关闭防火墙"></a>7.3.4 关闭防火墙</h3><ol>
<li><p><strong>临时关闭防火墙</strong></p>
<ul>
<li><p><strong>查看防火墙状态</strong>        <code>service iptables status</code></p>
</li>
<li><p><strong>临时关闭防火墙</strong>        <code>service iptables stop</code></p>
</li>
</ul>
</li>
<li><p><strong>开机启动时关闭防火墙</strong></p>
<ul>
<li><strong>查看防火墙开机启动状态</strong>    <code>chkconfig iptables --list</code> </li>
<li><strong>设置开机时关闭防火墙</strong>        <code>chkconfig iptables off</code></li>
</ul>
</li>
</ol>
<p><strong>实际测试需要使用下面这个指令</strong></p>
<p><code>[root@hadoop100 ~]# systemctl stop firewalld
[root@hadoop100 ~]# systemctl disable firewalld.service</code></p>
<h3 id="7-3-5-克隆虚拟机"><a href="#7-3-5-克隆虚拟机" class="headerlink" title="7.3.5 克隆虚拟机"></a>7.3.5 克隆虚拟机</h3><ul>
<li><strong>克隆虚拟机，选择完全克隆</strong></li>
<li>然后<strong>修改克隆后虚拟机的IP地址和MAC地址</strong></li>
<li><strong>修改主机名称</strong></li>
</ul>
<h1 id="8-常见使用指令"><a href="#8-常见使用指令" class="headerlink" title="8.常见使用指令"></a>8.常见使用指令</h1><h2 id="8-1-帮助指令"><a href="#8-1-帮助指令" class="headerlink" title="8.1 帮助指令"></a>8.1 帮助指令</h2><h3 id="8-1-1-man-获得帮助信息"><a href="#8-1-1-man-获得帮助信息" class="headerlink" title="8.1.1 man 获得帮助信息"></a>8.1.1 man 获得帮助信息</h3><ol>
<li><p><strong>基本用法</strong></p>
<p><code>man [命令或配置文件]    （功能描述：获得帮助信息）</code></p>
<h3 id="8-1-2-help-获得shell内置命令的帮助信息"><a href="#8-1-2-help-获得shell内置命令的帮助信息" class="headerlink" title="8.1.2 help 获得shell内置命令的帮助信息"></a>8.1.2 help 获得shell内置命令的帮助信息</h3></li>
<li><p><strong>基本用法</strong></p>
<p><code>help 命令   （功能描述：获得shell内置命令的帮助信息）</code></p>
</li>
</ol>
<h3 id="8-1-3-常用快捷键"><a href="#8-1-3-常用快捷键" class="headerlink" title="8.1.3 常用快捷键"></a>8.1.3 常用快捷键</h3><table>
<thead>
<tr>
<th>常用快捷键</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>ctrl + c</td>
<td>停止进程</td>
</tr>
<tr>
<td>ctrl+l</td>
<td>清屏；彻底清屏是：reset</td>
</tr>
<tr>
<td>ctrl + q</td>
<td>退出</td>
</tr>
<tr>
<td>善于用tab键</td>
<td>提示(更重要的是可以防止敲错)</td>
</tr>
<tr>
<td>上下键</td>
<td>查找执行过的命令</td>
</tr>
<tr>
<td>ctrl +alt</td>
<td>linux和Windows之间切换</td>
</tr>
</tbody></table>
<h2 id="8-2-文件目录类"><a href="#8-2-文件目录类" class="headerlink" title="8.2 文件目录类"></a>8.2 文件目录类</h2><h3 id="8-2-1-pwd-显示当前工作目录的绝对路径"><a href="#8-2-1-pwd-显示当前工作目录的绝对路径" class="headerlink" title="8.2.1 pwd 显示当前工作目录的绝对路径"></a>8.2.1 pwd 显示当前工作目录的绝对路径</h3><p><strong>pwd:print working directory 打印工作目录</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>pwd    （功能描述：显示当前工作目录的绝对路径）</code></p>
</li>
</ol>
<h3 id="8-2-2-ls-列出目录的内容"><a href="#8-2-2-ls-列出目录的内容" class="headerlink" title="8.2.2 ls 列出目录的内容"></a>8.2.2 ls 列出目录的内容</h3><p><strong>ls:list 列出目录内容</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>ls [选项] [目录或是文件]</code></p>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用)</td>
</tr>
<tr>
<td>-l</td>
<td>长数据串列出，包含文件的属性与权限等等数据；(常用)</td>
</tr>
</tbody></table>
</li>
<li><p><strong>显示说明</strong></p>
<p>每行列出的信息依次是： <strong>文件类型与权限</strong> <strong>链接数</strong> <strong>文件属主</strong> <strong>文件属组</strong> <strong>文件大小用byte来表示</strong> <strong>建立或最近修改的时间</strong> <strong>名字</strong></p>
</li>
<li><p><strong>案例实操</strong></p>
<p><strong>查看当前目录的所有内容信息</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 ~]$ ls -al</span><br><span class="line">总用量 44</span><br><span class="line">drwx------. 5 atguigu atguigu 4096 5月  27 15:15 .</span><br><span class="line">drwxr-xr-x. 3 root    root    4096 5月  27 14:03 ..</span><br><span class="line">drwxrwxrwx. 2 root    root    4096 5月  27 14:14 hello</span><br><span class="line">-rwxrw-r--. 1 atguigu atguigu   34 5月  27 14:20 test.txt</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="8-2-3-cd-切换目录"><a href="#8-2-3-cd-切换目录" class="headerlink" title="8.2.3 cd 切换目录"></a>8.2.3 cd 切换目录</h3><p><strong>cd:Change Directory切换路径</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>cd [参数]</code></p>
</li>
<li><p><strong>参数说明</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>cd 绝对路径</td>
<td>切换路径</td>
</tr>
<tr>
<td>cd相对路径</td>
<td>切换路径</td>
</tr>
<tr>
<td>cd ~或者cd</td>
<td>回到自己的家目录</td>
</tr>
<tr>
<td>cd -</td>
<td>回到上一次所在目录</td>
</tr>
<tr>
<td>cd ..</td>
<td>回到当前目录的上一级目录</td>
</tr>
<tr>
<td>cd -P</td>
<td>跳转到实际物理路径，而非快捷方式路径</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="8-2-4-mkdir-创建一个新的目录"><a href="#8-2-4-mkdir-创建一个新的目录" class="headerlink" title="8.2.4 mkdir 创建一个新的目录"></a>8.2.4 mkdir 创建一个新的目录</h3><p><strong>mkdir:Make directory 建立目录</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>mkdir [选项] 要创建的目录</code></p>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-p</td>
<td>创建多层目录</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="8-2-5-删除一个空的目录"><a href="#8-2-5-删除一个空的目录" class="headerlink" title="8.2.5 删除一个空的目录"></a>8.2.5 删除一个空的目录</h3><p><strong><em>rmdir</em>:Remove directory 移动目录</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>rmdir 要删除的空目录</code></p>
</li>
</ol>
<h3 id="8-2-6-touch-创建空文件"><a href="#8-2-6-touch-创建空文件" class="headerlink" title="8.2.6 touch 创建空文件"></a>8.2.6 touch 创建空文件</h3><ol>
<li><p><strong>基本语法</strong></p>
<p><code>touch 文件名称</code></p>
</li>
</ol>
<h3 id="8-2-7-cp-复制文件或目录"><a href="#8-2-7-cp-复制文件或目录" class="headerlink" title="8.2.7 cp  复制文件或目录"></a>8.2.7 cp  复制文件或目录</h3><ol>
<li><p><strong>基本语法</strong></p>
<p><code>cp [选项] source dest             （功能描述：复制source文件到dest）</code></p>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>递归复制整个文件夹</td>
</tr>
</tbody></table>
</li>
<li><p><strong>参数说明</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>source</td>
<td>源文件</td>
</tr>
<tr>
<td>dest</td>
<td>目标文件</td>
</tr>
</tbody></table>
</li>
<li><p><strong>案例实操</strong></p>
<p><strong>递归复制文件夹1下的所有文件夹</strong></p>
<p><code>cp - r 1/* ./</code></p>
</li>
</ol>
<h3 id="8-2-8-rm-移除文件或目录"><a href="#8-2-8-rm-移除文件或目录" class="headerlink" title="8.2.8 rm 移除文件或目录"></a>8.2.8 rm 移除文件或目录</h3><ol>
<li><p><strong>基本语法</strong></p>
<p><code>rm [选项] deleteFile          （功能描述：递归删除目录中所有内容）</code></p>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>递归删除目录中所有内容</td>
</tr>
<tr>
<td>-f</td>
<td>强制执行删除操作，而不提示用于进行确认。</td>
</tr>
<tr>
<td>-v</td>
<td>显示指令的详细执行过程</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="8-2-9-mv-移动文件与目录或重命名"><a href="#8-2-9-mv-移动文件与目录或重命名" class="headerlink" title="8.2.9 mv 移动文件与目录或重命名"></a>8.2.9 mv 移动文件与目录或重命名</h3><ol>
<li><strong>基本语法</strong><ul>
<li><code>mv oldNameFile newNameFile  （功能描述：重命名）</code></li>
<li><code>mv /temp/movefile /targetFolder （功能描述：移动文件）</code></li>
</ul>
</li>
</ol>
<h3 id="8-2-10-cat-查看文件内容"><a href="#8-2-10-cat-查看文件内容" class="headerlink" title="8.2.10 cat 查看文件内容"></a>8.2.10 cat 查看文件内容</h3><p><strong>查看文件内容，从第一行开始显示。</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>cat [选项] 要查看的文件</code></p>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>显示所有行的行号，包括空行。</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="8-2-11-more-文件内容分屏查看器"><a href="#8-2-11-more-文件内容分屏查看器" class="headerlink" title="8.2.11 more 文件内容分屏查看器"></a>8.2.11 more 文件内容分屏查看器</h3><p><strong>more指令是一个基于VI编辑器的文本过滤器，它以全屏幕的方式按页显示文本文件的内容。more指令中内置了若干快捷键，详见操作说明。</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>more 要查看的文件</code></p>
</li>
<li><p><strong>操作说明</strong></p>
<table>
<thead>
<tr>
<th>操作</th>
<th>功能说明</th>
</tr>
</thead>
<tbody><tr>
<td>空白键 (space)</td>
<td>代表向下翻一页；</td>
</tr>
<tr>
<td>Enter</td>
<td>代表向下翻『一行』；</td>
</tr>
<tr>
<td>q</td>
<td>代表立刻离开 more ，不再显示该文件内容。</td>
</tr>
<tr>
<td>Ctrl+F</td>
<td>向下滚动一屏</td>
</tr>
<tr>
<td>Ctrl+B</td>
<td>返回上一屏</td>
</tr>
<tr>
<td>=</td>
<td>输出当前行的行号</td>
</tr>
<tr>
<td>:f</td>
<td>输出文件名和当前行的行号</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="8-2-12-less-分屏显示文件内容"><a href="#8-2-12-less-分屏显示文件内容" class="headerlink" title="8.2.12 less 分屏显示文件内容"></a>8.2.12 less 分屏显示文件内容</h3><p><strong>less指令用来分屏查看文件内容，它的功能与more指令类似，但是比more指令更加强大，支持各种显示终端。less指令在显示文件内容时，并不是一次将整个文件加载之后才显示，而是根据显示需要加载内容，对于显示大型文件具有较高的效率。</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>less 要查看的文件</code></p>
</li>
<li><p><strong>操作说明</strong></p>
<table>
<thead>
<tr>
<th>操作</th>
<th>功能说明</th>
</tr>
</thead>
<tbody><tr>
<td>空白键</td>
<td>向下翻动一页；</td>
</tr>
<tr>
<td>[pagedown]</td>
<td>向下翻动一页</td>
</tr>
<tr>
<td>[pageup]</td>
<td>向上翻动一页；</td>
</tr>
<tr>
<td>/字串</td>
<td>向下搜寻『字串』的功能；n：向下查找；N：向上查找；</td>
</tr>
<tr>
<td>?字串</td>
<td>向上搜寻『字串』的功能；n：向上查找；N：向下查找；</td>
</tr>
<tr>
<td>q</td>
<td>离开 less 这个程序；</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="8-2-13-echo"><a href="#8-2-13-echo" class="headerlink" title="8.2.13 echo"></a>8.2.13 echo</h3><p><strong>echo输出内容到控制台</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>echo [选项] [输出内容]</code></p>
<p>选项： </p>
<p> <strong>-e： 支持反斜线控制的字符转换</strong></p>
<table>
<thead>
<tr>
<th>控制字符</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>\</td>
<td>输出\本身</td>
</tr>
<tr>
<td>\n</td>
<td>换行符</td>
</tr>
<tr>
<td>\t</td>
<td>制表符，也就是Tab键</td>
</tr>
</tbody></table>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 ~]$ echo &quot;hello\tworld&quot;</span><br><span class="line">hello\tworld</span><br><span class="line">[atguigu@hadoop101 ~]$ echo -e &quot;hello\tworld&quot;</span><br><span class="line">hello		world</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="8-2-14-head-显示文件头部内容"><a href="#8-2-14-head-显示文件头部内容" class="headerlink" title="8.2.14 head 显示文件头部内容"></a>8.2.14 head 显示文件头部内容</h3><p><strong>head用于显示文件的开头部分内容，默认情况下head指令显示文件的前10行内容。</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">head 文件	      （功能描述：查看文件头10行内容）</span><br><span class="line">head -n 5 文件      （功能描述：查看文件头5行内容，5可以是任意行数）</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n&lt;行数&gt;</td>
<td>指定显示头部内容的行数</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="8-2-15-tail-输出文件尾部内容"><a href="#8-2-15-tail-输出文件尾部内容" class="headerlink" title="8.2.15 tail 输出文件尾部内容"></a>8.2.15 tail 输出文件尾部内容</h3><p><strong>tail用于输出文件中尾部的内容，默认情况下tail指令显示文件的前10行内容。</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tail  文件 			（功能描述：查看文件尾10行内容）</span><br><span class="line">tail  -n 5 文件 		（功能描述：查看文件尾5行内容，5可以是任意行数）</span><br><span class="line">tail  -f  文件		（功能描述：实时追踪该文档的所有更新）</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n&lt;行数&gt;</td>
<td>输出文件尾部n行内容</td>
</tr>
<tr>
<td>-f</td>
<td>显示文件最新追加的内容，监视文件变化</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="8-2-16-gt-输出重定向和-gt-gt-追加"><a href="#8-2-16-gt-输出重定向和-gt-gt-追加" class="headerlink" title="8.2.16  &gt; 输出重定向和 &gt;&gt; 追加"></a>8.2.16  &gt; 输出重定向和 &gt;&gt; 追加</h3><ol>
<li><p><strong>基本语法</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ls -l &gt;文件		（功能描述：列表的内容写入文件a.txt中（覆盖写））</span><br><span class="line">ls -al &gt;&gt;文件		（功能描述：列表的内容追加到文件aa.txt的末尾）</span><br><span class="line">cat 文件1 &gt; 文件2	（功能描述：将文件1的内容覆盖到文件2）</span><br><span class="line">echo “内容” &gt;&gt; 文件</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">（1）将ls查看信息写入到文件中</span><br><span class="line">[root@hadoop101 ~]# ls -l&gt;houge.txt</span><br><span class="line">（2）将ls查看信息追加到文件中</span><br><span class="line">[root@hadoop101 ~]# ls -l&gt;&gt;houge.txt</span><br><span class="line">（3）采用echo将hello单词追加到文件中</span><br><span class="line">[root@hadoop101 ~]# echo hello&gt;&gt;houge.txt</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="8-2-17-ln-软链接"><a href="#8-2-17-ln-软链接" class="headerlink" title="8.2.17 ln 软链接"></a>8.2.17 ln 软链接</h3><p><strong>软链接也成为符号链接，类似于windows里的快捷方式，有自己的数据块，主要存放了链接其他文件的路径。</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>ln -s [原文件或目录] [软链接名]    （功能描述：给原文件创建一个软链接）</code></p>
</li>
<li><p><strong>经验技巧</strong></p>
<p>删除软链接： rm -rf 软链接名，而不是rm -rf 软链接名/</p>
<p>查询：通过ll就可以查看，列表属性<strong>第1位是l</strong>，尾部会有位置指向。</p>
</li>
</ol>
<h3 id="8-2-18-history-查看已经执行过历史命令"><a href="#8-2-18-history-查看已经执行过历史命令" class="headerlink" title="8.2.18 history 查看已经执行过历史命令"></a>8.2.18 history 查看已经执行过历史命令</h3><ol>
<li><p><strong>基本语法</strong></p>
<p><code>history                    （功能描述：查看已经执行过历史命令）</code></p>
</li>
</ol>
<h2 id="8-3-文件权限类"><a href="#8-3-文件权限类" class="headerlink" title="8.3 文件权限类"></a>8.3 文件权限类</h2><h3 id="8-3-1-文件属性"><a href="#8-3-1-文件属性" class="headerlink" title="8.3.1 文件属性"></a>8.3.1 文件属性</h3><p>Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。在Linux中我们可以使用ll或者ls -l命令来显示一个文件的属性以及文件所属的用户和组。</p>
<p>1．从左到右的10个字符表示：</p>
<p>​                             <img src="https://i.loli.net/2021/05/12/IDMlOfXtNHZiB5j.png" alt="image-20210512104500689">  </p>
<p><strong>如果没有权限，就会出现减号[ - ]而已</strong>。从左至右用0-9这些数字来表示:</p>
<ul>
<li><p>0首位表示类型</p>
<ul>
<li><p>在Linux中第一个字符代表这个文件是目录、文件或链接文件等等</p>
</li>
<li><p><strong>-代表文件</strong></p>
</li>
<li><p>d 代表目录</p>
</li>
<li><p><strong>l 链接文档(link file)；</strong></p>
</li>
</ul>
</li>
</ul>
<h3 id="8-3-2-chmod-改变权限"><a href="#8-3-2-chmod-改变权限" class="headerlink" title="8.3.2 chmod 改变权限"></a>8.3.2 chmod 改变权限</h3><ol>
<li><p><strong>基本语法</strong></p>
<ul>
<li><p>第一种方式变更权限</p>
<p>​        <code>chmod [{ugoa}{+-=}{rwx}] 文件或目录</code></p>
</li>
<li><p>第二种方式变更权限</p>
<p>​      <code>chmod [mode=421 ] [文件或目录]</code></p>
</li>
</ul>
</li>
<li><p><strong>经验技巧</strong></p>
<p>u:所有者 g:所有组 o:其他人 a:所有人(u、g、o的总和)</p>
<p>r=4 w=2 x=1    rwx=4+2+1=7</p>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">（1）修改文件使其所属主用户具有执行权限</span><br><span class="line">[root@hadoop101 ~]# cp xiyou&#x2F;dssz&#x2F;houge.txt .&#x2F;</span><br><span class="line">[root@hadoop101 ~]# chmod u+x houge.txt</span><br><span class="line">（2）修改文件使其所属组用户具有执行权限</span><br><span class="line">[root@hadoop101 ~]# chmod g+x houge.txt</span><br><span class="line">（3）修改文件所属主用户执行权限,并使其他用户具有执行权限</span><br><span class="line">[root@hadoop101 ~]# chmod u-x,o+x houge.txt</span><br><span class="line">（4）采用数字的方式，设置文件所有者、所属组、其他用户都具有可读可写可执行权限。</span><br><span class="line">[root@hadoop101 ~]# chmod 777 houge.txt</span><br><span class="line">（5）修改整个文件夹里面的所有文件的所有者、所属组、其他用户都具有可读可写可执行权限。</span><br><span class="line">[root@hadoop101 ~]# chmod -R 777 xiyou&#x2F;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="8-3-3-chown-改变所有者"><a href="#8-3-3-chown-改变所有者" class="headerlink" title="8.3.3 chown 改变所有者"></a>8.3.3 chown 改变所有者</h3><ol>
<li><p><strong>基本语法</strong></p>
<p><code>chown [选项] [最终用户] [文件或目录]      （功能描述：改变文件或者目录的所有者）</code></p>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-R</td>
<td>递归操作</td>
</tr>
</tbody></table>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">（1）修改文件所有者</span><br><span class="line">[root@hadoop101 ~]# chown atguigu houge.txt </span><br><span class="line">[root@hadoop101 ~]# ls -al</span><br><span class="line">-rwxrwxrwx. 1 atguigu root 551 5月  23 13:02 houge.txt</span><br><span class="line">（2）递归改变文件所有者和所有组</span><br><span class="line">[root@hadoop101 xiyou]# ll</span><br><span class="line">drwxrwxrwx. 2 root root 4096 9月   3 21:20 xiyou</span><br><span class="line">[root@hadoop101 xiyou]# chown -R atguigu:atguigu xiyou&#x2F;</span><br><span class="line">[root@hadoop101 xiyou]# ll</span><br><span class="line">drwxrwxrwx. 2 atguigu atguigu 4096 9月   3 21:20 xiyou</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="8-3-4-chgrp-改变所属组"><a href="#8-3-4-chgrp-改变所属组" class="headerlink" title="8.3.4 chgrp 改变所属组"></a>8.3.4 chgrp 改变所属组</h3><ol>
<li><p><strong>基本语法</strong></p>
<p><code>chgrp [最终用户组] [文件或目录]   （功能描述：改变文件或者目录的所属组）</code></p>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）修改文件的所属组</span><br><span class="line">[root@hadoop101 ~]# chgrp root houge.txt</span><br><span class="line">[root@hadoop101 ~]# ls -al</span><br><span class="line">-rwxrwxrwx. 1 atguigu root 551 5月  23 13:02 houge.txt</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="8-4-搜索查找类"><a href="#8-4-搜索查找类" class="headerlink" title="8.4 搜索查找类"></a>8.4 搜索查找类</h2><h3 id="8-4-1-find-查找文件或者目录"><a href="#8-4-1-find-查找文件或者目录" class="headerlink" title="8.4.1 find 查找文件或者目录"></a>8.4.1 find 查找文件或者目录</h3><p><strong>find指令将从指定目录向下递归地遍历其各个子目录，将满足条件的文件显示在终端。</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>find [搜索范围] [选项]</code></p>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-name&lt;查询方式&gt;</td>
<td>按照指定的文件名查找模式查找文件</td>
</tr>
<tr>
<td>-user&lt;用户名&gt;</td>
<td>查找属于指定用户名所有文件</td>
</tr>
<tr>
<td>-size&lt;文件大小&gt;</td>
<td>按照指定的文件大小查找文件。</td>
</tr>
</tbody></table>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">（1）按文件名：根据名称查找&#x2F;目录下的filename.txt文件。</span><br><span class="line">[root@hadoop101 ~]# find xiyou&#x2F; -name *.txt</span><br><span class="line">（2）按拥有者：查找&#x2F;opt目录下，用户名称为-user的文件</span><br><span class="line">[root@hadoop101 ~]# find xiyou&#x2F; -user atguigu</span><br><span class="line">（3）按文件大小：在&#x2F;home目录下查找大于200m的文件（+n 大于  -n小于   n等于）</span><br><span class="line">[root@hadoop101 ~]find &#x2F;home -size +204800</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>注意</strong></p>
<p>若报错显示<strong>find：路径必须在表达式之前：2</strong></p>
<p>则需要用转义字符’\‘</p>
<p><code>find xiyou/ -name \*.txt</code></p>
</li>
</ol>
<h3 id="8-4-2-locate快速定位文件路径"><a href="#8-4-2-locate快速定位文件路径" class="headerlink" title="8.4.2 locate快速定位文件路径"></a>8.4.2 locate快速定位文件路径</h3><p><strong>locate指令利用事先建立的系统中所有文件名称及路径的locate数据库实现快速定位给定的文件。Locate指令无需遍历整个文件系统，查询速度较快。为了保证查询结果的准确度，管理员必须定期更新locate时刻。</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>locate 搜索文件</code></p>
</li>
<li><p><strong>经验技巧</strong></p>
<p><strong>由于locate指令基于数据库进行查询，所以第一次运行前，必须使用updatedb指令创建locate数据库。</strong></p>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">查询文件夹</span><br><span class="line">[root@hadoop101 ~]# updatedb</span><br><span class="line">[root@hadoop101 ~]#locate tmp</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="8-4-3-grep-过滤查找及“-”管道符"><a href="#8-4-3-grep-过滤查找及“-”管道符" class="headerlink" title="8.4.3 grep 过滤查找及“|”管道符"></a>8.4.3 grep 过滤查找及“|”管道符</h3><p>管道符<strong>“|”</strong>，表示将前一个命令的处理结果输出传递给后面的命令处理</p>
<ol>
<li><p><strong>基本语法</strong></p>
<p><code>grep 选项 查找内容 源文件</code></p>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>显示匹配行及行号。</td>
</tr>
</tbody></table>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">查找某文件在第几行</span><br><span class="line">[root@hadoop101 ~]# ls | grep -n test</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="8-5-压缩和解压缩类"><a href="#8-5-压缩和解压缩类" class="headerlink" title="8.5 压缩和解压缩类"></a>8.5 压缩和解压缩类</h2><h3 id="8-5-1-gzip-gunzip-压缩"><a href="#8-5-1-gzip-gunzip-压缩" class="headerlink" title="8.5.1 gzip/gunzip 压缩"></a>8.5.1 gzip/gunzip 压缩</h3><ol>
<li><p><strong>基本语法</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gzip 文件		（功能描述：压缩文件，只能将文件压缩为*.gz文件）</span><br><span class="line">gunzip 文件.gz	（功能描述：解压缩文件命令）</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>经验技巧</strong></p>
<ol>
<li><p><strong>只能压缩文件不能压缩目录</strong></p>
</li>
<li><p><strong>不保留原来的文件</strong></p>
</li>
</ol>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">（1）gzip压缩</span><br><span class="line">[root@hadoop101 ~]# ls</span><br><span class="line">test.java</span><br><span class="line">[root@hadoop101 ~]# gzip houge.txt</span><br><span class="line">[root@hadoop101 ~]# ls</span><br><span class="line">houge.txt.gz</span><br><span class="line">（2）gunzip解压缩文件</span><br><span class="line">[root@hadoop101 ~]# gunzip houge.txt.gz </span><br><span class="line">[root@hadoop101 ~]# ls</span><br><span class="line">houge.txt</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h3 id="8-5-2-zip-unzip-压缩"><a href="#8-5-2-zip-unzip-压缩" class="headerlink" title="8.5.2 zip/unzip 压缩"></a>8.5.2 zip/unzip 压缩</h3><ol>
<li><p><strong>基本语法</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zip  [选项] XXX.zip  将要压缩的内容 		（功能描述：压缩文件和目录的命令）</span><br><span class="line">unzip [选项] XXX.zip						（功能描述：解压缩文件）</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>zip选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>压缩目录</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>unzip选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-d&lt;目录&gt;</td>
<td>指定解压后文件的存放目录</td>
</tr>
</tbody></table>
</li>
<li><p><strong>经验技巧</strong></p>
<p>zip 压缩命令在window/linux都通用，<strong>可以压缩目录且保留源文件</strong>。</p>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">（1）压缩 1.txt 和2.txt，压缩后的名称为mypackage.zip </span><br><span class="line">[root@hadoop101 opt]# touch bailongma.txt</span><br><span class="line">[root@hadoop101 ~]# zip houma.zip houge.txt bailongma.txt </span><br><span class="line">  adding: houge.txt (stored 0%)</span><br><span class="line">  adding: bailongma.txt (stored 0%)</span><br><span class="line">[root@hadoop101 opt]# ls</span><br><span class="line">houge.txt	bailongma.txt	houma.zip </span><br><span class="line">（2）解压 mypackage.zip</span><br><span class="line">[root@hadoop101 ~]# unzip houma.zip </span><br><span class="line">Archive:  houma.zip</span><br><span class="line"> extracting: houge.txt               </span><br><span class="line"> extracting: bailongma.txt       </span><br><span class="line">[root@hadoop101 ~]# ls</span><br><span class="line">houge.txt	bailongma.txt	houma.zip </span><br><span class="line">（3）解压mypackage.zip到指定目录-d</span><br><span class="line">[root@hadoop101 ~]# unzip houma.zip -d &#x2F;opt</span><br><span class="line">[root@hadoop101 ~]# ls &#x2F;opt&#x2F;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h3 id="8-5-3-tar-打包"><a href="#8-5-3-tar-打包" class="headerlink" title="8.5.3 tar 打包"></a>8.5.3 tar 打包</h3><ol>
<li><p><strong>基本语法</strong></p>
<p><code>tar [选项] XXX.tar.gz 将要打包进去的内容      （功能描述：打包目录，压缩后的文件格式.tar.gz）</code></p>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-c</td>
<td>产生.tar打包文件</td>
</tr>
<tr>
<td>-v</td>
<td>显示详细信息</td>
</tr>
<tr>
<td>-f</td>
<td>指定压缩后的文件名</td>
</tr>
<tr>
<td>-z</td>
<td>打包同时压缩</td>
</tr>
<tr>
<td>-x</td>
<td>解包.tar文件</td>
</tr>
</tbody></table>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1）压缩多个文件</span><br><span class="line">[root@hadoop101 opt]# tar -zcvf houma.tar.gz houge.txt bailongma.txt </span><br><span class="line">houge.txt</span><br><span class="line">bailongma.txt</span><br><span class="line">[root@hadoop101 opt]# ls</span><br><span class="line">houma.tar.gz houge.txt bailongma.txt </span><br><span class="line">（2）压缩目录</span><br><span class="line">[root@hadoop101 ~]# tar -zcvf xiyou.tar.gz xiyou&#x2F;</span><br><span class="line">xiyou&#x2F;</span><br><span class="line">xiyou&#x2F;mingjie&#x2F;</span><br><span class="line">xiyou&#x2F;dssz&#x2F;</span><br><span class="line">xiyou&#x2F;dssz&#x2F;houge.txt</span><br><span class="line">（3）解压到当前目录</span><br><span class="line">[root@hadoop101 ~]# tar -zxvf houma.tar.gz</span><br><span class="line">（4）解压到指定目录</span><br><span class="line">[root@hadoop101 ~]# tar -zxvf xiyou.tar.gz -C &#x2F;opt</span><br><span class="line">[root@hadoop101 ~]# ll &#x2F;opt&#x2F;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="8-6-进程线程类"><a href="#8-6-进程线程类" class="headerlink" title="8.6 进程线程类"></a>8.6 进程线程类</h2><p>进程是正在执行的一个程序或命令，每一个进程都是一个运行的实体，都有自己的地址空间，并占用一定的系统资源。</p>
<h3 id="8-6-1-ps-查看当前系统进程状态"><a href="#8-6-1-ps-查看当前系统进程状态" class="headerlink" title="8.6.1 ps 查看当前系统进程状态"></a>8.6.1 ps 查看当前系统进程状态</h3><ol>
<li><p><strong>基本语法</strong></p>
<p><code>ps -ef | grep xxx      （功能描述：可以查看子父进程之间的关系）</code></p>
</li>
<li><p><strong>功能说明</strong></p>
<p><strong>ps -ef显示信息说明</strong></p>
<p><strong>UID：用户ID</strong> </p>
<p><strong>PID：进程ID</strong> </p>
<p><strong>PPID：父进程ID</strong> </p>
<p><strong>C：CPU用于计算执行优先级的因子。数值越大，表明进程是CPU密集型运算，执行优先级会降低；数值越小，表明进程是I/O密集型运算，执行优先级会提高</strong> </p>
<p><strong>STIME：进程启动的时间</strong></p>
<p><strong>TTY：完整的终端名称</strong></p>
<p><strong>TIME：CPU时间</strong> </p>
<p><strong>CMD：启动进程所用的命令和参数</strong></p>
</li>
</ol>
<h3 id="8-6-2-kill-终止进程"><a href="#8-6-2-kill-终止进程" class="headerlink" title="8.6.2 kill 终止进程"></a>8.6.2 kill 终止进程</h3><ol>
<li><p><strong>基本语法</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kill  [选项] 进程号		（功能描述：通过进程号杀死进程）</span><br><span class="line">killall 进程名称			（功能描述：通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时很有用）</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-9</td>
<td>表示强迫进程立即停止</td>
</tr>
</tbody></table>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）杀死浏览器进程</span><br><span class="line">[root@hadoop101 桌面]# kill -9 5102</span><br><span class="line">（2）通过进程名称杀死进程</span><br><span class="line">[root@hadoop101 桌面]# killall firefox</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="8-6-3-top-查看系统健康状态"><a href="#8-6-3-top-查看系统健康状态" class="headerlink" title="8.6.3 top 查看系统健康状态"></a>8.6.3 top 查看系统健康状态</h3><ol>
<li><p><strong>基本语法</strong></p>
<p><code>top [选项]</code></p>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-d 秒数</td>
<td>指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令：</td>
</tr>
<tr>
<td>-i</td>
<td>使top不显示任何闲置或者僵死进程。</td>
</tr>
<tr>
<td>-p</td>
<td>通过指定监控进程ID来仅仅监控某个进程的状态。</td>
</tr>
</tbody></table>
</li>
<li><p><strong>操作说明</strong></p>
<table>
<thead>
<tr>
<th>操作</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>P</td>
<td>以CPU使用率排序，默认就是此项</td>
</tr>
<tr>
<td>M</td>
<td>以内存的使用率排序</td>
</tr>
<tr>
<td>N</td>
<td>以PID排序</td>
</tr>
<tr>
<td>q</td>
<td>退出top</td>
</tr>
</tbody></table>
</li>
<li><p><strong>查询结果字段显示</strong></p>
<p><strong>第一行信息为任务队列信息</strong></p>
<table>
<thead>
<tr>
<th>内容</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>12:26:46</td>
<td>系统当前时间</td>
</tr>
<tr>
<td>up 1 day, 13:32</td>
<td>系统的运行时间，本机已经运行1天  13小时32分钟</td>
</tr>
<tr>
<td>2 users</td>
<td>当前登录了两个用户</td>
</tr>
<tr>
<td>load average:   0.00, 0.00, 0.00</td>
<td>系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。</td>
</tr>
</tbody></table>
<p><strong>第二行为进程信息</strong></p>
<table>
<thead>
<tr>
<th>Tasks: 95 total</th>
<th>系统中的进程总数</th>
</tr>
</thead>
<tbody><tr>
<td>1 running</td>
<td>正在运行的进程数</td>
</tr>
<tr>
<td>94 sleeping</td>
<td>睡眠的进程</td>
</tr>
<tr>
<td>0 stopped</td>
<td>正在停止的进程</td>
</tr>
<tr>
<td>0 zombie</td>
<td>僵尸进程。如果不是0，需要手工检查僵尸进程</td>
</tr>
</tbody></table>
<p><strong>第三行为CPU信息</strong></p>
<table>
<thead>
<tr>
<th>Cpu(s): 0.1%us</th>
<th>用户模式占用的CPU百分比</th>
</tr>
</thead>
<tbody><tr>
<td>0.1%sy</td>
<td>系统模式占用的CPU百分比</td>
</tr>
<tr>
<td>0.0%ni</td>
<td>改变过优先级的用户进程占用的CPU百分比</td>
</tr>
<tr>
<td>99.7%id</td>
<td>空闲CPU的CPU百分比</td>
</tr>
<tr>
<td>0.1%wa</td>
<td>等待输入/输出的进程的占用CPU百分比</td>
</tr>
<tr>
<td>0.0%hi</td>
<td>硬中断请求服务占用的CPU百分比</td>
</tr>
<tr>
<td>0.1%si</td>
<td>软中断请求服务占用的CPU百分比</td>
</tr>
<tr>
<td>0.0%st</td>
<td>st（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。</td>
</tr>
</tbody></table>
<p><strong>第四行为物理内存信息</strong></p>
<table>
<thead>
<tr>
<th>Mem:  625344k total</th>
<th>物理内存的总量，单位KB</th>
</tr>
</thead>
<tbody><tr>
<td>571504k used</td>
<td>已经使用的物理内存数量</td>
</tr>
<tr>
<td>53840k free</td>
<td>空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了</td>
</tr>
<tr>
<td>65800k buffers</td>
<td>作为缓冲的内存数量</td>
</tr>
</tbody></table>
<p><strong>第五行为交换分区（swap）信息</strong></p>
<table>
<thead>
<tr>
<th>Swap:  524280k total</th>
<th>交换分区（虚拟内存）的总大小</th>
</tr>
</thead>
<tbody><tr>
<td>0k used</td>
<td>已经使用的交互分区的大小</td>
</tr>
<tr>
<td>524280k free</td>
<td>空闲交换分区的大小</td>
</tr>
<tr>
<td>409280k cached</td>
<td>作为缓存的交互分区的大小</td>
</tr>
</tbody></table>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 atguigu]# top -d 1</span><br><span class="line">[root@hadoop101 atguigu]# top -i</span><br><span class="line">[root@hadoop101 atguigu]# top -p 2575</span><br><span class="line">执行上述命令后，可以按P、M、N对查询出的进程结果进行排序。</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="8-6-4-netstat-显示网络统计信息和端口占用情况"><a href="#8-6-4-netstat-显示网络统计信息和端口占用情况" class="headerlink" title="8.6.4 netstat 显示网络统计信息和端口占用情况"></a>8.6.4 netstat 显示网络统计信息和端口占用情况</h3><ol>
<li><p><strong>基本语法</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">netstat -anp |grep 进程号	（功能描述：查看该进程网络信息）</span><br><span class="line">netstat -nlp	| grep 端口号	（功能描述：查看网络端口号占用情况）</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>选项说明</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>拒绝显示别名，能显示数字的全部转化成数字</td>
</tr>
<tr>
<td>-l</td>
<td>仅列出有在listen（监听）的服务状态</td>
</tr>
<tr>
<td>-p</td>
<td>表示显示哪个进程在调用</td>
</tr>
</tbody></table>
</li>
<li><p><strong>案例实操</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">（1）通过进程号查看该进程的网络信息</span><br><span class="line">[root@hadoop101 hadoop-2.7.2]# netstat -anp | grep 火狐浏览器进程号</span><br><span class="line">unix  2      [ ACC ]     STREAM     LISTENING     20670  3115&#x2F;firefox        &#x2F;tmp&#x2F;orbit-root&#x2F;linc-c2b-0-5734667cbe29</span><br><span class="line">unix  3      [ ]         STREAM     CONNECTED     20673  3115&#x2F;firefox        &#x2F;tmp&#x2F;orbit-root&#x2F;linc-c2b-0-5734667cbe29</span><br><span class="line">unix  3      [ ]         STREAM     CONNECTED     20668  3115&#x2F;firefox        </span><br><span class="line">unix  3      [ ]         STREAM     CONNECTED     20666  3115&#x2F;firefox     </span><br><span class="line"></span><br><span class="line">（2）查看某端口号是否被占用</span><br><span class="line">[root@hadoop101 桌面]# netstat -nlp | grep 20670 </span><br><span class="line">unix  2      [ ACC ]     STREAM     LISTENING     20670  3115&#x2F;firefox        &#x2F;tmp&#x2F;orbit-root&#x2F;linc-c2b-0-5734667cbe29</span><br></pre></td></tr></table></figure>

</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://extrali.com/2020/11/09/Hadoop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="黎达">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Extrali">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/09/Hadoop/" class="post-title-link" itemprop="url">Hadoop</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>

              <time title="Erstellt: 2020-11-09 15:01:57" itemprop="dateCreated datePublished" datetime="2020-11-09T15:01:57+08:00">2020-11-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Bearbeitet am</span>
                <time title="Geändert am: 2021-08-28 16:32:51" itemprop="dateModified" datetime="2021-08-28T16:32:51+08:00">2021-08-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>title: Hadoop<br>date: 2020-11-09 15:01:57<br>tags: Hadoop<br>categories: 大数据</p>
<h1 id="1-大数据基础"><a href="#1-大数据基础" class="headerlink" title="1. 大数据基础"></a>1. 大数据基础</h1><ul>
<li>大数据主要解决<strong>海量数据的存储</strong>和<strong>海量数据的分析计算</strong>问题。</li>
</ul>
<h2 id="1-1-大数据特点-4V"><a href="#1-1-大数据特点-4V" class="headerlink" title="1.1 大数据特点(4V)"></a>1.1 大数据特点(4V)</h2><ol>
<li><p><strong>Volume(大量)</strong></p>
</li>
<li><p><strong>Velocity(高速)</strong></p>
</li>
<li><p><strong>Variety(多样)</strong></p>
</li>
<li><p><strong>Value(低价值密度)</strong></p>
</li>
</ol>
<h2 id="1-2-大数据部门组织结构"><a href="#1-2-大数据部门组织结构" class="headerlink" title="1.2 大数据部门组织结构"></a>1.2 大数据部门组织结构</h2><p><img src="https://i.loli.net/2020/11/10/xN3YQGhVe8RJMSZ.png" alt="image-20201110102636205.png"></p>
<h1 id="2-Hadoop"><a href="#2-Hadoop" class="headerlink" title="2. Hadoop"></a>2. Hadoop</h1><h2 id="2-1-Hadoop的优势"><a href="#2-1-Hadoop的优势" class="headerlink" title="2.1 Hadoop的优势"></a>2.1 Hadoop的优势</h2><p><img src="https://i.loli.net/2020/11/10/GEtHTOdyblsQNuV.png" alt="image-20201110111905495"></p>
<h2 id="2-2-Hadoop组成"><a href="#2-2-Hadoop组成" class="headerlink" title="2.2 Hadoop组成"></a>2.2 Hadoop组成</h2><p><img src="https://i.loli.net/2020/11/10/z5bQgjJWA8FLYv3.png" alt="image-20201110111726171"></p>
<h3 id="2-2-1-HDFS架构概述"><a href="#2-2-1-HDFS架构概述" class="headerlink" title="2.2.1 HDFS架构概述"></a>2.2.1 HDFS架构概述</h3><ol>
<li><strong>NameNode(nn)：</strong>存储文件的<strong>元数据</strong>，如文件名，文件目录结构，文件属性(生成时间，副本数，文件权限)，以及每个文件的块列表和块所在的DataNode等。</li>
<li><strong>DataNode(dn)：**</strong>在本地文件系统存储文件块数据<strong>，以及</strong>块数据的校验和**。</li>
<li><strong>Secondary NameNode(2nn)：</strong>用来<strong>监控HDFS状态的辅助后台程序</strong>，<strong>每隔一段时间对NameNode元数据备份</strong>。</li>
</ol>
<h3 id="2-2-2-YARN架构"><a href="#2-2-2-YARN架构" class="headerlink" title="2.2.2 YARN架构"></a>2.2.2 YARN架构</h3><p><img src="https://i.loli.net/2021/05/16/VZfFIjm78HtxuBD.png" alt="image-20210516202206233"></p>
<h3 id="2-2-3-MapReduce架构"><a href="#2-2-3-MapReduce架构" class="headerlink" title="2.2.3 MapReduce架构"></a>2.2.3 MapReduce架构</h3><p><strong>MapReduce将计算过程分为两个阶段：Map和Reduce</strong></p>
<ol>
<li>Map阶段并行处理输入数据</li>
<li>Reduce阶段对Map结果进行汇总</li>
</ol>
<p><img src="https://i.loli.net/2021/05/16/OoELtUk37im6Xcv.png" alt="image-20210516202253968"></p>
<h3 id="2-2-4-HDFS、YARN、MapReduce三者关系"><a href="#2-2-4-HDFS、YARN、MapReduce三者关系" class="headerlink" title="2.2.4 HDFS、YARN、MapReduce三者关系"></a>2.2.4 HDFS、YARN、MapReduce三者关系</h3><p><img src="https://i.loli.net/2021/05/16/EXsUKzAtHnlFGWD.png" alt="image-20210516202503543"></p>
<h2 id="2-3-大数据技术生态体系"><a href="#2-3-大数据技术生态体系" class="headerlink" title="2.3 大数据技术生态体系"></a>2.3 大数据技术生态体系</h2><p><img src="https://i.loli.net/2020/11/10/HaPX9uErInL4z6k.png" alt="image-20201110113503546"></p>
<ol>
<li><p><strong>Sqoop</strong>：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库(MySql)间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p>
</li>
<li><p><strong>Flume</strong>：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</p>
</li>
<li><p><strong>Kafka</strong>：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：</p>
<ol>
<li>通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。</li>
<li>高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。</li>
<li>支持通过Kafka服务器和消费机集群来分区消息。、</li>
<li>支持Hadoop并行数据加载。</li>
</ol>
</li>
<li><p><strong>Storm</strong>：Storm用于“连续计算”，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。</p>
</li>
<li><p><strong>Spark</strong>：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。</p>
</li>
<li><p><strong>Oozie</strong>：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。</p>
</li>
<li><p><strong>Hbase</strong>：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</p>
</li>
<li><p><strong>Hive</strong>：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p>
</li>
<li><p><strong>ZooKeeper</strong>：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</p>
</li>
</ol>
<h1 id="3-Hadoop运行环境搭建"><a href="#3-Hadoop运行环境搭建" class="headerlink" title="3. Hadoop运行环境搭建"></a>3. Hadoop运行环境搭建</h1><h2 id="3-1-运行环境搭建"><a href="#3-1-运行环境搭建" class="headerlink" title="3.1 运行环境搭建"></a>3.1 运行环境搭建</h2><p><strong>一.安装Linux</strong></p>
<ol>
<li><p>安装Linux虚拟机</p>
</li>
<li><p>关闭防火墙</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">关闭防火墙</span><br><span class="line">    sudo service iptables stop</span><br><span class="line">    sudo chkconfig iptables off</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置静态IP，改主机名</p>
<ol>
<li><p>修改IP</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33</span><br><span class="line"></span><br><span class="line"># 修改</span><br><span class="line">DEVICE&#x3D;eth0</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">NAME&#x3D;&quot;eth0&quot;</span><br><span class="line">IPADDR&#x3D;192.168.2.101     # 要看自己的网段</span><br><span class="line">PREFIX&#x3D;24</span><br><span class="line">GATEWAY&#x3D;192.168.2.2</span><br><span class="line">DNS1&#x3D;192.168.2.2</span><br></pre></td></tr></table></figure>
</li>
<li><p>改主机名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;sysconfig&#x2F;network</span><br><span class="line"></span><br><span class="line">改HOSTNAME&#x3D;那一行</span><br><span class="line"></span><br><span class="line">直接使用指令hostnamectl set-hostname [主机名]即可</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>配置hosts文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;hosts</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">192.168.2.110   hadoop001</span><br><span class="line">192.168.2.111   hadoop002</span><br><span class="line">192.168.2.112   hadoop003</span><br><span class="line">192.168.2.113   hadoop004</span><br><span class="line">192.168.2.114   hadoop005</span><br><span class="line">192.168.2.115   hadoop006</span><br><span class="line">192.168.2.116   hadoop007</span><br><span class="line">192.168.2.117   hadoop008</span><br><span class="line">192.168.2.118   hadoop009</span><br><span class="line">192.168.2.119   hadoop010</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建用户名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">创建一个一般用户bigdata，给他配置密码</span><br><span class="line">    useradd bigdata</span><br><span class="line">    passwd  bigdata</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置用户为sudoers</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;sudoers</span><br><span class="line"> </span><br><span class="line">修改&#x2F;etc&#x2F;sudoers 文件，在%wheel 这行下面添加一行，如下所示：</span><br><span class="line">## Allow root to run any commands anywhere</span><br><span class="line">root ALL&#x3D;(ALL) ALL</span><br><span class="line">## Allows people in group wheel to run all commands</span><br><span class="line">%wheel ALL&#x3D;(ALL) ALL</span><br><span class="line">atguigu ALL&#x3D;(ALL) NOPASSWD:ALL</span><br><span class="line">注意： atguigu 这一行不要直接放到 root 行下面，因为所有用户都属于 wheel 组，你先</span><br><span class="line">配置了 atguigu 具有免密功能，但是程序执行到%wheel 行时， 该功能又被覆盖回需要</span><br><span class="line">密码。所以 atguigu 要放到%wheel 这行下面。</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建文件夹，并赋予权限给新建的用户名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir &#x2F;opt&#x2F;module &#x2F;opt&#x2F;software</span><br><span class="line">chown bigdata:bigdata &#x2F;opt&#x2F;module &#x2F;opt&#x2F;software</span><br></pre></td></tr></table></figure>


</li>
</ol>
<p><strong>二.安装JDK</strong></p>
<blockquote>
<p> 在开始安装前，需要检查一下现有的JDK</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 查询是否安装Java软件：</span><br><span class="line">[root@hadoop001 ~]# rpm -qa | grep java</span><br><span class="line">java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64</span><br><span class="line">tzdata-java-2013g-1.el6.noarch</span><br><span class="line">java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64</span><br><span class="line"></span><br><span class="line"># 如果安装的版本低于1.7，卸载该JDK：</span><br><span class="line">[bigdata@hadoop001 jdk1.8.0_144]$ rpm -qa | grep java | xargs sudo rpm -e --nodeps</span><br><span class="line"># 查看JDK安装路径：</span><br><span class="line">[atguigu@hadoop101 ~]$ which java</span><br></pre></td></tr></table></figure>

<ol>
<li><p><strong>将JDK和Hadoop的压缩包传到/opt/software/目录下</strong></p>
</li>
<li><p>解压JDK到<strong>/opt/module</strong>目录下</p>
<p><code>tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/</code></p>
</li>
<li><p>配置JDK环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">打开&#x2F;etc&#x2F;profile文件</span><br><span class="line">[atguigu@hadoop101 software]$ sudo vi &#x2F;etc&#x2F;profile</span><br><span class="line">在profile文件末尾添加JDK路径</span><br><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144</span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br><span class="line"></span><br><span class="line">让修改后的文件生效</span><br><span class="line">[atguigu@hadoop101 jdk1.8.0_144]$ source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置hadoop同理，但是在profile文件后需要加上的是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#HADOOP_HOME</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><strong>三.克隆虚拟机</strong></p>
<ul>
<li>将hadoop101虚拟机进行克隆</li>
<li>修改主机的主机名</li>
<li>修改IP地址和MAC地址</li>
</ul>
<h2 id="3-2-Hadoop目录结构"><a href="#3-2-Hadoop目录结构" class="headerlink" title="3.2 Hadoop目录结构"></a>3.2 Hadoop目录结构</h2><ol>
<li><p><strong>查看Hadoop目录结构</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 hadoop-2.7.2]$ ll</span><br><span class="line">总用量 52</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 bin</span><br><span class="line">drwxr-xr-x. 3 atguigu atguigu 4096 5月 22 2017 etc</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 include</span><br><span class="line">drwxr-xr-x. 3 atguigu atguigu 4096 5月 22 2017 lib</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 libexec</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 15429 5月 22 2017 LICENSE.txt</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  101 5月 22 2017 NOTICE.txt</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 1366 5月 22 2017 README.txt</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 sbin</span><br><span class="line">drwxr-xr-x. 4 atguigu atguigu 4096 5月 22 2017 share</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>重要目录</strong></p>
<ol>
<li>bin目录：存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本</li>
<li>etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件</li>
<li>lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）</li>
<li>sbin目录：存放启动或停止Hadoop相关服务的脚本</li>
<li>share目录：存放Hadoop的依赖jar包、文档、和官方案例</li>
</ol>
</li>
</ol>
<h1 id="4-Hadoop运行模式"><a href="#4-Hadoop运行模式" class="headerlink" title="4. Hadoop运行模式"></a>4. Hadoop运行模式</h1><ol>
<li>Hadoop 官方网站： <a href="http://hadoop.apache.org/" target="_blank" rel="noopener">http://hadoop.apache.org/</a>  </li>
<li>Hadoop运行模式包括：<strong>本地模式</strong>、<strong>伪分布式模式</strong>以及<strong>完全分布式模式</strong><ul>
<li><strong>本地模式：</strong>单机运行，只是用来演示以下官方案例。</li>
<li><strong>伪分布式模式：</strong>也是单机运行，但是具备 Hadoop 集群的所有功能， 一台服务器模拟一个分布式的环境。</li>
<li><strong>完全分布式模式：</strong>多态服务器组成分布式环境</li>
</ul>
</li>
</ol>
<h3 id="4-1-本地模式运行（官方WordCount）"><a href="#4-1-本地模式运行（官方WordCount）" class="headerlink" title="4.1 本地模式运行（官方WordCount）"></a>4.1 本地模式运行（官方WordCount）</h3><ol>
<li><p>在hadoop-3.1.3 文件下面创建一个 wcinput 文件夹 ，并在其中创建word.txt</p>
</li>
<li><p>执行程序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar</span><br><span class="line">share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar</span><br><span class="line">wordcount wcinput wcoutput</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="4-2-完全分布式运行模式"><a href="#4-2-完全分布式运行模式" class="headerlink" title="4.2 完全分布式运行模式"></a>4.2 完全分布式运行模式</h2><ol>
<li><strong>配置集群</strong></li>
<li><strong>单点启动</strong></li>
<li><strong>配置ssh</strong></li>
<li><strong>群起并测试集群</strong></li>
</ol>
<h3 id="4-2-1-编写集群分发脚本xsync"><a href="#4-2-1-编写集群分发脚本xsync" class="headerlink" title="4.2.1 编写集群分发脚本xsync"></a>4.2.1 编写集群分发脚本xsync</h3><ol>
<li><p><strong>scp（secure copy）安全拷贝</strong></p>
<ol>
<li><p><strong>scp定义</strong></p>
<p>scp可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）</p>
</li>
<li><p><strong>基本语法</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp  -r   $pdir&#x2F;$fname       $user@$host:$pdir&#x2F;$fname</span><br><span class="line">命令 递归 要拷贝的文件路径&#x2F;名称 目的地用户@主机:目的地路径&#x2F;名称</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>案例实操</strong></p>
<ul>
<li><p>在 hadoop102 上， 将 hadoop102 中/opt/module/jdk1.8.0_212 目录拷贝到<br>hadoop103 上。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ scp -r &#x2F;opt&#x2F;module&#x2F;jdk1.8.0_212 atguigu@hadoop103:&#x2F;opt&#x2F;module</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 hadoop103 上， 将 hadoop102 中/opt/module/hadoop-3.1.3 目录拷贝到<br>hadoop103 上。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 ~]$ scp -r atguigu@hadoop102:&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3 &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 hadoop103 上操作， 将 hadoop102 中/opt/module 目录下所有目录拷贝到<br>hadoop104 上。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 opt]$ scp -r atguigu@hadoop102:&#x2F;opt&#x2F;module&#x2F;* atguigu@hadoop104:&#x2F;opt&#x2F;module</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
</li>
<li><p><strong>rsync远程同步工具</strong></p>
<p><strong>rsync</strong>主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。</p>
<p><strong>rsync和scp的区别：</strong>用rsync做文件复制要比scp的速度快，<strong>rsync只对差异文件做更新</strong>。<strong>scp是把所有文件都复制过去</strong>。</p>
<ol>
<li><p><strong>基本语法</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rsync  -av      $pdir&#x2F;$fname       $user@$host:$pdir&#x2F;$fname</span><br><span class="line">命令  选项参数 要拷贝的文件路径&#x2F;名称   目的地用户@主机:目的地路径&#x2F;名称</span><br></pre></td></tr></table></figure>

<p><strong>选项参数说明：</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>归档拷贝</td>
</tr>
<tr>
<td>-v</td>
<td>显示复制过程</td>
</tr>
</tbody></table>
</li>
</ol>
</li>
<li><p><strong>xsync集群分发脚本</strong></p>
<ol>
<li><p><strong>需求：</strong>循环复制文件到所有节点的相同目录下</p>
</li>
<li><p><strong>需求分析：</strong></p>
<p>（a）rsync命令原始拷贝：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync -av &#x2F;opt&#x2F;module atguigu@hadoop103:&#x2F;opt&#x2F;</span><br></pre></td></tr></table></figure>

<p>（b）期望脚本</p>
<p><code>xsync 要同步的文件名称</code></p>
<p>（c）期望脚本在任何路径都能使用（脚本放在声明了全局环境变量的路径）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[extrali@hadoop103 test]$ echo $PATH</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;sbin:&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144&#x2F;bin:&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;bin:&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;sbin:&#x2F;home&#x2F;extrali&#x2F;.local&#x2F;bin:&#x2F;home&#x2F;extrali&#x2F;bin</span><br></pre></td></tr></table></figure>
</li>
<li><p>脚本实现</p>
<p>（a）在/home/extrali/bin目录下创建xsync文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[extrali@hadoop102 ~]$ pwd</span><br><span class="line">&#x2F;home&#x2F;extrali</span><br><span class="line">[extrali@hadoop102 ~]$ mkdir bin&#x2F;</span><br><span class="line">[extrali@hadoop102 ~]$ cd bin&#x2F;</span><br><span class="line">[extrali@hadoop102 bin]$ vim xsync</span><br></pre></td></tr></table></figure>

<p>在该文件中编写如下代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">#1. 判断参数个数</span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">  echo Not Enough Arguement!</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line">#2. 遍历集群所有机器</span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">  echo &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; $host &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">  #3. 遍历所有目录，挨个发送</span><br><span class="line">  for file in $@</span><br><span class="line">  do</span><br><span class="line">    #4. 判断文件是否存在</span><br><span class="line">    if [ -e $file ]</span><br><span class="line">      then</span><br><span class="line">        #5. 获取父目录</span><br><span class="line">        pdir&#x3D;$(cd -P $(dirname $file); pwd)</span><br><span class="line">        #6. 获取当前文件的名称</span><br><span class="line">        fname&#x3D;$(basename $file)</span><br><span class="line">        ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">        rsync -av $pdir&#x2F;$fname $host:$pdir</span><br><span class="line">      else</span><br><span class="line">        echo $file does not exists!</span><br><span class="line">    fi</span><br><span class="line">  done</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>（b）修改脚本xsync具有执行权限</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[extrali@hadoop102 bin]$ chmod +x xsync</span><br></pre></td></tr></table></figure>

<p>（c）测试脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync &#x2F;home&#x2F;extrali&#x2F;</span><br></pre></td></tr></table></figure>

<p>（d）将脚本复制到/bin中，以便全局调用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp xsync &#x2F;bin&#x2F;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h3 id="4-2-2-SSH无密登录"><a href="#4-2-2-SSH无密登录" class="headerlink" title="4.2.2 SSH无密登录"></a>4.2.2 SSH无密登录</h3><ol>
<li><p><strong>配置ssh</strong></p>
<ol>
<li><p><strong>基本语法</strong></p>
<p> <code>ssh 另一台电脑的IP地址</code></p>
</li>
</ol>
</li>
<li><p><strong>无密钥配置</strong></p>
<ol>
<li><p>免密登录原理</p>
<p><img src="https://i.loli.net/2021/05/19/Qqnd7EZs3bOaluU.png" alt="image-20210519145309259"></p>
</li>
<li><p><strong>生成公钥和私钥</strong></p>
<p><code>ll -a</code>可以<strong>显示隐藏文件</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[extrali@hadoop102 .ssh]$ pwd</span><br><span class="line">&#x2F;home&#x2F;extrali&#x2F;.ssh</span><br><span class="line">[extrali@hadoop102 .ssh]$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>

<p>然后敲（三个回车），就会生成两个文件 <strong>id_rsa（私钥）</strong>、 <strong>id_rsa.pub（公钥）</strong>  </p>
</li>
<li><p>将公钥拷贝到要免密登录的目标机器上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102</span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103</span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<ul>
<li>还需要在 hadoop103 上采用 extrali 账号配置一下无密登录到 hadoop102、 hadoop103、<br>hadoop104 服务器上。</li>
<li>还需要在 hadoop104 上采用 extrali号配置一下无密登录到 hadoop102、 hadoop103、<br>hadoop104 服务器上。</li>
<li>还需要在 hadoop102 上采用 root 账号，配置一下无密登录到 hadoop102、 hadoop103、<br>hadoop104；  </li>
</ul>
</li>
</ol>
</li>
<li><p>.ssh文件夹下（~/.ssh）的文件功能解释</p>
<table>
<thead>
<tr>
<th><strong>known_hosts</strong></th>
<th><strong>记录 ssh 访问过计算机的公钥（public key）</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>id_rsa</strong></td>
<td><strong>生成的私钥</strong></td>
</tr>
<tr>
<td><strong>id_rsa.pub</strong></td>
<td><strong>生成的公钥</strong></td>
</tr>
<tr>
<td><strong>authorized_keys</strong></td>
<td><strong>存放授权过的无密登录服务器公钥</strong></td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="4-2-3-集群部署"><a href="#4-2-3-集群部署" class="headerlink" title="4.2.3 集群部署"></a>4.2.3 集群部署</h3><ol>
<li><p><strong>集群部署规划</strong></p>
<p><strong>注意：</strong></p>
<ul>
<li><strong>NameNode</strong> 和 <strong>SecondaryNameNode</strong> 不要安装在同一台服务器；</li>
<li><strong>ResourceManager</strong> 也很消耗内存，不要和 <strong>NameNode</strong>、 <strong>SecondaryNameNode</strong> 配置在<br>同一台机器上。  </li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td><strong>NameNode</strong> DataNode</td>
<td>DataNode</td>
<td><strong>SecondaryNameNode</strong> DataNode</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td><strong>ResourceManager</strong> NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
</li>
<li><p><strong>配置文件说明</strong></p>
<p>Hadoop 配置文件分两类：<strong>默认配置文件</strong>和<strong>自定义配置文件</strong>，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。</p>
<ul>
<li><p><strong>默认配置文件：</strong></p>
<table>
<thead>
<tr>
<th>要获取的默认文件</th>
<th>文件存放在 Hadoop 的 jar 包中的位置</th>
</tr>
</thead>
<tbody><tr>
<td>[core-default.xml]</td>
<td>hadoop-common-3.1.3.jar/core-default.xml</td>
</tr>
<tr>
<td>[hdfs-default.xml]</td>
<td>hadoop-hdfs-3.1.3.jar/hdfs-default.xml</td>
</tr>
<tr>
<td>[yarn-default.xml]</td>
<td>hadoop-yarn-common-3.1.3.jar/yarn-default.xml</td>
</tr>
<tr>
<td>[mapred-default.xml]</td>
<td>hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml</td>
</tr>
</tbody></table>
</li>
<li><p><strong>自定义配置文件：</strong></p>
<p><strong>core-site.xml</strong>、 <strong>hdfs-site.xml</strong>、 <strong>yarn-site.xml</strong>、 <strong>mapred-site.xml</strong> 四个配置文件存放在<br><strong>$HADOOP_HOME/etc/hadoop</strong> 这个路径上， 用户可以根据项目需求重新进行修改配置。  </p>
</li>
</ul>
</li>
<li><p><strong>配置集群</strong></p>
<ol>
<li><p><strong>核心配置文件</strong></p>
<p>配置<strong>core-site.xml</strong>，配置为如下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定 NameNode 的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;hdfs:&#x2F;&#x2F;hadoop102:8020&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 指定 hadoop 数据的存储目录 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;data&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 配置 HDFS 网页登录使用的静态用户为 extrali --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.http.staticuser.user&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;extrali&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>HDFS配置文件</strong></p>
<p>配置<strong>hdfs-site.xml</strong>，配置为如下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- nn web 端访问地址--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.http-address&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;hadoop102:9870&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 2nn web 端访问地址--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;hadoop104:9868&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>YARN配置文件</strong></p>
<p>配置<strong>yarn-site.xml</strong>，配置为如下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定 MR 走 shuffle --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 指定 ResourceManager 的地址--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;hadoop103&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 环境变量的继承 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.env-whitelist&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CO</span><br><span class="line">NF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAP</span><br><span class="line">RED_HOME&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>MapReduce配置文件</strong></p>
<p>配置<strong>mapred-site.xml</strong>，配置为如下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定 MapReduce 程序运行在 Yarn 上 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p><strong>在集群上分发配置好的Hadoop配置文件</strong></p>
<p><code>xsync /opt/module/hadoop-3.1.3/etc/hadoop/</code></p>
</li>
</ol>
<h3 id="4-2-4-群起集群"><a href="#4-2-4-群起集群" class="headerlink" title="4.2.4 群起集群"></a>4.2.4 群起集群</h3><ol>
<li><p><strong>配置workers</strong></p>
<p><code>vim /opt/module/hadoop-3.1.3/etc/hadoop/workers</code></p>
<p>在该文件中添加如下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure>

<p>*<em>注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。  *</em></p>
<p>然后同步到所有节点。</p>
<p><code>xsync /opt/module/hadoop-3.1.3/etc/</code></p>
</li>
<li><p><strong>启动集群</strong></p>
<ul>
<li><p><strong>如果集群是第一次启动</strong>，需要在 hadoop102 节点格式化 NameNode（注意： 格式<br>化 NameNode， 会产生新的集群 id， 导致 NameNode 和 DataNode 的集群 id 不一致，集群找<br>不到以往数据。 <strong>如果集群在运行过程中报错，需要重新格式化 NameNode 的话， 一定要先停</strong><br><strong>止 namenode 和 datanode 进程， 并且要删除所有机器的 data 和 logs 目录，然后再进行格式</strong><br><strong>化。</strong> ）  </p>
<p><code>[extrali@hadoop102 hadoop-3.1.3]$ hdfs namenode -format</code></p>
</li>
<li><p><strong>启动HDFS</strong></p>
<p><code>[extrali@hadoop102 hadoop-3.1.3]$ start-dfs.sh</code></p>
</li>
<li><p><strong>在配置了ResourceManager的节点（hadoop103）上启动YARN</strong></p>
<p><code>[extrali@hadoop103 hadoop-3.1.3]$ start-yarn.sh</code></p>
</li>
<li><p><strong>Web端查看HDFS的NameNode</strong></p>
<p>（a）浏览器中输入：<a href="http://hadoop102:9870" target="_blank" rel="noopener">http://hadoop102:9870</a></p>
<p>（b）查看HDFS上存储的数据信息</p>
</li>
<li><p>Web端查看YARN的ResourceManager</p>
<p>（a）浏览器中输入：<a href="http://hadoop103:8088" target="_blank" rel="noopener">http://hadoop103:8088</a></p>
<p>（b）查看YARN上运行的Job信息</p>
</li>
</ul>
<p><strong>注意：第一次在Web上查看信息时显示不出来，经测试发现是防火墙没关，需要使用以下指令关闭防火墙。</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# systemctl stop firewalld</span><br><span class="line">[root@hadoop100 ~]# systemctl disable firewalld.service</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>集群基本测试</strong></p>
<ol>
<li><p>上传文件到集群</p>
<ul>
<li><p>上传小文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[extrali@hadoop102 ~]$ hadoop fs -mkdir &#x2F;input</span><br><span class="line">[extrali@hadoop102 ~]$ hadoop fs -put $HADOOP_HOME&#x2F;wcinput&#x2F;word.txt &#x2F;input</span><br></pre></td></tr></table></figure>
</li>
<li><p>上传大文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[extrali@hadoop102 hadoop]$ hadoop fs -put &#x2F;opt&#x2F;software&#x2F;jdk-8u144-linux-x64.tar.gz &#x2F;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>上传文件后查看文件存放在什么位置</strong></p>
<ul>
<li><p><strong>查看HDFS文件存储路径</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[extrali@hadoop102 subdir0]$ pwd</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;data&#x2F;dfs&#x2F;data&#x2F;current&#x2F;BP-1280735098-192.168.1.102-1621412767684&#x2F;current&#x2F;finalized&#x2F;subdir0&#x2F;subdir0</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>查看HDFS在磁盘存储文件内容</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[extrali@hadoop102 subdir0]$ cat blk_1073741825</span><br><span class="line">hadoop yarn</span><br><span class="line">hadoop mapredurce</span><br><span class="line">extrali</span><br><span class="line">extrali</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>拼接</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-rw-rw-r--. 1 extrali extrali 134217728 5月  19 22:51 blk_1073741826</span><br><span class="line">-rw-rw-r--. 1 extrali extrali   1048583 5月  19 22:51 blk_1073741826_1002.meta</span><br><span class="line">-rw-rw-r--. 1 extrali extrali  51298114 5月  19 22:51 blk_1073741827</span><br><span class="line">-rw-rw-r--. 1 extrali extrali    400775 5月  19 22:51 blk_1073741827_1003.meta</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[extrali@hadoop102 subdir0]$ cat blk_1073741826&gt;&gt;tmp.tar.gz</span><br><span class="line">[extrali@hadoop102 subdir0]$ cat blk_1073741827&gt;&gt;tmp.tar.gz</span><br><span class="line">[extrali@hadoop102 subdir0]$ tar -zxvf tmp.tar.gz</span><br></pre></td></tr></table></figure>

<p>解压后发现内容就是jdk</p>
</li>
</ul>
</li>
<li><p><strong>下载</strong></p>
<p><code>[extrali@hadoop102 software]$ hadoop fs -get /jdk-8u144-linux-x64.tar.gz ./</code></p>
</li>
<li><p><strong>执行wordcount程序</strong></p>
<p><code>[extrali@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</code></p>
</li>
</ol>
</li>
</ol>
<h3 id="4-2-5-配置历史服务器"><a href="#4-2-5-配置历史服务器" class="headerlink" title="4.2.5 配置历史服务器"></a>4.2.5 配置历史服务器</h3><p>为了查看程序的历史运行情况，需要配置一下历史服务器。具体配置步骤如下：  </p>
<ol>
<li><p><strong>配置mapred-site.xml</strong></p>
<p>在该文件里面增加如下配置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 历史服务器端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;hadoop102:10020&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 历史服务器 web 端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;hadoop102:19888&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>分发配置</strong></p>
<p><code>[atguigu@hadoop102 hadoop]$ xsync
$HADOOP_HOME/etc/hadoop/mapred-site.xml</code></p>
</li>
<li><p><strong>在hadoop102启动历史服务器</strong></p>
<p><code>mapred --daemon start historyserver</code></p>
</li>
<li><p><strong>查看JobHistory</strong></p>
<p><a href="http://hadoop102:19888" target="_blank" rel="noopener">http://hadoop102:19888</a></p>
</li>
</ol>
<h3 id="4-2-6-配置日志的聚集"><a href="#4-2-6-配置日志的聚集" class="headerlink" title="4.2.6 配置日志的聚集"></a>4.2.6 配置日志的聚集</h3><p>日志聚集概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上。</p>
<p><img src="https://i.loli.net/2021/05/20/znuaBk6Oylh5JwQ.png" alt="image-20210520102538697"></p>
<p>日志聚集功能的好处：可以方便的查看到程序运行详情，方便开发调试。</p>
<p><strong>注意：开启日志聚集功能，需要重新启动NodeManager、ResourceManager和HistoryServer</strong></p>
<p><strong>开启日志聚集功能步骤如下：</strong></p>
<ol>
<li><p><strong>配置yarn-site.xml</strong></p>
<p>在该文件里面增加如下配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 开启日志聚集功能 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 设置日志聚集服务器地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log.server.url&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;http:&#x2F;&#x2F;hadoop102:19888&#x2F;jobhistory&#x2F;logs&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 设置日志保留时间为 7 天 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;604800&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>分发配置</strong></p>
<p><code>xsync $HADOOP_PATH/etc/hadoop/yarn-site.xml</code></p>
</li>
<li><p>重启NodeManager、ResourceManager和HistoryServer</p>
</li>
<li><p>删除HDFS上已经存在的输出文件</p>
<p><code>hadoop fs -rm -r /output</code></p>
</li>
<li><p>执行WordCount程序</p>
</li>
<li><p><strong>查看日志</strong></p>
</li>
</ol>
<h3 id="4-2-7-集群启动-停止方式总结"><a href="#4-2-7-集群启动-停止方式总结" class="headerlink" title="4.2.7 集群启动/停止方式总结"></a>4.2.7 集群启动/停止方式总结</h3><ol>
<li><p><strong>整体启动/停止</strong></p>
<ul>
<li><p>整体启动/HDFS</p>
<p><code>start-dfs.sh/stop-dfs.sh</code></p>
</li>
<li><p>整体启动/停止YARN</p>
<p><code>start-yarn.sh/stop-yarn.sh</code></p>
</li>
</ul>
</li>
<li><p><strong>各个服务组件逐一启动/停止</strong></p>
<ul>
<li><p>分别启动/停止HDFS组件</p>
<p><code>hdfs --daemon start/stop namenode/datanode/secondarynamenode</code></p>
</li>
<li><p>分别启动/停止YARN组件</p>
<p><code>yarn --daemon start/stop resourcemanager/nodemanager</code></p>
</li>
</ul>
</li>
</ol>
<h3 id="4-2-8-编写HDFS集群常用脚本"><a href="#4-2-8-编写HDFS集群常用脚本" class="headerlink" title="4.2.8 编写HDFS集群常用脚本"></a>4.2.8 编写HDFS集群常用脚本</h3><ol>
<li><p><strong>Hadoop集群启停脚本（包含HDFS、YARN和HistoryServer）：myhadoop.sh</strong></p>
<p>在home/extrali/bin下新建<strong>myhadoop.sh</strong>，输入如下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">	echo &quot;No Args Input...&quot;</span><br><span class="line">	exit ;</span><br><span class="line">fi</span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">	echo &quot; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 启动 hadoop 集群 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;	</span><br><span class="line">	echo &quot; --------------- 启动 hdfs ---------------&quot;</span><br><span class="line">	ssh hadoop102 &quot;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;sbin&#x2F;start-dfs.sh&quot;</span><br><span class="line">	echo &quot; --------------- 启动 yarn ---------------&quot;</span><br><span class="line">	ssh hadoop103 &quot;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;sbin&#x2F;start-yarn.sh&quot;</span><br><span class="line">	echo &quot; --------------- 启动 historyserver ---------------&quot;</span><br><span class="line">	ssh hadoop102 &quot;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;bin&#x2F;mapred --daemon start historyserver&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">	echo &quot; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 关闭 hadoop 集群 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;</span><br><span class="line">	echo &quot; --------------- 关闭 historyserver ---------------&quot;	</span><br><span class="line">	ssh hadoop102 &quot;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;bin&#x2F;mapred --daemon stop historyserver&quot;</span><br><span class="line">	echo &quot; --------------- 关闭 yarn ---------------&quot;</span><br><span class="line">	ssh hadoop103 &quot;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;sbin&#x2F;stop-yarn.sh&quot;</span><br><span class="line">	echo &quot; --------------- 关闭 hdfs ---------------&quot;</span><br><span class="line">	ssh hadoop102 &quot;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;sbin&#x2F;stop-dfs.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>同时赋予脚本执行权限</p>
</li>
<li><p><strong>查看三台服务器Java进程脚本：jpsall</strong></p>
<p>脚本内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">	echo &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; $host &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">	ssh $host jps</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="4-2-9-常用端口号说明"><a href="#4-2-9-常用端口号说明" class="headerlink" title="4.2.9 常用端口号说明"></a>4.2.9 常用端口号说明</h3><table>
<thead>
<tr>
<th>端口名称</th>
<th>Hadoop2.x</th>
<th>Hadoop3.x</th>
</tr>
</thead>
<tbody><tr>
<td>NameNode 内部通信端口</td>
<td>8020 / 9000</td>
<td><strong>8020</strong> / 9000/9820</td>
</tr>
<tr>
<td>NameNode HTTP UI</td>
<td>50070</td>
<td><strong>9870</strong></td>
</tr>
<tr>
<td>MapReduce 查看执行任务端口</td>
<td>8088</td>
<td><strong>8088</strong></td>
</tr>
<tr>
<td>历史服务器通信端口</td>
<td>19888</td>
<td><strong>19888</strong></td>
</tr>
</tbody></table>
<h1 id="5-HDFS"><a href="#5-HDFS" class="headerlink" title="5.HDFS"></a>5.HDFS</h1><h2 id="5-1-HDFS概述"><a href="#5-1-HDFS概述" class="headerlink" title="5.1 HDFS概述"></a>5.1 HDFS概述</h2><h3 id="5-1-1-HDFS产出背景及定义"><a href="#5-1-1-HDFS产出背景及定义" class="headerlink" title="5.1.1 HDFS产出背景及定义"></a>5.1.1 HDFS产出背景及定义</h3><ol>
<li><p>HDFS产生背景</p>
<p>随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切<strong>需要一种系统来管理多台机器上的文件</strong>，这就是<strong>分布式文件管理系统</strong>。 <strong>HDFS 只是分布式文件管理系统中的一种</strong>。  </p>
</li>
<li><p>HDFS定义</p>
<p>​    <strong>HDFS（Hadoop Distributed File System）</strong>，它是一个文件系统，用于存储文件，通过目录树来定位文件； 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。<br>​    <strong>HDFS 的使用场景：适合一次写入，多次读出的场景</strong>。 <strong>一个文件经过创建、写入和关闭之后就不需要改变。</strong></p>
</li>
</ol>
<h3 id="5-1-2-HDFS优缺点"><a href="#5-1-2-HDFS优缺点" class="headerlink" title="5.1.2 HDFS优缺点"></a>5.1.2 HDFS优缺点</h3><p><img src="https://i.loli.net/2021/05/20/WzgDOuZQ7RtT3LU.png" alt="image-20210520151452016"></p>
<p><img src="https://i.loli.net/2021/05/20/FSYWIucj1sNPkH4.png" alt="image-20210520151537957"></p>
<h3 id="5-1-3-HDFS组成架构"><a href="#5-1-3-HDFS组成架构" class="headerlink" title="5.1.3 HDFS组成架构"></a>5.1.3 HDFS组成架构</h3><p><img src="https://i.loli.net/2021/05/20/DbayBKGFUQNVPTu.png" alt="image-20210520151732822"></p>
<p>3） <strong>Client</strong>：就是客户端。</p>
<ul>
<li>文件切分。 文件上传HDFS的时候， Client将文件切分成一个一个的Block， 然后进行上传；</li>
<li>与NameNode交互， 获取文件的位置信息；</li>
<li>与DataNode交互， 读取或者写入数据；</li>
<li>Client提供一些命令来管理HDFS， 比如NameNode格式化；</li>
<li>Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作；</li>
</ul>
<p>4） <strong>Secondary NameNode</strong>：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。</p>
<ul>
<li>辅助NameNode， 分担其工作量， 比如<strong>定期合并Fsimage和Edits</strong>， 并推送给NameNode ；</li>
<li>在紧急情况下， 可<strong>辅助恢复NameNode</strong>。  </li>
</ul>
<h3 id="5-1-4-HDFS文件块大小"><a href="#5-1-4-HDFS文件块大小" class="headerlink" title="5.1.4 HDFS文件块大小"></a>5.1.4 HDFS文件块大小</h3><p><img src="https://i.loli.net/2021/05/20/D3jIr8cNsYU7RoM.png" alt="image-20210520152513566"></p>
<ol>
<li>如果HDFS的块设置<strong>太小</strong>，<strong>会增加寻址时间</strong>，程序一直在找块的开始位置；</li>
<li>如果HDFS的块设置<strong>太大</strong>，从<strong>磁盘传输数据的时间</strong>会明显<strong>大于定位这个块开始位置所需的时间</strong>。 导致程序在处理这块数据时， 会非常慢。  </li>
</ol>
<p><strong>总结：HDFS块的大小设置主要取决于磁盘传输速率。</strong></p>
<h2 id="5-2-HDFS的Shell操作"><a href="#5-2-HDFS的Shell操作" class="headerlink" title="5.2 HDFS的Shell操作"></a>5.2 HDFS的Shell操作</h2><ol>
<li><p><strong>基本语法</strong></p>
<p><code>hadoop fs 具体指令   OR     hdfs dfs 具体指令</code></p>
<p>两个指令完全相同</p>
</li>
<li><p><strong>常用命令实操</strong></p>
<p>-help：输出这个命令参数  </p>
<p><code>hadoop fs -help rm</code> </p>
<p>创建/sanguo文件夹</p>
<p><code>hadoop fs -mkdir /sanguo</code></p>
</li>
</ol>
<h3 id="5-2-1-上传"><a href="#5-2-1-上传" class="headerlink" title="5.2.1 上传"></a>5.2.1 上传</h3><ol>
<li><p><strong>-moveFromLocal</strong>：从本地<strong>剪切</strong>粘贴到HDFS</p>
<p><code>hadoop fs -moveFromLocal ./shuguo.txt /sanguo</code></p>
</li>
<li><p><strong>-copyFromLocal</strong>：从本地文件系统中<strong>拷贝</strong>文件到HDFS路径中</p>
<p><code>hadoop fs -copyFromLocal weiguo.txt /sanguo</code></p>
</li>
<li><p><strong>-put</strong>：等同于copyFromLocal，生产环境更习惯用put</p>
<p><code>hadoop fs -put ./wuguo.txt /sanguo</code></p>
</li>
<li><p><strong>-appendToFile</strong>：<strong>追加</strong>一个文件到已经存在的文件末尾</p>
<p><code>hadoop fs -appendToFile liubei.txt /sanguo/shuguo.txt</code></p>
</li>
</ol>
<h3 id="5-2-2-下载"><a href="#5-2-2-下载" class="headerlink" title="5.2.2  下载"></a>5.2.2  下载</h3><ol>
<li><p><strong>-copyToLocal</strong>：从HDFS<strong>拷贝</strong>到本地</p>
<p><code>hadoop fs -copyToLocal /sanguo/shuguo.txt ./</code></p>
</li>
<li><p><strong>-get</strong>：等同于copyToLocal，生产环境更习惯用get</p>
<p><code>hadoop fs -get /sanguo/shuguo.txt ./shuguo2.txt</code></p>
</li>
</ol>
<h3 id="5-2-3-HDFS直接操作"><a href="#5-2-3-HDFS直接操作" class="headerlink" title="5.2.3 HDFS直接操作"></a>5.2.3 HDFS直接操作</h3><ol>
<li><p><strong>-ls：显示目录信息</strong></p>
<p><code>hadoop fs -ls /sanguo</code></p>
</li>
<li><p><strong>-cat：显示文件内容</strong></p>
<p><code>hadoop fs -cat /sanguo/shuguo.txt</code></p>
</li>
<li><p><strong>-chgrp、-chmod、-chown</strong>：Linux文件系统中的用法一样，修改文件所属权限</p>
<p><code>hadoop fs -chmod 666 /sanguo/shuguo.txt</code></p>
<p><code>hadoop fs -chown extrali:extrali /sanguo/shuguo.txt</code></p>
</li>
<li><p><strong>-mkdir</strong>：创建路径</p>
<p><code>hadoop fs -mkdir /jinguo</code></p>
</li>
<li><p><strong>-cp</strong>：从HDFS的一个路径拷贝到HDFS的另一个路径</p>
<p><code>hadoop -cp /sanguo/shuguo.txt /jinguo</code></p>
</li>
<li><p><strong>-mv</strong>：在HDFS目录中移动文件</p>
<p><code>hadoop fs -mv /sanguo/wuguo.txt /jinguo</code></p>
</li>
<li><p><strong>-tail</strong>：显示一个文件的末尾1kb的数据</p>
<p><code>hadoop fs -tail /jinguo/shuguo.txt</code></p>
</li>
<li><p><strong>-rm</strong>：删除文件或文件夹</p>
<p><code>hadoop fs -rm /sanguo/shuguo.txt</code></p>
</li>
<li><p><strong>-rm -r</strong>：<strong>递归</strong>删除目录以及目录里面内容</p>
<p><code>hadoop fs -rm -r /sanguo</code></p>
</li>
<li><p><strong>-du</strong>：统计文件夹的大小信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -du -s -h &#x2F;jinguo</span><br><span class="line">27 81 &#x2F;jinguo</span><br><span class="line"></span><br><span class="line">hadoop-3.1.3]$ hadoop fs -du -h &#x2F;jinguo</span><br><span class="line">14 42 &#x2F;jinguo&#x2F;shuguo.txt</span><br><span class="line">7 21 &#x2F;jinguo&#x2F;weiguo.txt</span><br><span class="line">6 18 &#x2F;jinguo&#x2F;wuguo.tx</span><br></pre></td></tr></table></figure>

<p><strong>说明：</strong>27 表示<strong>文件大小</strong>； 81 表示 27*3 个副本； /jinguo 表示查看的目录  </p>
</li>
<li><p><strong>-setrep</strong>：设置HDFS中文件的副本数量</p>
<p><code>hadoop fs -setrep 10 /jinguo/shuguo.txt</code></p>
<p>这里设置的副本数只是记录在 NameNode 的元数据中，是否真的会有这么多副本，还得看 DataNode 的数量。因为目前只有 3 台设备，最多也就 3 个副本，只有节点数的增加到 10台时， 副本数才能达到 10。  </p>
</li>
</ol>
<h2 id="5-3-HDFS的API操作"><a href="#5-3-HDFS的API操作" class="headerlink" title="5.3 HDFS的API操作"></a>5.3 HDFS的API操作</h2><h3 id="5-3-1-客户端环境准备"><a href="#5-3-1-客户端环境准备" class="headerlink" title="5.3.1 客户端环境准备"></a>5.3.1 客户端环境准备</h3><ol>
<li><p>找到资料包路径下的 Windows 依赖文件夹， 拷贝 hadoop-3.1.0 到非中文路径（比如 d:\）。</p>
</li>
<li><p><strong>配置HADOOP_HOME环境变量</strong></p>
<p><img src="https://i.loli.net/2021/05/30/yTe9aHckjpRgb3G.png" alt="image-20210530134226566"></p>
</li>
<li><p>配置Path环境变量</p>
<p><img src="https://i.loli.net/2021/05/30/18QFXDglOBb5JWS.png" alt="image-20210530134257434"></p>
</li>
<li><p>在IDEA中创建一个Maven工程HdfsClientDemo，并导入相应的依赖坐标+日志添加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.1.3&lt;&#x2F;version&gt;</span><br><span class="line">	&lt;&#x2F;dependency&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;junit&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;junit&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;4.12&lt;&#x2F;version&gt;</span><br><span class="line">	&lt;&#x2F;dependency&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.7.30&lt;&#x2F;version&gt;</span><br><span class="line">	&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br></pre></td></tr></table></figure>

<p>在项目的src/main/resources目录下，新建一个文件，命名为”log4j.properties“，并在文件中填入</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger&#x3D;INFO, stdout</span><br><span class="line">log4j.appender.stdout&#x3D;org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d %p [%c] - %m%n</span><br><span class="line">log4j.appender.logfile&#x3D;org.apache.log4j.FileAppender</span><br><span class="line">log4j.appender.logfile.File&#x3D;target&#x2F;spring.log</span><br><span class="line">log4j.appender.logfile.layout&#x3D;org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern&#x3D;%d %p [%c] - %m%n</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建包名：com.hust.hdfs</p>
</li>
<li><p>创建HdfsClient类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public class HdfsClient &#123;</span><br><span class="line">    @Test</span><br><span class="line">    public void testMkdirs() throws IOException, URISyntaxException,InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F; 1 获取文件系统</span><br><span class="line">        Configuration configuration &#x3D; new Configuration();</span><br><span class="line">        &#x2F;&#x2F; FileSystem fs &#x3D; FileSystem.get(new</span><br><span class="line">        URI(&quot;hdfs:&#x2F;&#x2F;hadoop102:8020&quot;), configuration);</span><br><span class="line">        FileSystem fs &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;hadoop102:8020&quot;),</span><br><span class="line">        configuration,&quot;extrali&quot;);</span><br><span class="line">        &#x2F;&#x2F; 2 创建目录</span><br><span class="line">        fs.mkdirs(new Path(&quot;&#x2F;xiyou&#x2F;huaguoshan&#x2F;&quot;));</span><br><span class="line">        &#x2F;&#x2F; 3 关闭资源</span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行程序</p>
<p>客户端去操作 HDFS 时，是有一个用户身份的。默认情况下， HDFS 客户端 API 会从采用 <strong>Windows 默认用户</strong>访问 HDFS，会报权限异常错误。所以在访问 HDFS 时，<strong>一定要配置用户</strong>。  </p>
<p><code>org.apache.hadoop.security.AccessControlException: Permission denied:
user=56576, access=WRITE,
inode=&quot;/xiyou/huaguoshan&quot;:atguigu:supergroup:drwxr-xr-x</code></p>
</li>
</ol>
<h3 id="5-3-2-HDFS的API案例实操"><a href="#5-3-2-HDFS的API案例实操" class="headerlink" title="5.3.2 HDFS的API案例实操"></a>5.3.2 HDFS的API案例实操</h3><h4 id="5-3-2-1-HDFS文件上传（测试参数优先级）"><a href="#5-3-2-1-HDFS文件上传（测试参数优先级）" class="headerlink" title="5.3.2.1 HDFS文件上传（测试参数优先级）"></a>5.3.2.1 HDFS文件上传（测试参数优先级）</h4><ol>
<li><p>编写源代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testCopyFromLocalFile() throws IOException,InterruptedException, URISyntaxException &#123;</span><br><span class="line">    &#x2F;&#x2F; 1 获取文件系统</span><br><span class="line">    Configuration configuration &#x3D; new Configuration();</span><br><span class="line">    configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;);</span><br><span class="line">    FileSystem fs &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;hadoop102:8020&quot;),</span><br><span class="line">    configuration, &quot;extrali&quot;);</span><br><span class="line">    &#x2F;&#x2F; 2 上传文件</span><br><span class="line">    fs.copyFromLocalFile(new Path(&quot;d:&#x2F;sunwukong.txt&quot;), new</span><br><span class="line">    Path(&quot;&#x2F;xiyou&#x2F;huaguoshan&quot;));</span><br><span class="line">    &#x2F;&#x2F; 3 关闭资源</span><br><span class="line">    fs.close();</span><br><span class="line">｝</span><br></pre></td></tr></table></figure>
</li>
<li><p>将hdfs-site.xml拷贝到项目的resources资源目录下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>参数优先级</strong></p>
<p><strong>参数优先级排序：</strong>（1） 客户端代码中设置的值 &gt;（2） ClassPath 下的用户自定义配置文件 &gt;（3）然后是服务器的自定义配置（xxx-site.xml） &gt;（4）服务器的默认配置（xxx-default.xml）  </p>
</li>
</ol>
<h4 id="5-3-2-2-HDFS文件下载"><a href="#5-3-2-2-HDFS文件下载" class="headerlink" title="5.3.2.2 HDFS文件下载"></a>5.3.2.2 HDFS文件下载</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testCopyToLocalFile() throws IOException,InterruptedException, URISyntaxException&#123;</span><br><span class="line">    &#x2F;&#x2F; 1 获取文件系统</span><br><span class="line">    Configuration configuration &#x3D; new Configuration();</span><br><span class="line">    FileSystem fs &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;hadoop102:8020&quot;),</span><br><span class="line">    configuration, &quot;extrali&quot;);</span><br><span class="line">    &#x2F;&#x2F; 2 执行下载操作</span><br><span class="line">    &#x2F;&#x2F; boolean delSrc 指是否将原文件删除</span><br><span class="line">    &#x2F;&#x2F; Path src 指要下载的文件路径</span><br><span class="line">    &#x2F;&#x2F; Path dst 指将文件下载到的路径</span><br><span class="line">    &#x2F;&#x2F; boolean useRawLocalFileSystem 是否开启文件校验</span><br><span class="line">    fs.copyToLocalFile(false, new Path(&quot;&#x2F;xiyou&#x2F;huaguoshan&#x2F;sunwukong.txt&quot;), new 	   			</span><br><span class="line">    Path(&quot;d:&#x2F;sunwukong2.txt&quot;),</span><br><span class="line">    true);</span><br><span class="line">    &#x2F;&#x2F; 3 关闭资源</span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="5-3-2-3-HDFS文件更名和移动"><a href="#5-3-2-3-HDFS文件更名和移动" class="headerlink" title="5.3.2.3 HDFS文件更名和移动"></a>5.3.2.3 HDFS文件更名和移动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testRename() throws IOException, InterruptedException,URISyntaxException&#123;</span><br><span class="line">    &#x2F;&#x2F; 1 获取文件系统</span><br><span class="line">    Configuration configuration &#x3D; new Configuration();</span><br><span class="line">    FileSystem fs &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;hadoop102:8020&quot;),</span><br><span class="line">    configuration, &quot;extrali&quot;);</span><br><span class="line">    &#x2F;&#x2F; 2 修改文件名称</span><br><span class="line">    fs.rename(new Path(&quot;&#x2F;xiyou&#x2F;huaguoshan&#x2F;sunwukong.txt&quot;), new</span><br><span class="line">    Path(&quot;&#x2F;meihouwang.txt&quot;));</span><br><span class="line">    &#x2F;&#x2F; 3 关闭资源</span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="5-3-2-4-HDFS删除文件和目录"><a href="#5-3-2-4-HDFS删除文件和目录" class="headerlink" title="5.3.2.4 HDFS删除文件和目录"></a>5.3.2.4 HDFS删除文件和目录</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testDelete() throws IOException, InterruptedException, URISyntaxException&#123;</span><br><span class="line">    &#x2F;&#x2F; 1 获取文件系统</span><br><span class="line">    Configuration configuration &#x3D; new Configuration();</span><br><span class="line">    FileSystem fs &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;hadoop102:8020&quot;),</span><br><span class="line">    configuration, &quot;extrali&quot;);</span><br><span class="line">    &#x2F;&#x2F; 2 执行删除</span><br><span class="line">    fs.delete(new Path(&quot;&#x2F;xiyou&quot;), true);</span><br><span class="line">    &#x2F;&#x2F; 3 关闭资源</span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="5-3-2-5-HDFS文件详情查看"><a href="#5-3-2-5-HDFS文件详情查看" class="headerlink" title="5.3.2.5 HDFS文件详情查看"></a>5.3.2.5 HDFS文件详情查看</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testListFiles() throws IOException &#123;</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; listFiles &#x3D; fs.listFiles(new Path(&quot;&#x2F;&quot;), true);</span><br><span class="line">    while (listFiles.hasNext())&#123;</span><br><span class="line">    LocatedFileStatus fileStatus &#x3D; listFiles.next();</span><br><span class="line">    System.out.println(&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;+fileStatus.getPath()+&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;);</span><br><span class="line">    System.out.println(fileStatus.getPermission());</span><br><span class="line">    System.out.println(fileStatus.getOwner());</span><br><span class="line">    System.out.println(fileStatus.getGroup());</span><br><span class="line">    System.out.println(fileStatus.getLen());</span><br><span class="line">    System.out.println(fileStatus.getModificationTime());</span><br><span class="line">    System.out.println(fileStatus.getReplication());</span><br><span class="line">    System.out.println(fileStatus.getBlockSize());</span><br><span class="line">    System.out.println(fileStatus.getPath().getName());</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 获取块信息</span><br><span class="line">    BlockLocation[] blockLocations &#x3D; fileStatus.getBlockLocations();</span><br><span class="line">    System.out.println(Arrays.toString(blockLocations));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="5-3-2-6-HDFS文件和文件夹判断"><a href="#5-3-2-6-HDFS文件和文件夹判断" class="headerlink" title="5.3.2.6 HDFS文件和文件夹判断"></a>5.3.2.6 HDFS文件和文件夹判断</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testListStatus() throws IOException, InterruptedException,URISyntaxException&#123;</span><br><span class="line">    &#x2F;&#x2F; 判断是文件还是文件夹</span><br><span class="line">    FileStatus[] listStatus &#x3D; fs.listStatus(new Path(&quot;&#x2F;&quot;));</span><br><span class="line">    for (FileStatus fileStatus : listStatus) &#123;</span><br><span class="line">        &#x2F;&#x2F; 如果是文件</span><br><span class="line">        if (fileStatus.isFile()) &#123;</span><br><span class="line">        System.out.println(&quot;f:&quot;+fileStatus.getPath().getName());</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">        System.out.println(&quot;d:&quot;+fileStatus.getPath().getName());</span><br><span class="line">        &#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="5-4-HDFS读写流程"><a href="#5-4-HDFS读写流程" class="headerlink" title="5.4 HDFS读写流程"></a>5.4 HDFS读写流程</h2><h3 id="5-4-1-HDFS写操作流程"><a href="#5-4-1-HDFS写操作流程" class="headerlink" title="5.4.1 HDFS写操作流程"></a>5.4.1 HDFS写操作流程</h3><h4 id="5-4-1-1-剖析文件写入"><a href="#5-4-1-1-剖析文件写入" class="headerlink" title="5.4.1.1 剖析文件写入"></a>5.4.1.1 剖析文件写入</h4><p><img src="https://i.loli.net/2021/05/30/l4YaAoW2PISmejN.png" alt="image-20210530171037621"></p>
<ol>
<li>客户端通过 Distributed FileSystem 模块向NameNode请求上传文件， <strong>NameNode 检查目标文件是否已存在，父目录是否存在。</strong></li>
<li>NameNode 返回是否可以上传。  </li>
<li>客户端请求第一个 Block 上传到<strong>哪几个 DataNode 服务器</strong>上。  </li>
<li>NameNode 返回 3 个 DataNode 节点， 分别为 dn1、 dn2、 dn3。  </li>
<li>客户端通过 FSDataOutputStream 模块请求 dn1 上传数据， dn1 收到请求会继续调用dn2，然后 dn2 调用 dn3，将这个<strong>通信管道</strong>建立完成。  </li>
<li>dn1、 dn2、 dn3 逐级应答客户端。  </li>
<li>客户端开始往 dn1 上传第一个 Block（先从磁盘读取数据放到一个本地内存缓存），<strong>以 Packet 为单位</strong>， dn1 收到一个 Packet 就会传给 dn2， dn2 传给 dn3； <strong>dn1 每传一个 packet会放入一个应答队列等待应答</strong>。  </li>
<li>当一个 Block 传输完成之后，客户端再次请求 NameNode 上传第二个 Block 的服务器。（重复执行 3-7 步）。  </li>
</ol>
<p><strong>Packet：Packet是Client端向Dataode，或者DataNode的PipLine之间传输数据的基本单位，默认<code>64kB</code>。</strong></p>
<p><strong>Chunk：Chunk是最小的Hadoop中最小的单位，是Client向DataNode或DataNode的PipLne之间进行数据校验的基本单位，默认<code>512Byte</code>,因为用作校验(自己校验自己)，故每个chunk需要带有<code>4Byte</code>的校验位。所以世纪每个chunk写入packet的大小为516Byte，真实数据与校验值数据的比值为128:1。</strong></p>
<h4 id="5-4-1-2-网络拓扑-节点距离计算"><a href="#5-4-1-2-网络拓扑-节点距离计算" class="headerlink" title="5.4.1.2 网络拓扑-节点距离计算"></a>5.4.1.2 网络拓扑-节点距离计算</h4><p>在 HDFS 写数据的过程中， NameNode 会选择<strong>距离待上传数据最近距离的 DataNode 接收数据</strong>。 那么这个最近距离怎么计算呢？  </p>
<p>*<em>节点距离：两个节点到达最近的共同祖先的距离总和。  *</em></p>
<p><img src="https://i.loli.net/2021/05/30/s9TXJg1ozaH8V34.png" alt="image-20210530172421747"></p>
<p>例如，假设有数据中心 d1 机架 r1 中的节点 n1。 该节点可以表示为<strong>/d1/r1/n1</strong>。 利用这种标记，这里给出四种距离描述。  </p>
<h4 id="5-4-1-3-机架感知（副本存储节点选择）"><a href="#5-4-1-3-机架感知（副本存储节点选择）" class="headerlink" title="5.4.1.3 机架感知（副本存储节点选择）"></a>5.4.1.3 机架感知（副本存储节点选择）</h4><ol>
<li><p>机架感知说明</p>
<ol>
<li><p>官方说明</p>
<blockquote>
<p>For the common case, when the replication factor is three, HDFS’s placement policy is to <strong>put one replica on the local machine</strong> if the writer is on a datanode, <strong>otherwise on a random datanode</strong>, <strong>another replica on anode in a different (remote) rack</strong>, and <strong>the last on a different node in the same remote rack</strong>. This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a file do not evenly distribute across the racks. One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks. This policy improves write performance without compromising data reliability or read performance.  </p>
</blockquote>
</li>
<li><p>源码说明</p>
<p>Crtl + n 查找 BlockPlacementPolicyDefault，在该类中查找 chooseTargetInOrder 方法。</p>
</li>
</ol>
</li>
<li><p>Hadoop3.1.3副本节点选择</p>
<p><img src="https://i.loli.net/2021/05/30/3UJ9WFwSGdKZhQH.png" alt="image-20210530174553011"></p>
<h3 id="5-4-2-HDFS读操作流程"><a href="#5-4-2-HDFS读操作流程" class="headerlink" title="5.4.2 HDFS读操作流程"></a>5.4.2 HDFS读操作流程</h3><p><img src="https://i.loli.net/2021/05/30/5XcUoTxisjMkGut.png" alt="image-20210530174849362"></p>
<ol>
<li>客户端通过 DistributedFileSystem 向 NameNode 请求下载文件， NameNode 通过查询元数据，<strong>找到文件块所在的 DataNode 地址</strong>。  </li>
<li>挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。  </li>
<li>DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 Packet 为单位来做校验）。</li>
<li>客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。  </li>
</ol>
</li>
</ol>
<h2 id="5-5-NameNode-和-SecondaryNameNode"><a href="#5-5-NameNode-和-SecondaryNameNode" class="headerlink" title="5.5 NameNode 和 SecondaryNameNode"></a>5.5 NameNode 和 SecondaryNameNode</h2><h3 id="5-5-1-NN-和-2NN-工作机制"><a href="#5-5-1-NN-和-2NN-工作机制" class="headerlink" title="5.5.1 NN 和 2NN 工作机制"></a>5.5.1 NN 和 2NN 工作机制</h3><p><strong>思考：</strong> <strong>NameNode 中的元数据是存储在哪里的？</strong><br>    首先，我们做个假设，如果存储在 NameNode 节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。 <strong>因此产生在磁盘中备份元数据的FsImage</strong>。  </p>
<p>​    这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新 FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦 NameNode 节点断电，就会产生数据丢失。 因此，引入 <strong>Edits 文件（只进行追加操作，效率很高）</strong> 。<strong>每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到 Edits 中</strong>。 这样，一旦 NameNode 节点断电，可以通过 FsImage 和 Edits 的合并，合成元数据。  </p>
<p>​    但是，如果长时间添加数据到 Edits 中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行 FsImage 和 Edits 的合并，如果这个操作由NameNode节点完成，又会效率过低。</p>
<p>​    <strong>因此，引入一个新的节点SecondaryNamenode，专门用于 FsImage 和 Edits 的合并。</strong>  </p>
<p><img src="https://i.loli.net/2021/05/30/e8rqP3O4CVosWGH.png" alt="image-20210530180430271"></p>
<ol>
<li><strong>第一阶段：NameNode启动</strong><ul>
<li>第一次启动 NameNode 格式化后，创建 Fsimage 和 Edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。  </li>
<li>客户端对元数据进行增删改的请求。</li>
<li>NameNode 记录操作日志，更新滚动日志。  </li>
<li>NameNode 在内存中对元数据进行增删改。  </li>
</ul>
</li>
<li><strong>第二阶段：Secondary NameNode工作</strong><ul>
<li>Secondary NameNode 询问 NameNode 是否需要 CheckPoint。 直接带回 NameNode是否检查结果。  </li>
<li>Secondary NameNode 请求执行 CheckPoint。  </li>
<li>NameNode 滚动正在写的 Edits 日志。  </li>
<li>将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。  </li>
<li>Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。  </li>
<li>生成新的镜像文件 fsimage.chkpoint。  </li>
<li>拷贝 fsimage.chkpoint 到 NameNode。  </li>
<li>NameNode 将 fsimage.chkpoint 重新命名成 fsimage。  </li>
</ul>
</li>
</ol>
<h3 id="5-5-2-Fsimage-和-Edits-解析"><a href="#5-5-2-Fsimage-和-Edits-解析" class="headerlink" title="5.5.2 Fsimage 和 Edits 解析"></a>5.5.2 Fsimage 和 Edits 解析</h3><p><img src="https://i.loli.net/2021/05/30/Qmjo1iRPfgsKSJU.png" alt="image-20210530193143682"></p>
<ol>
<li><p><strong>oiv查看Fsimage文件</strong></p>
<ul>
<li><p>查看oiv和oev命令</p>
<blockquote>
<p><strong>oiv</strong>   apply the offline <strong>fsimage</strong> viewer to an fsimage</p>
<p><strong>oev</strong>   apply the offline <strong>edits</strong> viewer to an edits file</p>
</blockquote>
</li>
<li><p>基本语法</p>
<p><code>hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径</code></p>
</li>
<li><p>案例实操</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 current]$ pwd</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;data&#x2F;dfs&#x2F;name&#x2F;current</span><br><span class="line">[atguigu@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o &#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;fsimage.xml</span><br><span class="line">[atguigu@hadoop102 current]$ cat &#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;fsimage.xml</span><br></pre></td></tr></table></figure>

<p>将显示的 xml 文件内容拷贝到 Idea 中创建的 xml 文件中，并格式化。部分显示结果如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;inode&gt;</span><br><span class="line">    &lt;id&gt;16386&lt;&#x2F;id&gt;</span><br><span class="line">    &lt;type&gt;DIRECTORY&lt;&#x2F;type&gt;</span><br><span class="line">    &lt;name&gt;input&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;mtime&gt;1621435263452&lt;&#x2F;mtime&gt;</span><br><span class="line">    &lt;permission&gt;extrali:supergroup:0755&lt;&#x2F;permission&gt;</span><br><span class="line">    &lt;nsquota&gt;-1&lt;&#x2F;nsquota&gt;</span><br><span class="line">    &lt;dsquota&gt;-1&lt;&#x2F;dsquota&gt;</span><br><span class="line">&lt;&#x2F;inode&gt;</span><br><span class="line">&lt;inode&gt;</span><br><span class="line">    &lt;id&gt;16387&lt;&#x2F;id&gt;</span><br><span class="line">    &lt;type&gt;FILE&lt;&#x2F;type&gt;</span><br><span class="line">    &lt;name&gt;word.txt&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;replication&gt;3&lt;&#x2F;replication&gt;</span><br><span class="line">    &lt;mtime&gt;1621435263423&lt;&#x2F;mtime&gt;</span><br><span class="line">    &lt;atime&gt;1621483542548&lt;&#x2F;atime&gt;</span><br><span class="line">    &lt;preferredBlockSize&gt;134217728&lt;&#x2F;preferredBlockSize&gt;			  </span><br><span class="line">    &lt;permission&gt;extrali:supergroup:0644&lt;&#x2F;permission&gt;</span><br><span class="line">    &lt;blocks&gt;</span><br><span class="line">        &lt;block&gt;</span><br><span class="line">            &lt;id&gt;1073741825&lt;&#x2F;id&gt;</span><br><span class="line">            &lt;genstamp&gt;1001&lt;&#x2F;genstamp&gt;</span><br><span class="line">            &lt;numBytes&gt;46&lt;&#x2F;numBytes&gt;</span><br><span class="line">        &lt;&#x2F;block&gt;</span><br><span class="line">    &lt;&#x2F;blocks&gt;</span><br><span class="line">    &lt;storagePolicyId&gt;0&lt;&#x2F;storagePolicyId&gt;</span><br><span class="line">&lt;&#x2F;inode&gt;</span><br></pre></td></tr></table></figure>

<p><strong>思考：</strong>可以看出， Fsimage 中没有记录块所对应 DataNode，为什么？<br><strong>在集群启动后，要求 DataNode 上报数据块信息，并间隔一段时间后再次上报。</strong>  </p>
</li>
</ul>
</li>
<li><p><strong>oev查看Edits文件</strong></p>
<ul>
<li><p>基本语法</p>
<p><code>hdfs oev -p 文件类型 -i 编辑日志 -o 转换后文件输出路径</code></p>
</li>
<li><p>案例实操</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 current]$ hdfs oev -p XML -i</span><br><span class="line">edits_0000000000000000012-0000000000000000013 -o &#x2F;opt&#x2F;module&#x2F;hadoop-</span><br><span class="line">3.1.3&#x2F;edits.xml</span><br><span class="line">[atguigu@hadoop102 current]$ cat &#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;edits.xml</span><br></pre></td></tr></table></figure>

<p>将显示的 xml 文件内容拷贝到 Idea 中创建的 xml 文件中，并格式化。显示结果如下。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; standalone&#x3D;&quot;yes&quot;?&gt;</span><br><span class="line">&lt;EDITS&gt;</span><br><span class="line">&lt;EDITS_VERSION&gt;-64&lt;&#x2F;EDITS_VERSION&gt;</span><br><span class="line">&lt;RECORD&gt;</span><br><span class="line">    &lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;&#x2F;OPCODE&gt;</span><br><span class="line">    &lt;DATA&gt;</span><br><span class="line">        &lt;TXID&gt;6&lt;&#x2F;TXID&gt;</span><br><span class="line">    &lt;&#x2F;DATA&gt;</span><br><span class="line">&lt;&#x2F;RECORD&gt;</span><br><span class="line">&lt;RECORD&gt;</span><br><span class="line">    &lt;OPCODE&gt;OP_MKDIR&lt;&#x2F;OPCODE&gt;</span><br><span class="line">    &lt;DATA&gt;</span><br><span class="line">        &lt;TXID&gt;7&lt;&#x2F;TXID&gt;</span><br><span class="line">        &lt;LENGTH&gt;0&lt;&#x2F;LENGTH&gt;</span><br><span class="line">        &lt;INODEID&gt;16386&lt;&#x2F;INODEID&gt;</span><br><span class="line">        &lt;PATH&gt;&#x2F;input&lt;&#x2F;PATH&gt;</span><br><span class="line">        &lt;TIMESTAMP&gt;1621435219597&lt;&#x2F;TIMESTAMP&gt;</span><br><span class="line">        &lt;PERMISSION_STATUS&gt;</span><br><span class="line">            &lt;USERNAME&gt;extrali&lt;&#x2F;USERNAME&gt;</span><br><span class="line">            &lt;GROUPNAME&gt;supergroup&lt;&#x2F;GROUPNAME&gt;</span><br><span class="line">            &lt;MODE&gt;493&lt;&#x2F;MODE&gt;</span><br><span class="line">        &lt;&#x2F;PERMISSION_STATUS&gt;</span><br><span class="line">    &lt;&#x2F;DATA&gt;</span><br><span class="line">&lt;&#x2F;RECORD&gt;</span><br><span class="line">&lt;RECORD&gt;</span><br><span class="line">    &lt;OPCODE&gt;OP_ADD&lt;&#x2F;OPCODE&gt;</span><br><span class="line">    &lt;DATA&gt;</span><br><span class="line">        &lt;TXID&gt;8&lt;&#x2F;TXID&gt;</span><br><span class="line">        &lt;LENGTH&gt;0&lt;&#x2F;LENGTH&gt;</span><br><span class="line">        &lt;INODEID&gt;16387&lt;&#x2F;INODEID&gt;</span><br><span class="line">        &lt;PATH&gt;&#x2F;input&#x2F;word.txt._COPYING_&lt;&#x2F;PATH&gt;</span><br><span class="line">        &lt;REPLICATION&gt;3&lt;&#x2F;REPLICATION&gt;</span><br><span class="line">        &lt;MTIME&gt;1621435261137&lt;&#x2F;MTIME&gt;</span><br><span class="line">        &lt;ATIME&gt;1621435261137&lt;&#x2F;ATIME&gt;</span><br><span class="line">        &lt;BLOCKSIZE&gt;134217728&lt;&#x2F;BLOCKSIZE&gt;</span><br><span class="line">        &lt;CLIENT_NAME&gt;DFSClient_NONMAPREDUCE_1703766323_1&lt;&#x2F;CLIENT_NAME&gt;</span><br><span class="line">        &lt;CLIENT_MACHINE&gt;192.168.1.102&lt;&#x2F;CLIENT_MACHINE&gt;</span><br><span class="line">        &lt;OVERWRITE&gt;true&lt;&#x2F;OVERWRITE&gt;</span><br><span class="line">        &lt;PERMISSION_STATUS&gt;</span><br><span class="line">            &lt;USERNAME&gt;extrali&lt;&#x2F;USERNAME&gt;</span><br><span class="line">            &lt;GROUPNAME&gt;supergroup&lt;&#x2F;GROUPNAME&gt;</span><br><span class="line">            &lt;MODE&gt;420&lt;&#x2F;MODE&gt;</span><br><span class="line">        &lt;&#x2F;PERMISSION_STATUS&gt;</span><br><span class="line">        &lt;ERASURE_CODING_POLICY_ID&gt;0&lt;&#x2F;ERASURE_CODING_POLICY_ID&gt;</span><br><span class="line">        &lt;RPC_CLIENTID&gt;30084518-032c-4a6f-a1c9-cd4c8f9bcb11&lt;&#x2F;RPC_CLIENTID&gt;</span><br><span class="line">        &lt;RPC_CALLID&gt;3&lt;&#x2F;RPC_CALLID&gt;</span><br><span class="line">    &lt;&#x2F;DATA&gt;</span><br><span class="line">&lt;&#x2F;RECORD&gt;</span><br><span class="line">&lt;RECORD&gt;</span><br><span class="line">    &lt;OPCODE&gt;OP_ALLOCATE_BLOCK_ID&lt;&#x2F;OPCODE&gt;</span><br><span class="line">    &lt;DATA&gt;</span><br><span class="line">        &lt;TXID&gt;9&lt;&#x2F;TXID&gt;</span><br><span class="line">        &lt;BLOCK_ID&gt;1073741825&lt;&#x2F;BLOCK_ID&gt;</span><br><span class="line">    &lt;&#x2F;DATA&gt;</span><br><span class="line">&lt;&#x2F;RECORD&gt;</span><br><span class="line">&lt;RECORD&gt;</span><br><span class="line">    &lt;OPCODE&gt;OP_SET_GENSTAMP_V2&lt;&#x2F;OPCODE&gt;</span><br><span class="line">    &lt;DATA&gt;</span><br><span class="line">        &lt;TXID&gt;10&lt;&#x2F;TXID&gt;</span><br><span class="line">        &lt;GENSTAMPV2&gt;1001&lt;&#x2F;GENSTAMPV2&gt;</span><br><span class="line">    &lt;&#x2F;DATA&gt;</span><br><span class="line">&lt;&#x2F;RECORD&gt;</span><br><span class="line">&lt;RECORD&gt;</span><br><span class="line">    &lt;OPCODE&gt;OP_ADD_BLOCK&lt;&#x2F;OPCODE&gt;</span><br><span class="line">    &lt;DATA&gt;</span><br><span class="line">        &lt;TXID&gt;11&lt;&#x2F;TXID&gt;</span><br><span class="line">        &lt;PATH&gt;&#x2F;input&#x2F;word.txt._COPYING_&lt;&#x2F;PATH&gt;</span><br><span class="line">        &lt;BLOCK&gt;</span><br><span class="line">            &lt;BLOCK_ID&gt;1073741825&lt;&#x2F;BLOCK_ID&gt;</span><br><span class="line">            &lt;NUM_BYTES&gt;0&lt;&#x2F;NUM_BYTES&gt;</span><br><span class="line">            &lt;GENSTAMP&gt;1001&lt;&#x2F;GENSTAMP&gt;</span><br><span class="line">        &lt;&#x2F;BLOCK&gt;</span><br><span class="line">        &lt;RPC_CLIENTID&#x2F;&gt;</span><br><span class="line">        &lt;RPC_CALLID&gt;-2&lt;&#x2F;RPC_CALLID&gt;</span><br><span class="line">    &lt;&#x2F;DATA&gt;</span><br><span class="line">&lt;&#x2F;RECORD&gt;</span><br><span class="line">&lt;RECORD&gt;</span><br><span class="line">    &lt;OPCODE&gt;OP_CLOSE&lt;&#x2F;OPCODE&gt;</span><br><span class="line">    &lt;DATA&gt;</span><br><span class="line">        &lt;TXID&gt;12&lt;&#x2F;TXID&gt;</span><br><span class="line">        &lt;LENGTH&gt;0&lt;&#x2F;LENGTH&gt;</span><br><span class="line">        &lt;INODEID&gt;0&lt;&#x2F;INODEID&gt;</span><br><span class="line">        &lt;PATH&gt;&#x2F;input&#x2F;word.txt._COPYING_&lt;&#x2F;PATH&gt;</span><br><span class="line">        &lt;REPLICATION&gt;3&lt;&#x2F;REPLICATION&gt;</span><br><span class="line">        &lt;MTIME&gt;1621435263423&lt;&#x2F;MTIME&gt;</span><br><span class="line">        &lt;ATIME&gt;1621435261137&lt;&#x2F;ATIME&gt;</span><br><span class="line">        &lt;BLOCKSIZE&gt;134217728&lt;&#x2F;BLOCKSIZE&gt;</span><br><span class="line">        &lt;CLIENT_NAME&#x2F;&gt;</span><br><span class="line">        &lt;CLIENT_MACHINE&#x2F;&gt;</span><br><span class="line">        &lt;OVERWRITE&gt;false&lt;&#x2F;OVERWRITE&gt;</span><br><span class="line">        &lt;BLOCK&gt;</span><br><span class="line">            &lt;BLOCK_ID&gt;1073741825&lt;&#x2F;BLOCK_ID&gt;</span><br><span class="line">            &lt;NUM_BYTES&gt;46&lt;&#x2F;NUM_BYTES&gt;</span><br><span class="line">            &lt;GENSTAMP&gt;1001&lt;&#x2F;GENSTAMP&gt;</span><br><span class="line">        &lt;&#x2F;BLOCK&gt;</span><br><span class="line">        &lt;PERMISSION_STATUS&gt;</span><br><span class="line">            &lt;USERNAME&gt;extrali&lt;&#x2F;USERNAME&gt;</span><br><span class="line">            &lt;GROUPNAME&gt;supergroup&lt;&#x2F;GROUPNAME&gt;</span><br><span class="line">            &lt;MODE&gt;420&lt;&#x2F;MODE&gt;</span><br><span class="line">        &lt;&#x2F;PERMISSION_STATUS&gt;</span><br><span class="line">    &lt;&#x2F;DATA&gt;</span><br><span class="line">&lt;&#x2F;RECORD&gt;</span><br><span class="line">&lt;RECORD&gt;</span><br><span class="line">    &lt;OPCODE&gt;OP_RENAME_OLD&lt;&#x2F;OPCODE&gt;</span><br><span class="line">    &lt;DATA&gt;</span><br><span class="line">        &lt;TXID&gt;13&lt;&#x2F;TXID&gt;</span><br><span class="line">        &lt;LENGTH&gt;0&lt;&#x2F;LENGTH&gt;</span><br><span class="line">        &lt;SRC&gt;&#x2F;input&#x2F;word.txt._COPYING_&lt;&#x2F;SRC&gt;</span><br><span class="line">        &lt;DST&gt;&#x2F;input&#x2F;word.txt&lt;&#x2F;DST&gt;</span><br><span class="line">        &lt;TIMESTAMP&gt;1621435263452&lt;&#x2F;TIMESTAMP&gt;</span><br><span class="line">        &lt;RPC_CLIENTID&gt;30084518-032c-4a6f-a1c9-cd4c8f9bcb11&lt;&#x2F;RPC_CLIENTID&gt;</span><br><span class="line">        &lt;RPC_CALLID&gt;9&lt;&#x2F;RPC_CALLID&gt;</span><br><span class="line">    &lt;&#x2F;DATA&gt;</span><br><span class="line">&lt;&#x2F;RECORD&gt;</span><br><span class="line">&lt;RECORD&gt;</span><br><span class="line">    &lt;OPCODE&gt;OP_END_LOG_SEGMENT&lt;&#x2F;OPCODE&gt;</span><br><span class="line">    &lt;DATA&gt;</span><br><span class="line">        &lt;TXID&gt;14&lt;&#x2F;TXID&gt;</span><br><span class="line">    &lt;&#x2F;DATA&gt;</span><br><span class="line">&lt;&#x2F;RECORD&gt;</span><br><span class="line">&lt;&#x2F;EDITS&gt;</span><br></pre></td></tr></table></figure>

<h3 id="5-5-3-CheckPoint时间设置"><a href="#5-5-3-CheckPoint时间设置" class="headerlink" title="5.5.3 CheckPoint时间设置"></a>5.5.3 CheckPoint时间设置</h3></li>
</ul>
</li>
<li><p>通常情况下，SecondaryNameNode<strong>每隔一小时</strong>执行一次。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hdfs-default.xml]</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.checkpoint.period&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;3600s&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>一分钟检查一次操作次数，当<strong>操作次数达到 1 百万</strong>时， SecondaryNameNode 执行一次。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.checkpoint.txns&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;1000000&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;操作动作次数&lt;&#x2F;description&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;60s&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt; 1 分钟检查一次操作次数&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="5-6-DataNode"><a href="#5-6-DataNode" class="headerlink" title="5.6 DataNode"></a>5.6 DataNode</h2><h3 id="5-6-1-DataNode工作机制"><a href="#5-6-1-DataNode工作机制" class="headerlink" title="5.6.1 DataNode工作机制"></a>5.6.1 DataNode工作机制</h3><p><img src="https://i.loli.net/2021/05/30/xcorW9VIfdb4jqG.png" alt="image-20210530200135957"></p>
<ul>
<li><p>一个数据块在 DataNode 上以文件形式存储在磁盘上，包括两个文件，<strong>一个是数据本身</strong>，<strong>一个是元数据包括数据块的长度，块数据的校验和，以及时间戳</strong>。  </p>
</li>
<li><p>DataNode 启动后向 NameNode 注册，通过后，<strong>周期性（6 小时） 的向 NameNode 上报所有的块信息</strong>。  </p>
<p>DN 向 NN 汇报当前解读信息的时间间隔，默认 <strong>6 小时</strong>；  </p>
</li>
<li><p><strong>心跳是每 3 秒一次</strong>，心跳返回结果带有 NameNode 给该 DataNode 的命令如复制块数据到另一台机器，或删除某个数据块。 <strong>如果超过 10 分钟没有收到某个 DataNode 的心跳，则认为该节点不可用</strong>。  </p>
</li>
<li><p>集群运行中可以安全加入和退出一些机器。  </p>
</li>
</ul>
<h3 id="5-6-2-数据完整性"><a href="#5-6-2-数据完整性" class="headerlink" title="5.6.2  数据完整性"></a>5.6.2  数据完整性</h3><p><strong>思考：</strong>如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？ 同理 DataNode 节点上的数据损坏了， 却没有发现，是否也很危险， 那么如何解决呢？<br><strong>如下是 DataNode 节点保证数据完整性的方法。</strong> </p>
<ol>
<li>当 DataNode 读取 Block 的时候，它会计算 CheckSum。  </li>
<li>如果计算后的 CheckSum，与 Block 创建时值不一样，说明 Block 已经损坏。  </li>
<li>Client 读取其他 DataNode 上的 Block。  </li>
<li>常见的校验算法 crc（32）， md5（128）， sha1（160）  </li>
<li>DataNode 在其文件创建后周期验证 CheckSum。  </li>
</ol>
<p><img src="https://i.loli.net/2021/05/30/8jnSYyoIBODQRAm.png" alt="image-20210530201033348"></p>
<h3 id="5-6-3-掉线时限参数设置"><a href="#5-6-3-掉线时限参数设置" class="headerlink" title="5.6.3 掉线时限参数设置"></a>5.6.3 掉线时限参数设置</h3><p><img src="https://i.loli.net/2021/05/30/jM8Q35YNP6olHra.png" alt="image-20210530201506796"></p>
<p>需要注意的是 hdfs-site.xml 配置文件中的 heartbeat.recheck.interval 的单位为毫秒，dfs.heartbeat.interval 的单位为秒。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;300000&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.heartbeat.interval&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<h1 id="6-MapReduce"><a href="#6-MapReduce" class="headerlink" title="6.MapReduce"></a>6.MapReduce</h1><h2 id="6-1-MapReduce概述"><a href="#6-1-MapReduce概述" class="headerlink" title="6.1 MapReduce概述"></a>6.1 MapReduce概述</h2><h3 id="6-1-1-MapReduce定义"><a href="#6-1-1-MapReduce定义" class="headerlink" title="6.1.1 MapReduce定义"></a>6.1.1 MapReduce定义</h3><ul>
<li>MapReduce是一个<strong>分布式运算程序</strong>的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架</li>
<li>MapReduce的核心功能是将<strong>用户编写的业务逻辑代码</strong>和<strong>自带默认组件</strong>整合成一个完整的<strong>分布式运行程序</strong>，并发运行在一个Hadoop集群上</li>
</ul>
<h3 id="6-1-2-MapReduce优缺点"><a href="#6-1-2-MapReduce优缺点" class="headerlink" title="6.1.2 MapReduce优缺点"></a>6.1.2 MapReduce优缺点</h3><ol>
<li><p><strong>优点</strong></p>
<ul>
<li><p><strong>MapReduce易于编程</strong></p>
<p><strong>它简单的实现一些接口，就可以完成一个分布式程序</strong>， 这个分布式程序可以分布到大量廉价的 PC 机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得 MapReduce 编程变得非常流行。  </p>
</li>
<li><p><strong>良好的扩展性</strong></p>
<p>当你的计算资源不能得到满足的时候，你可以通过<strong>简单的增加机器</strong>来扩展它的计算能力。</p>
</li>
<li><p><strong>高容错性</strong></p>
<p>MapReduce 设计的初衷就是使程序能够部署在廉价的 PC 机器上，这就要求它具有很高的容错性。比如<strong>其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行不至于这个任务运行失败</strong>，而且这个过程不需要人工参与，而完全是由 Hadoop 内部完成的。  </p>
</li>
<li><p><strong>适合PB级以上海量数据的离线处理</strong></p>
<p>可以实现上千台服务器集群并发工作，提供数据处理能力  </p>
</li>
</ul>
</li>
<li><p><strong>缺点</strong></p>
<ul>
<li><p><strong>不擅长实时计算</strong></p>
<p>MapReduce 无法像 MySQL 一样，在毫秒或者秒级内返回结果  </p>
</li>
<li><p><strong>不擅长流式计算</strong></p>
<p><strong>流式计算的输入数据是动态的</strong>，而 <strong>MapReduce 的输入数据集是静态的</strong>，不能动态变化。这是因为 MapReduce 自身的设计特点决定了数据源必须是静态的。</p>
</li>
<li><p><strong>不擅长DAG（有向无环图）计算</strong></p>
<p>多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，<br>MapReduce 并不是不能做，而是使用后， <strong>每个 MapReduce 作业的输出结果都会写入到磁盘，会造成大量的磁盘 IO，导致性能非常的低下。</strong>  </p>
</li>
</ul>
</li>
</ol>
<h3 id="6-1-3-MapReduce核心思想"><a href="#6-1-3-MapReduce核心思想" class="headerlink" title="6.1.3 MapReduce核心思想"></a>6.1.3 MapReduce核心思想</h3><p><img src="https://i.loli.net/2021/06/19/cxmfPJE9s63WRFe.png" alt="image-20210619225054149"></p>
<ol>
<li>分布式的运行程序往往需要分为至少2个阶段</li>
<li>第一个阶段的 MapTask 并发实例，完全并行运行，互不相干  </li>
<li>第二个阶段的 ReduceTask 并发实例互不相干，但是他们的数据依赖于上一个阶段的所有 MapTask 并发实例的输出</li>
<li>MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce 阶段，如果用户的业务逻辑非常复杂，那就只能多个 MapReduce 程序，串行运行   </li>
</ol>
<p><strong>总结：</strong>分析 WordCount 数据流走向深入理解 MapReduce 核心思想  </p>
<h3 id="6-1-4-MapReduce进程"><a href="#6-1-4-MapReduce进程" class="headerlink" title="6.1.4 MapReduce进程"></a>6.1.4 MapReduce进程</h3><p>一个完整的 MapReduce 程序在分布式运行时有三类实例进程：  </p>
<ol>
<li><strong>MrAppMaster</strong>：负责整个程序的过程调度及状态协调。  </li>
<li><strong>MapTask</strong>：负责 Map 阶段的整个数据处理流程。</li>
<li><strong>ReduceTask</strong>：负责 Reduce 阶段的整个数据处理流程。  </li>
</ol>
<h3 id="6-1-5-官方WordCount源码"><a href="#6-1-5-官方WordCount源码" class="headerlink" title="6.1.5 官方WordCount源码"></a>6.1.5 官方WordCount源码</h3><p>采用<strong>反编译工具</strong>反编译源码，发现 WordCount 案例有 Map 类、 Reduce 类和驱动类。 且数据的类型是 Hadoop 自身封装的序列化类型。  </p>
<h3 id="6-1-6-常用数据序列化类型"><a href="#6-1-6-常用数据序列化类型" class="headerlink" title="6.1.6 常用数据序列化类型"></a>6.1.6 常用数据序列化类型</h3><table>
<thead>
<tr>
<th>Java 类型</th>
<th>Hadoop Writable 类型</th>
</tr>
</thead>
<tbody><tr>
<td>Boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>Byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>Int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>Float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>Long</td>
<td>LongWritable</td>
</tr>
<tr>
<td>Double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td><strong>String</strong></td>
<td><strong>Text</strong></td>
</tr>
<tr>
<td>Map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>Array</td>
<td>ArrayWritable</td>
</tr>
<tr>
<td>Null</td>
<td>NullWritable</td>
</tr>
</tbody></table>
<h3 id="6-1-7-MapReduce编程规范"><a href="#6-1-7-MapReduce编程规范" class="headerlink" title="6.1.7 MapReduce编程规范"></a>6.1.7 MapReduce编程规范</h3><p>用户编写的程序分成三个部分： <strong>Mapper、 Reducer 和 Driver</strong>。  </p>
<p><img src="https://i.loli.net/2021/06/19/aut6DEbTfS3d9AU.png" alt="image-20210619230804891"></p>
<p><img src="https://i.loli.net/2021/06/19/oLFhMCqZXSb47BP.png" alt="image-20210619230919450"></p>
<h3 id="6-1-8-WordCount案例实操"><a href="#6-1-8-WordCount案例实操" class="headerlink" title="6.1.8 WordCount案例实操"></a>6.1.8 WordCount案例实操</h3><ol>
<li><p>需求</p>
<p>在给定文本文件中统计输出每一个单词出现的总次数</p>
<ul>
<li><p>输入数据</p>
<p>hello.txt</p>
</li>
<li><p>期望输出数据</p>
<p>atguigu 2<br>banzhang 1<br>cls 2<br>hadoop 1<br>jiao 1<br>ss 2<br>xue 1  </p>
</li>
</ul>
</li>
<li><p>需求分析</p>
<p>按照 MapReduce 编程规范，分别编写 Mapper， Reducer， Driver。  </p>
</li>
</ol>
<p><img src="https://i.loli.net/2021/06/19/97VMaWZHkjuvsrL.png" alt="image-20210619231856410"></p>
<ol start="3">
<li><p>环境准备</p>
<ol>
<li><p>创建Maven工程，MapReduceDemo</p>
</li>
<li><p>在 pom.xml 文件中添加如下依赖  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.1.3&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;junit&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;junit&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;4.12&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.7.30&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在项目的 src/main/resources 目录下，新建一个文件，命名为“log4j.properties”，在文件中填入。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger&#x3D;INFO, stdout</span><br><span class="line">log4j.appender.stdout&#x3D;org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d %p [%c] - %m%n</span><br><span class="line">log4j.appender.logfile&#x3D;org.apache.log4j.FileAppender</span><br><span class="line">log4j.appender.logfile.File&#x3D;target&#x2F;spring.log</span><br><span class="line">log4j.appender.logfile.layout&#x3D;org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern&#x3D;%d %p [%c] - %m%n</span><br></pre></td></tr></table></figure>
</li>
<li><p>常见包名：com.hust.mapreduce.wordcount</p>
</li>
</ol>
</li>
<li><p>编写程序</p>
<ol>
<li><p>编写Mapper类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">package com.hust.mapreduce.wordcount;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class WordCountMapper extends Mapper&lt;LongWritable, Text,Text,LongWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private Text k &#x3D; new Text();</span><br><span class="line">    private LongWritable v &#x3D; new LongWritable(1);</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">    	&#x2F;&#x2F;1.获取一行</span><br><span class="line">        String line &#x3D; value.toString();</span><br><span class="line">		</span><br><span class="line">		&#x2F;&#x2F;2.切割</span><br><span class="line">        String[] words &#x3D; line.split(&quot; &quot;);</span><br><span class="line">		</span><br><span class="line">		&#x2F;&#x2F;3.输出</span><br><span class="line">        for(String s:words)&#123;</span><br><span class="line">            k.set(s);</span><br><span class="line">            context.write(k,v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Reducer类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">package com.hust.mapreduce.wordcount;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class WordCountReducer extends Reducer&lt;Text, LongWritable,Text,LongWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private LongWritable v &#x3D; new LongWritable();;</span><br><span class="line">    private int sum;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">    </span><br><span class="line">    	&#x2F;&#x2F;1.累加求和</span><br><span class="line">        sum &#x3D; 0;</span><br><span class="line">        for(LongWritable num: values)&#123;</span><br><span class="line">            sum +&#x3D; num.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F;2.输出</span><br><span class="line">        v.set(sum);</span><br><span class="line">        context.write(key, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Driver驱动类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">package com.hust.mapreduce.wordcount;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class WordCountDriver &#123;</span><br><span class="line">    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F;1.获取配置信息以及获取job对象</span><br><span class="line">        Configuration conf &#x3D; new Configuration();</span><br><span class="line">        Job job &#x3D; Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;2.关联本Driver程序的jar</span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;3.关联Mapper类和Reducer类的jar</span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;4.设置Mapper输出的kv类型</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(LongWritable.class);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;5.设置最终输出的kv类型</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(LongWritable.class);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;6.设置输入和输出路径</span><br><span class="line">        FileInputFormat.setInputPaths(job, new Path(args[0]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(args[1]));</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;7.提交job</span><br><span class="line">        boolean result &#x3D; job.waitForCompletion(true);</span><br><span class="line">        System.exit(result? 0: 1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>本地测试</p>
</li>
</ol>
</li>
</ol>
<p><strong>提交到集群测试：</strong></p>
<ol>
<li>用maven打jar包，需要添加的打包插件依赖</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.6.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;descriptorRefs&gt;</span><br><span class="line">                &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                &lt;goals&gt;</span><br><span class="line">                	&lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">            &lt;&#x2F;executions&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">    &lt;&#x2F;plugins&gt;</span><br><span class="line">&lt;&#x2F;build&gt;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>将程序打成jar包</p>
</li>
<li><p>修改<strong>不带依赖的 jar 包</strong>名称为 wc.jar，并拷贝该 jar 包到 Hadoop 集群的/opt/module/hadoop-3.1.3 路径。  </p>
</li>
<li><p>执行WordCount程序。</p>
<p><code>hadoop jar wc.jar com.hust.mapreduce.wordcount.WordCountDriver /input /output/outputword</code></p>
<p><strong>注意：此时由于集群中的hadoop已经配置了使用hdfs作为存储，所以后面的路径也是跟着hdfs中的路径</strong></p>
</li>
</ol>
<h2 id="6-2-Hadoop序列化"><a href="#6-2-Hadoop序列化" class="headerlink" title="6.2 Hadoop序列化"></a>6.2 Hadoop序列化</h2><h3 id="6-2-1-序列化概述"><a href="#6-2-1-序列化概述" class="headerlink" title="6.2.1 序列化概述"></a>6.2.1 序列化概述</h3><ol>
<li><p>什么是序列化？</p>
<ul>
<li><strong>序列化</strong>就是把<strong>内存中的对象，转换成字节序列</strong>（或其他数据传输协议）以便于<strong>存储到磁盘（持久化）和网络传输</strong>。</li>
<li><strong>反序列化</strong>就是将收到<strong>字节序列</strong>（或其他数据传输协议）或者是磁盘的持久化数据，<strong>转换成内存中的对象</strong>。  </li>
</ul>
</li>
<li><p>为什么要做序列化？</p>
<p>一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，<strong>可以将“活的”对象发送到远程计算机</strong>。  </p>
</li>
<li><p>为什么不用java的序列化？</p>
<p><strong>Java 的序列化是一个重量级序列化框架</strong>（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息， Header，继承体系等），不便于在网络中高效传输。所以，Hadoop 自己开发了一套序列化机制（<strong>Writable</strong>）。  </p>
</li>
<li><p>Hadoop序列化特点：</p>
<ul>
<li><strong>紧凑</strong> ： 高效使用存储空间。</li>
<li><strong>快速</strong>： 读写数据的额外开销小。</li>
<li><strong>互操作</strong>： 支持多语言的交互  </li>
</ul>
</li>
</ol>
<h3 id="6-2-2-自定义bean对象实现序列化接口-Writable"><a href="#6-2-2-自定义bean对象实现序列化接口-Writable" class="headerlink" title="6.2.2 自定义bean对象实现序列化接口(Writable)"></a>6.2.2 自定义bean对象实现序列化接口(Writable)</h3><p>​        在企业开发中往往常用的基本序列化类型不能满足所有需求， 比如在 Hadoop 框架内部传递一个 bean 对象， 那么该对象就需要<strong>实现序列化接口</strong>。  </p>
<p>具体实现bean对象序列化步骤如下7步：</p>
<ol>
<li><p>必须实现Writable接口</p>
</li>
<li><p>反序列化时，需要反射调用空参构造函数，所以必须要有空参构造</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public FlowBean() &#123;</span><br><span class="line">	super();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>重写序列化方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void write(DataOutput out) throws IOException &#123;</span><br><span class="line">    out.writeLong(upFlow);</span><br><span class="line">    out.writeLong(downFlow);</span><br><span class="line">    out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>重写反序列化方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">    upFlow &#x3D; in.readLong();</span><br><span class="line">    downFlow &#x3D; in.readLong();</span><br><span class="line">    sumFlow &#x3D; in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>注意反序列化的顺序和序列化的顺序完全一致</strong></p>
</li>
<li><p>要想把结果显示在文件中，需要重写 toString()，可用”\t”分开，方便后续用</p>
</li>
<li><p><strong>如果需要将自定义的 bean 放在 key 中传输，则还需要实现 Comparable 接口</strong>，因为MapReduce 框中的 Shuffle 过程要求对 key 必须能排序。 详见后面排序案例  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public int compareTo(FlowBean o) &#123;</span><br><span class="line">    &#x2F;&#x2F; 倒序排列，从大到小</span><br><span class="line">    return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="6-2-3-序列化案例实操"><a href="#6-2-3-序列化案例实操" class="headerlink" title="6.2.3 序列化案例实操"></a>6.2.3 序列化案例实操</h3><ol>
<li><p>需求</p>
<p>统计每一个手机号耗费的总上行流量、 总下行流量、总流量  </p>
<ol>
<li><p>输入数据</p>
<p>phone.txt</p>
</li>
<li><p>输入数据格式</p>
<table>
<thead>
<tr>
<th>7</th>
<th>13590439668</th>
<th>192.168.100.4</th>
<th>1116</th>
<th>954</th>
<th>200</th>
</tr>
</thead>
<tbody><tr>
<td>id</td>
<td>手机号码</td>
<td>网络ip</td>
<td>上行流量</td>
<td>下行流量</td>
<td>网络状态码</td>
</tr>
</tbody></table>
</li>
<li><p>期望输出数据格式</p>
<table>
<thead>
<tr>
<th>13560436666</th>
<th>1116</th>
<th>954</th>
<th>2070</th>
</tr>
</thead>
<tbody><tr>
<td>手机号码</td>
<td>上行流量</td>
<td>下行流量</td>
<td>总流量</td>
</tr>
</tbody></table>
</li>
</ol>
</li>
<li><p>需求分析</p>
<p><img src="https://i.loli.net/2021/06/26/ND4Q7BAqcVTsfdo.png" alt="image-20210626105945502"></p>
</li>
<li><p>编写MapReduce程序</p>
<ol>
<li><p>编写流量统计的Bean对象</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">package com.hust.mapreduce.writable;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line">import java.io.DataInput;</span><br><span class="line">import java.io.DataOutput;</span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;1.继承Writable接口</span><br><span class="line">public class FlowBean implements Writable &#123;</span><br><span class="line"></span><br><span class="line">    private long upFlow;&#x2F;&#x2F;上行流量</span><br><span class="line"></span><br><span class="line">    private long downFlow;&#x2F;&#x2F;下行流量</span><br><span class="line"></span><br><span class="line">    private long sumFlow;&#x2F;&#x2F;总流量</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;2.提供无参构造方法</span><br><span class="line">    public FlowBean()&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;3.提供getter和setter方法</span><br><span class="line">    public long getUpFlow() &#123;</span><br><span class="line">        return upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setUpFlow(long upFlow) &#123;</span><br><span class="line">        this.upFlow &#x3D; upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public long getDownFlow() &#123;</span><br><span class="line">        return downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setDownFlow(long downFlow) &#123;</span><br><span class="line">        this.downFlow &#x3D; downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public long getSumFlow() &#123;</span><br><span class="line">        return sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setSumFlow(long sumFlow) &#123;</span><br><span class="line">        this.sumFlow &#x3D; sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setSumFlow()&#123;</span><br><span class="line">        this.sumFlow&#x3D;this.downFlow+this.upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;4.实现序列化和反序列化方法，注意顺序一定要保持一致</span><br><span class="line">    @Override</span><br><span class="line">    public void write(DataOutput dataOutput) throws IOException &#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void readFields(DataInput dataInput) throws IOException &#123;</span><br><span class="line">        this.upFlow&#x3D;dataInput.readLong();</span><br><span class="line">        this.downFlow&#x3D;dataInput.readLong();</span><br><span class="line">        this.sumFlow&#x3D;dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;5.重写toString()方法</span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return upFlow +</span><br><span class="line">                &quot;\t&quot; + downFlow +</span><br><span class="line">                &quot;\t&quot; + sumFlow ;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Mapper类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">package com.hust.mapreduce.writable;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class FlowMapper extends Mapper&lt;LongWritable, Text,Text,FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private Text k &#x3D;new Text();</span><br><span class="line"></span><br><span class="line">    private FlowBean v &#x3D; new FlowBean();</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        String line &#x3D; value.toString();</span><br><span class="line"></span><br><span class="line">        String[] words &#x3D; line.split(&quot;\\t&quot;);</span><br><span class="line"></span><br><span class="line">        String phoneNumber &#x3D; words[1];</span><br><span class="line"></span><br><span class="line">        k.set(phoneNumber);</span><br><span class="line"></span><br><span class="line">        v.setUpFlow(Long.parseLong(words[words.length-3]));</span><br><span class="line">        v.setDownFlow(Long.parseLong(words[words.length-2]));</span><br><span class="line">        v.setSumFlow();</span><br><span class="line"></span><br><span class="line">        context.write(k,v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Reducer类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">package com.hust.mapreduce.writable;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class FlowReducer extends Reducer&lt;Text,FlowBean,Text,FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private FlowBean outV &#x3D; new FlowBean();</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        long totalUp &#x3D; 0;</span><br><span class="line">        long totalDown &#x3D; 0;</span><br><span class="line">        for(FlowBean bean: values)&#123;</span><br><span class="line">            totalUp +&#x3D;bean.getUpFlow();</span><br><span class="line">            totalDown +&#x3D;bean.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line">        outV.setUpFlow(totalUp);</span><br><span class="line">        outV.setDownFlow(totalDown);</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line">        context.write(key,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Driver类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">package com.hust.mapreduce.writable;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class FlowDriver &#123;</span><br><span class="line">    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F;1.获取配置信息以及获取job对象</span><br><span class="line">        Configuration conf &#x3D; new Configuration();</span><br><span class="line">        Job job &#x3D; Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;2.关联本Driver程序的jar</span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;3.关联Mapper类和Reducer类的jar</span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;4.设置Mapper输出的kv类型</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;5.设置最终输出的kv类型</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;6.设置输入和输出路径</span><br><span class="line">        FileInputFormat.setInputPaths(job, new Path(&quot;E:\\input\\inputflow&quot;));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(&quot;E:\\output\\outputflow&quot;));</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;7.提交job</span><br><span class="line">        boolean result &#x3D; job.waitForCompletion(true);</span><br><span class="line">        System.exit(result? 0: 1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h2 id="6-3-MapReduce框架原理"><a href="#6-3-MapReduce框架原理" class="headerlink" title="6.3 MapReduce框架原理"></a>6.3 MapReduce框架原理</h2><p><img src="https://i.loli.net/2021/06/26/q2xUB6LjTY89EgG.png" alt="image-20210626125619570"></p>
<h3 id="6-3-1-InputFormat数据输入"><a href="#6-3-1-InputFormat数据输入" class="headerlink" title="6.3.1 InputFormat数据输入"></a>6.3.1 InputFormat数据输入</h3><h4 id="6-3-1-1-切片与MapTask并行度决定机制"><a href="#6-3-1-1-切片与MapTask并行度决定机制" class="headerlink" title="6.3.1.1 切片与MapTask并行度决定机制"></a>6.3.1.1 切片与MapTask并行度决定机制</h4><ol>
<li><p>问题提出</p>
<p>MapTask 的并行度决定 Map 阶段的任务处理并发度，进而影响到整个 Job 的处理速度。<br><strong>思考： 1G 的数据， 启动 8 个 MapTask， 可以提高集群的并发处理能力。那么 1K 的数据，也启动 8 个 MapTask，会提高集群性能吗？ MapTask 并行任务是否越多越好呢？ 哪些因素影响了 MapTask 并行度？</strong>  </p>
</li>
<li><p><strong>MapTask并行度决定机制</strong></p>
<p><strong>数据块：</strong>Block 是 HDFS 物理上把数据分成一块一块。 数据块是 HDFS 存储数据单位。<br><strong>数据切片：</strong>数据切片只是在逻辑上对输入进行分片， 并不会在磁盘上将其切分成片进行存储。数据切片是 MapReduce 程序计算输入数据的单位，一个切片会对应启动一个 MapTask。</p>
<p><img src="https://i.loli.net/2021/06/26/po7WzAMnT9eOQug.png" alt="image-20210626203131151"></p>
</li>
</ol>
<h4 id="6-3-1-2-Job提交流程源码和切片源码详解"><a href="#6-3-1-2-Job提交流程源码和切片源码详解" class="headerlink" title="6.3.1.2 Job提交流程源码和切片源码详解"></a>6.3.1.2 Job提交流程源码和切片源码详解</h4><ol>
<li><p>Job提交流程源码详解</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line">    &#x2F;&#x2F; 1 建立连接</span><br><span class="line">    connect();</span><br><span class="line">        &#x2F;&#x2F; 1）创建提交 Job 的代理</span><br><span class="line">        new Cluster(getConfiguration());</span><br><span class="line">            &#x2F;&#x2F; （1）判断是本地运行环境还是 yarn 集群运行环境</span><br><span class="line">            initialize(jobTrackAddr, conf);</span><br><span class="line">    &#x2F;&#x2F; 2 提交 job</span><br><span class="line">    submitter.submitJobInternal(Job.this, cluster)</span><br><span class="line">        &#x2F;&#x2F; 1）创建给集群提交数据的 Stag 路径</span><br><span class="line">        Path jobStagingArea &#x3D; JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line">        &#x2F;&#x2F; 2）获取 jobid ，并创建 Job 路径</span><br><span class="line">        JobID jobId &#x3D; submitClient.getNewJobID();</span><br><span class="line">        &#x2F;&#x2F; 3）拷贝 jar 包到集群</span><br><span class="line">        copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">        rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line">        &#x2F;&#x2F; 4）计算切片，生成切片规划文件</span><br><span class="line">        writeSplits(job, submitJobDir);</span><br><span class="line">        maps &#x3D; writeNewSplits(job, jobSubmitDir);</span><br><span class="line">        input.getSplits(job);</span><br><span class="line">        &#x2F;&#x2F; 5）向 Stag 路径写 XML 配置文件</span><br><span class="line">        writeConf(conf, submitJobFile);</span><br><span class="line">        conf.writeXml(out);</span><br><span class="line">        &#x2F;&#x2F; 6）提交 Job,返回提交状态</span><br><span class="line">        status &#x3D; submitClient.submitJob(jobId, submitJobDir.toString(),job.getCredentials());</span><br></pre></td></tr></table></figure>

<p><img src="https://i.loli.net/2021/06/26/uWkcv5KhJYNat4m.png" alt="image-20210626212835668"></p>
<ol start="2">
<li><p><code>FileInputFormat切片源码解析(input.getsplits(job))</code></p>
<p><img src="https://i.loli.net/2021/06/26/3GLHnBfeZRJpd7A.png" alt="image-20210626230725587"></p>
</li>
</ol>
</li>
</ol>
<h4 id="6-3-1-3-FileInputFormat切片机制"><a href="#6-3-1-3-FileInputFormat切片机制" class="headerlink" title="6.3.1.3 FileInputFormat切片机制"></a>6.3.1.3 FileInputFormat切片机制</h4><p><img src="https://i.loli.net/2021/06/26/scbSkQJaqR8COXG.png" alt="image-20210626233219485"></p>
<p><img src="https://i.loli.net/2021/06/26/D2dGpHtu6mCniKq.png" alt="image-20210626233230125"></p>
<h4 id="6-3-1-4-TextInputFormat"><a href="#6-3-1-4-TextInputFormat" class="headerlink" title="6.3.1.4 TextInputFormat"></a>6.3.1.4 TextInputFormat</h4><ol>
<li><p>TextInputFormat实现类</p>
<p><strong>思考：在运行 MapReduce 程序时，输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等</strong>。 那么，针对不同的数据类型， MapReduce 是如何读取这些数据的呢？<br>FileInputFormat 常见的接口实现类包括： <strong>TextInputFormat、 KeyValueTextInputFormat、NLineInputFormat、 CombineTextInputFormat 和自定义 InputFormat 等</strong>。  </p>
</li>
<li><p>TextInputFormat</p>
<p>TextInputFormat 是默认的 FileInputFormat 实现类。<strong>按行读取每条记录</strong>。 <strong>键是存储该行在整个文件中的起始字节偏移量， LongWritable 类型</strong>。<strong>值是这行的内容，不包括任何行终止符（换行符和回车符）， Text 类型</strong>。  </p>
</li>
</ol>
<p>   以下是一个示例，比如，一个分片包含了如下4条文本记录。</p>
   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Rich learning form</span><br><span class="line">Intelligent learning engine</span><br><span class="line">Learning more convenient</span><br><span class="line">From the real demand for more close to the enterprise</span><br></pre></td></tr></table></figure>

<p>   每条记录表示以下键值对</p>
   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(0,Rich learning form)</span><br><span class="line">(20,Intelligent learning engine)</span><br><span class="line">(49,Learning more convenient)</span><br><span class="line">(74,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure>

<h4 id="6-3-1-5-CombineTextInputFormat切片机制"><a href="#6-3-1-5-CombineTextInputFormat切片机制" class="headerlink" title="6.3.1.5 CombineTextInputFormat切片机制"></a>6.3.1.5 CombineTextInputFormat切片机制</h4><p>框架默认的<strong>TextInputFormat</strong>切片机制是对任务按文件规划切片，<strong>不过文件多小，都会是一个单独的切片</strong>，都会交给一个MapTask，这样<strong>如果有大量小文件，就会产生大量的MapTask</strong>，处理效率极其低下。</p>
<ol>
<li><p><strong>应用场景：</strong></p>
<p>CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样多个小文件就可以交给一个MapTask处理。</p>
</li>
<li><p><strong>虚拟存储切片最大值设置：</strong></p>
<p><code>CombineTextInputFormat.setMaxInputSplitSize(job,4194304)//4M</code></p>
<p>注意： 虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。  </p>
</li>
<li><p><strong>切片机制：</strong></p>
<p>生成切片过程包括： 虚拟存储过程和切片过程二部分。  </p>
<p><img src="https://i.loli.net/2021/06/27/NI8EULWrHVlzSni.png" alt="image-20210627221228126"></p>
<ol>
<li><p>虚拟存储过程</p>
<ul>
<li><p>将输入目录下所有文件大小， 依次和设置的 setMaxInputSplitSize 值比较， 如果不大于设置的最大值， 逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块； 当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时将文件均分成 2 个虚拟存储块（防止出现太小切片）。</p>
</li>
<li><p>例如 setMaxInputSplitSize 值为 4M， 输入文件大小为 8.02M，则先逻辑上分成一个4M。 剩余的大小为 4.02M，如果按照 4M 逻辑划分，就会出现 0.02M 的小的虚拟存储文件， 所以将剩余的 4.02M 文件切分成（2.01M 和 2.01M）两个文件。</p>
</li>
</ul>
</li>
<li><p>切片过程</p>
<ol>
<li>判断虚拟存储的文件大小是否大于 setMaxInputSplitSize 值，大于等于则单独形成一个切片。 </li>
<li>如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。  </li>
<li>测试举例：有 4 个小文件大小分别为 1.7M、 5.1M、 3.4M 以及 6.8M 这四个小文件，则虚拟存储之后形成 6 个文件块，大小分别为：<br>1.7M，（2.55M、 2.55M） ， 3.4M 以及（3.4M、 3.4M）<br>最终会形成 3 个切片，大小分别为：<br>（1.7+2.55）M，（2.55+3.4）M， （3.4+3.4）M   </li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="6-3-1-6-CombineTextInputFormat案例实操"><a href="#6-3-1-6-CombineTextInputFormat案例实操" class="headerlink" title="6.3.1.6 CombineTextInputFormat案例实操"></a>6.3.1.6 CombineTextInputFormat案例实操</h4><ol>
<li><p>需求</p>
<p>将输入的大量小文件合并成一个切片统一处理。</p>
<ol>
<li><p>输入数据</p>
<p>准备4个小文件a.txt   b.txt   c.txt   d.txt</p>
</li>
<li><p>期望</p>
<p>期望一个切片处理4个文件</p>
</li>
</ol>
</li>
<li><p>实现过程</p>
<ol>
<li><p>不做任何处理，运行6.1.8节的WordCount案例程序，观察切片数为4。</p>
<p><code>number of splits:4</code> </p>
</li>
<li><p>在 WordcountDriver 中增加如下代码， 运行程序，并观察运行的切片个数为 3。  </p>
<ul>
<li><p>驱动类中添加代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 如果不设置 InputFormat，它默认用的是 TextInputFormat.class</span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">&#x2F;&#x2F;虚拟存储切片最大值设置 4m</span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);</span><br></pre></td></tr></table></figure>
</li>
<li><p>运行结果为3个切片</p>
<p><code>number of splits:3</code></p>
</li>
</ul>
</li>
<li><p>在 WordcountDriver 中增加如下代码， 运行程序，并观察运行的切片个数为 1。</p>
<ul>
<li><p>驱动中添加代码如下：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 如果不设置 InputFormat，它默认用的是 TextInputFormat.class</span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">&#x2F;&#x2F;虚拟存储切片最大值设置 20m</span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, 20971520);</span><br></pre></td></tr></table></figure>
</li>
<li><p>运行结果为1个切片</p>
<p><code>number of splits:1</code></p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="6-3-2-MapReduce工作流程"><a href="#6-3-2-MapReduce工作流程" class="headerlink" title="6.3.2 MapReduce工作流程"></a>6.3.2 MapReduce工作流程</h3><p><img src="https://i.loli.net/2021/06/27/yYpnmWRhrwtaTCb.png" alt="image-20210627223654111"></p>
<p><img src="https://i.loli.net/2021/06/27/GnaX1rVjwO2Qdlq.png" alt="image-20210627225622448"></p>
<p>上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解如下：</p>
<ol>
<li>MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中</li>
<li>从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</li>
<li>多个溢出文件会被合并成大的溢出文件  </li>
<li>在溢出过程及合并的过程中，都要调用 Partitioner 进行分区和针对 key 进行排序  </li>
<li>ReduceTask 根据自己的分区号，去各个 MapTask 机器上取相应的结果分区数据</li>
<li>ReduceTask 会抓取到同一个分区的来自不同 MapTask 的结果文件， ReduceTask 会将这些文件再进行合并（归并排序）  </li>
<li>合并成大文件后， Shuffle 的过程也就结束了，后面进入 ReduceTask 的逻辑运算过程（从文件中取出一个一个的键值对 Group，调用用户自定义的 reduce()方法    </li>
</ol>
<p><strong>注意：</strong></p>
<ol>
<li>Shuffle 中的缓冲区大小会影响到 MapReduce 程序的执行效率，原则上说，缓冲区越大，磁盘 io 的次数越少，执行速度就越快。</li>
<li>缓冲区的大小可以通过参数调整，参数： mapreduce.task.io.sort.mb 默认 100M。  </li>
</ol>
<h3 id="6-3-3-Shuffle机制"><a href="#6-3-3-Shuffle机制" class="headerlink" title="6.3.3 Shuffle机制"></a>6.3.3 Shuffle机制</h3><h4 id="6-3-3-1-Shuffle机制"><a href="#6-3-3-1-Shuffle机制" class="headerlink" title="6.3.3.1 Shuffle机制"></a>6.3.3.1 Shuffle机制</h4><p><strong>Map 方法之后，Reduce 方法之前</strong>的数据处理过程称之为 Shuffle。  </p>
<p><img src="https://i.loli.net/2021/06/28/ghjxdyDNP57Xw1G.png" alt="image-20210628222049528"></p>
<h4 id="6-3-3-2-Partition分区"><a href="#6-3-3-2-Partition分区" class="headerlink" title="6.3.3.2 Partition分区"></a>6.3.3.2 Partition分区</h4><p><img src="https://i.loli.net/2021/06/28/ksP5eptxvQgLOcB.png" alt="image-20210628223206394"></p>
<p><img src="https://i.loli.net/2021/06/28/dyWu4XMeUTmn2LB.png" alt="image-20210628224946243"></p>
<p><img src="https://i.loli.net/2021/06/29/3e4ScbiJ6vqzaxg.png" alt="image-20210629181914696"></p>
<h4 id="6-3-3-3-Partition分区案例实操"><a href="#6-3-3-3-Partition分区案例实操" class="headerlink" title="6.3.3.3 Partition分区案例实操"></a>6.3.3.3 Partition分区案例实操</h4><ol>
<li><p>需求</p>
<p>将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p>
<ul>
<li><p>输入数据</p>
<p>phone_data.txt</p>
</li>
<li><p>期望输出数据</p>
<p>手机号 136、 137、 138、 139 开头都分别放到一个独立的 4 个文件中，其他开头的放到一个文件中。</p>
</li>
</ul>
</li>
<li><p>需求分析</p>
<p><img src="https://i.loli.net/2021/06/29/Ca6klfQsGO3bhzA.png" alt="image-20210629182757321"></p>
</li>
<li><p>在案例6.2.3的基础上，增加一个分区类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.partitioner;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line">public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int getPartition(Text text, FlowBean flowBean, int numPartitions)</span><br><span class="line">    &#123;</span><br><span class="line">        &#x2F;&#x2F;获取手机号前三位 prePhone</span><br><span class="line">        String phone &#x3D; text.toString();</span><br><span class="line">        String prePhone &#x3D; phone.substring(0, 3);</span><br><span class="line">        &#x2F;&#x2F;定义一个分区号变量 partition,根据 prePhone 设置分区号</span><br><span class="line">        int partition;</span><br><span class="line">        if(&quot;136&quot;.equals(prePhone))&#123;</span><br><span class="line">        partition &#x3D; 0;</span><br><span class="line">        &#125;else if(&quot;137&quot;.equals(prePhone))&#123;</span><br><span class="line">        partition &#x3D; 1;</span><br><span class="line">        &#125;else if(&quot;138&quot;.equals(prePhone))&#123;</span><br><span class="line">        partition &#x3D; 2;</span><br><span class="line">        &#125;else if(&quot;139&quot;.equals(prePhone))&#123;</span><br><span class="line">        partition &#x3D; 3;</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">        partition &#x3D; 4;</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F;最后返回分区号 partition</span><br><span class="line">        return partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在驱动函数中增加自定义数据分区设置和 ReduceTask 设置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;8 指定自定义分区器</span><br><span class="line">job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;9 同时指定相应数量的 ReduceTask</span><br><span class="line">job.setNumReduceTasks(5);</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="6-3-3-4-WritableComparable排序"><a href="#6-3-3-4-WritableComparable排序" class="headerlink" title="6.3.3.4 WritableComparable排序"></a>6.3.3.4 WritableComparable排序</h4><p><img src="https://i.loli.net/2021/06/29/eTkYOmPdnoGDWN2.png" alt="image-20210629213552818"></p>
<p><img src="https://i.loli.net/2021/06/29/lLwJcvWzRK2rduP.png" alt="image-20210629214710729"></p>
<p><img src="https://i.loli.net/2021/06/29/ZsECyhMtxDO4mTw.png" alt="image-20210629214923383"></p>
<p><strong>自定义排序WritableComparable原理分析：</strong></p>
<ul>
<li>bean 对象做为 <strong>key</strong> 传输，需要实现 WritableComparable 接口重写 compareTo 方法， 就可以实现排序。  </li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public int compareTo(FlowBean bean) &#123;</span><br><span class="line">    int result;</span><br><span class="line">    &#x2F;&#x2F; 按照总流量大小，倒序排列</span><br><span class="line">    if (this.sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">    result &#x3D; -1;</span><br><span class="line">    &#125;else if (this.sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">    result &#x3D; 1;</span><br><span class="line">    &#125;else &#123;</span><br><span class="line">    result &#x3D; 0;</span><br><span class="line">    &#125;</span><br><span class="line">    return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="6-3-3-5-WritableComparable-排序案例实操（全排序）"><a href="#6-3-3-5-WritableComparable-排序案例实操（全排序）" class="headerlink" title="6.3.3.5 WritableComparable 排序案例实操（全排序）"></a>6.3.3.5 WritableComparable 排序案例实操（全排序）</h4><ol>
<li><p>需求</p>
<p>根据案例 6.2.3 序列化案例产生的结果再次对总流量进行倒序排序。  </p>
<ul>
<li><p>输入数据</p>
<p>phone_data.txt</p>
</li>
<li><p>期望输出数据</p>
<p>13509468723 7335 110349 <strong>117684</strong><br>13736230513 2481 24681 <strong>27162</strong><br>13956435636 132 1512 <strong>1644</strong><br>13846544121 264 0 <strong>264</strong><br>。。 。 。 。。  </p>
</li>
</ul>
</li>
<li><p>需求分析</p>
<p><img src="https://i.loli.net/2021/06/29/Sp1AY5ndQmLOfzr.png" alt="image-20210629220044878"></p>
<p>由于需要对总流量进行排序，那么需要将FlowBean作为Mapper输出的key，因此需要实现WritableComparable接口</p>
</li>
<li><p>代码实现</p>
<ul>
<li><p>FlowBean对象额外增加了比较功能</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public int compareTo(FlowBean o) &#123;</span><br><span class="line">    &#x2F;&#x2F;按照总流量比较,倒序排列</span><br><span class="line">    if(this.sumFlow &gt; o.sumFlow)&#123;</span><br><span class="line">    	return -1;</span><br><span class="line">    &#125;else if(this.sumFlow &lt; o.sumFlow)&#123;</span><br><span class="line">    	return 1;</span><br><span class="line">    &#125;else &#123;</span><br><span class="line">    	return 0;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Mapper类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.writablecompable;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">public class FlowMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt;</span><br><span class="line">&#123;</span><br><span class="line">    private FlowBean outK &#x3D; new FlowBean();</span><br><span class="line">    private Text outV &#x3D; new Text();</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">    throws IOException, InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F;1 获取一行数据</span><br><span class="line">        String line &#x3D; value.toString();</span><br><span class="line">        &#x2F;&#x2F;2 按照&quot;\t&quot;,切割数据</span><br><span class="line">        String[] split &#x3D; line.split(&quot;\t&quot;);</span><br><span class="line">        &#x2F;&#x2F;3 封装 outK outV</span><br><span class="line">        outK.setUpFlow(Long.parseLong(split[1]));</span><br><span class="line">        outK.setDownFlow(Long.parseLong(split[2]));</span><br><span class="line">        outK.setSumFlow();</span><br><span class="line">        outV.set(split[0]);</span><br><span class="line">        &#x2F;&#x2F;4 写出 outK outV</span><br><span class="line">        context.write(outK,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Reducer类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.writablecompable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">public class FlowReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt;</span><br><span class="line">&#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context</span><br><span class="line">    context) throws IOException, InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F;遍历 values 集合,循环写出,避免总流量相同的情况</span><br><span class="line">        for (Text value : values) &#123;</span><br><span class="line">            &#x2F;&#x2F;调换 KV 位置,反向写出</span><br><span class="line">            context.write(value,key);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Driver类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;4 设置 Map 端输出数据的 KV 类型</span><br><span class="line">job.setMapOutputKeyClass(FlowBean.class);</span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ol>
<h4 id="6-3-3-6-WritableComparable-排序案例实操（区内排序）"><a href="#6-3-3-6-WritableComparable-排序案例实操（区内排序）" class="headerlink" title="6.3.3.6 WritableComparable 排序案例实操（区内排序）"></a>6.3.3.6 WritableComparable 排序案例实操（区内排序）</h4><ol>
<li><p>需求</p>
<p>要求每个省份手机号输出的文件中按照总流量内部排序。  </p>
</li>
<li><p>需求分析</p>
<p>基于前一个需求， <strong>增加自定义分区类</strong>， 分区按照省份手机号设置。  </p>
</li>
</ol>
<h4 id="6-3-3-7-Combiner合并"><a href="#6-3-3-7-Combiner合并" class="headerlink" title="6.3.3.7 Combiner合并"></a>6.3.3.7 Combiner合并</h4><p><img src="https://i.loli.net/2021/06/30/UnM36Smoq5V4yK2.png" alt="image-20210630000544964"></p>
<ul>
<li><p>自定义Combiner实现步骤</p>
<ol>
<li><p>自定义一个 Combiner 继承 Reducer，重写 Reduce 方法  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text,</span><br><span class="line">IntWritable&gt; &#123;</span><br><span class="line">    private IntWritable outV &#x3D; new IntWritable();</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context</span><br><span class="line">        context) throws IOException, InterruptedException &#123;</span><br><span class="line">        int sum &#x3D; 0;</span><br><span class="line">        for (IntWritable value : values) &#123;</span><br><span class="line">        	sum +&#x3D; value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        outV.set(sum);</span><br><span class="line">        context.write(key,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在Job驱动类中设置：</p>
<p><code>job.setCombinerClass(WordCountCombiner.class);</code></p>
</li>
</ol>
</li>
</ul>
<h4 id="6-3-3-8-Combiner-合并案例实操"><a href="#6-3-3-8-Combiner-合并案例实操" class="headerlink" title="6.3.3.8 Combiner 合并案例实操"></a>6.3.3.8 Combiner 合并案例实操</h4><ol>
<li><p>需求</p>
<p>统计过程中对每一个 MapTask 的输出进行局部汇总，以减小网络传输量即采用Combiner 功能。  </p>
<ul>
<li><p>数据输入</p>
<p>hello.txt</p>
</li>
<li><p>期望输出数据</p>
<p>期望： Combine 输入数据多，输出时经过合并，输出数据降低。  </p>
</li>
</ul>
</li>
<li><p>需求分析</p>
<p><img src="https://i.loli.net/2021/06/30/jQTBvHoWJ97zMxs.png" alt="image-20210630124054510"></p>
</li>
<li><p>案例实操-方案一</p>
<ul>
<li><p>增加一个 WordCountCombiner 类继承 Reducer  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.combiner;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">public class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text,</span><br><span class="line">IntWritable&gt; &#123;</span><br><span class="line">    private IntWritable outV &#x3D; new IntWritable();</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context</span><br><span class="line">    context) throws IOException, InterruptedException &#123;</span><br><span class="line">        int sum &#x3D; 0;</span><br><span class="line">        for (IntWritable value : values) &#123;</span><br><span class="line">        	sum +&#x3D; value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F;封装 outKV</span><br><span class="line">        outV.set(sum);</span><br><span class="line">        &#x2F;&#x2F;写出 outKV</span><br><span class="line">        context.write(key,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在WordCountDriver驱动类中指定Combiner</p>
<p><code>// 指定需要使用 combiner，以及用哪个类作为 combiner 的逻辑
job.setCombinerClass(WordCountCombiner.class);</code></p>
</li>
</ul>
</li>
<li><p>案例实操-方案二</p>
<p>将 WordcountReducer 作为 Combiner 在 WordcountDriver 驱动类中指定  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 指定需要使用 Combiner，以及用哪个类作为 Combiner 的逻辑</span><br><span class="line">job.setCombinerClass(WordCountReducer.class);</span><br></pre></td></tr></table></figure>

<p>运行程序，如下图所示  </p>
<p><img src="https://i.loli.net/2021/06/30/oNpBwXltaJGrnz5.png" alt="image-20210630162526082"></p>
</li>
</ol>
<h3 id="6-3-4-OutputFormat数据输出"><a href="#6-3-4-OutputFormat数据输出" class="headerlink" title="6.3.4 OutputFormat数据输出"></a>6.3.4 OutputFormat数据输出</h3><h4 id="6-3-4-1-OutputFormat接口实现类"><a href="#6-3-4-1-OutputFormat接口实现类" class="headerlink" title="6.3.4.1 OutputFormat接口实现类"></a>6.3.4.1 OutputFormat接口实现类</h4><p><img src="https://i.loli.net/2021/06/30/iEOJtl8wqP57LKa.png" alt="image-20210630162631126"></p>
<h4 id="6-3-4-2-自定义OutputFormat案例实操"><a href="#6-3-4-2-自定义OutputFormat案例实操" class="headerlink" title="6.3.4.2 自定义OutputFormat案例实操"></a>6.3.4.2 自定义OutputFormat案例实操</h4><ol>
<li><p>需求</p>
<p>过滤输入的 log 日志，包含 atguigu 的网站输出到 e:/atguigu.log，不包含 atguigu 的网站输出到 e:/other.log。</p>
<ul>
<li><p>输入数据</p>
<p>log.txt</p>
</li>
<li><p>期望输出数据</p>
<p>atguigu.log     other.log</p>
</li>
</ul>
</li>
<li><p>需求分析</p>
<p><img src="https://i.loli.net/2021/06/30/ZM1f4XnSlyKNUFv.png" alt="image-20210630165134150"></p>
</li>
<li><p>案例实操</p>
<ol>
<li><p>编写LogMapper类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.outputformat;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">public class LogMapper extends Mapper&lt;LongWritable, Text,Text,</span><br><span class="line">NullWritable&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">    throws IOException, InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F;不做任何处理,直接写出一行 log 数据</span><br><span class="line">        context.write(value,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写LogReducer类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.outputformat;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">public class LogReducer extends Reducer&lt;Text, NullWritable,Text,</span><br><span class="line">NullWritable&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context</span><br><span class="line">    context) throws IOException, InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F; 防止有相同的数据,迭代写出</span><br><span class="line">        for (NullWritable value : values) &#123;</span><br><span class="line">        	context.write(key,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>自定义一个 LogOutputFormat 类  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.outputformat;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line">import org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">public class LogOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;</span><br><span class="line">&#123;</span><br><span class="line">    @Override</span><br><span class="line">    public RecordWriter&lt;Text, NullWritable&gt;</span><br><span class="line">    getRecordWriter(TaskAttemptContext job) throws IOException,</span><br><span class="line">    InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F;创建一个自定义的 RecordWriter 返回</span><br><span class="line">        LogRecordWriter logRecordWriter &#x3D; new LogRecordWriter(job);</span><br><span class="line">        return logRecordWriter;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写 LogRecordWriter 类  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.outputformat;</span><br><span class="line">import org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IOUtils;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line">import org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">public class LogRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123;</span><br><span class="line">    private FSDataOutputStream atguiguOut;</span><br><span class="line">    private FSDataOutputStream otherOut;</span><br><span class="line">    public LogRecordWriter(TaskAttemptContext job) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            &#x2F;&#x2F;获取文件系统对象</span><br><span class="line">            FileSystem fs &#x3D; FileSystem.get(job.getConfiguration());</span><br><span class="line">            &#x2F;&#x2F;用文件系统对象创建两个输出流对应不同的目录</span><br><span class="line">            atguiguOut &#x3D; fs.create(new Path(&quot;d:&#x2F;hadoop&#x2F;atguigu.log&quot;));</span><br><span class="line">            otherOut &#x3D; fs.create(new Path(&quot;d:&#x2F;hadoop&#x2F;other.log&quot;));</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">        	e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public void write(Text key, NullWritable value) throws IOException,</span><br><span class="line">    InterruptedException &#123;</span><br><span class="line">        String log &#x3D; key.toString();</span><br><span class="line">        &#x2F;&#x2F;根据一行的 log 数据是否包含 atguigu,判断两条输出流输出的内容</span><br><span class="line">        if (log.contains(&quot;atguigu&quot;)) &#123;</span><br><span class="line">        	atguiguOut.writeBytes(log + &quot;\n&quot;);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">        	otherOut.writeBytes(log + &quot;\n&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public void close(TaskAttemptContext context) throws IOException,</span><br><span class="line">    InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F;关流</span><br><span class="line">        IOUtils.closeStream(atguiguOut);</span><br><span class="line">        IOUtils.closeStream(otherOut);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写LogDriver类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.outputformat;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">public class LogDriver &#123;</span><br><span class="line">    public static void main(String[] args) throws IOException,</span><br><span class="line">    ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        Configuration conf &#x3D; new Configuration();</span><br><span class="line">        Job job &#x3D; Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(LogDriver.class);</span><br><span class="line">        job.setMapperClass(LogMapper.class);</span><br><span class="line">        job.setReducerClass(LogReducer.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        &#x2F;&#x2F;设置自定义的 outputformat</span><br><span class="line">        job.setOutputFormatClass(LogOutputFormat.class);</span><br><span class="line">        FileInputFormat.setInputPaths(job, new Path(&quot;D:\\input&quot;));</span><br><span class="line">        &#x2F;&#x2F;虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat</span><br><span class="line">        &#x2F;&#x2F;而 fileoutputformat 要输出一个_SUCCESS 文件，所以在这还得指定一个输出目录</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(&quot;D:\\logoutput&quot;));</span><br><span class="line">        boolean b &#x3D; job.waitForCompletion(true);</span><br><span class="line">        System.exit(b ? 0 : 1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h3 id="6-3-5-MapReduce-内核源码解析"><a href="#6-3-5-MapReduce-内核源码解析" class="headerlink" title="6.3.5 MapReduce 内核源码解析"></a>6.3.5 MapReduce 内核源码解析</h3><h4 id="6-3-5-1-MapTask工作机制"><a href="#6-3-5-1-MapTask工作机制" class="headerlink" title="6.3.5.1 MapTask工作机制"></a>6.3.5.1 MapTask工作机制</h4><p><img src="https://i.loli.net/2021/06/30/YfwoSEnGXbM57rh.png" alt="image-20210630173239696"></p>
<ol>
<li><p><strong>Read阶段：</strong>MapTask 通过 InputFormat 获得的 RecordReader， 从输入 <strong>InputSplit</strong> 中解析出一个个 key/value。  </p>
</li>
<li><p><strong>Map阶段：</strong>该阶段主要是将解析出的 key/value 交给用户编写 <strong>map()函数</strong>处理，并产生一系列新的 key/value。  </p>
</li>
<li><p><strong>Collect阶段：</strong>在用户编写 map()函数中，当数据处理完成后，一般会调用<strong>OutputCollector.collect()</strong>输出结果。在该函数内部，它会将生成的 key/value 分区（ 调用Partitioner） ， 并写入一个环形内存缓冲区中。</p>
</li>
<li><p><strong>Spill溢写阶段：</strong>当环形缓冲区满后， MapReduce 会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、 压缩等操作。  </p>
<p><strong>Spill溢写阶段详情：</strong></p>
<ol>
<li>利用<strong>快速排序</strong>算法对缓存区内的数据进行排序，排序方式是，<strong>先按照分区编号Partition 进行排序，然后按照 key 进行排序</strong>。这样， 经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照 key 有序。</li>
<li>按照分区编号由小到大<strong>依次将每个分区中</strong>的数据写入任务工作目录下的临时文件 output/spillN.out（N 表示当前溢写次数）中。<strong>如果用户设置了 Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</strong>    </li>
<li>将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过 1MB，则将内存索引写到文件 output/spillN.out.index 中。  </li>
</ol>
</li>
<li><p><strong>Merge阶段：</strong>当所有数据处理完成后， MapTask 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
</li>
</ol>
<ul>
<li><p>当所有数据处理完后， MapTask 会将所有临时文件合并成一个大文件， 并保存到文件output/file.out 中，同时生成相应的索引文件 output/file.out.index。  </p>
</li>
<li><p>在进行文件合并过程中， MapTask <strong>以分区为单位进行合并</strong>。对于某个分区， 它将采用多轮递归合并的方式。 每轮合并 mapreduce.task.io.sort.factor（默认 10） 个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。  </p>
</li>
<li><p>让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。  </p>
</li>
</ul>
<h4 id="6-3-5-2-ReduceTask工作机制"><a href="#6-3-5-2-ReduceTask工作机制" class="headerlink" title="6.3.5.2 ReduceTask工作机制"></a>6.3.5.2 ReduceTask工作机制</h4><p><img src="https://i.loli.net/2021/06/30/ZxvHW9ay4BnpAT8.png" alt="image-20210630195302161"></p>
<ol>
<li><strong>Copy阶段：</strong>ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 </li>
<li><strong>Sort阶段：</strong>在远程拷贝数据的同时， ReduceTask 启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照 MapReduce 语义，用户编写 reduce()函数输入数据是按 key 进行聚集的一组数据。 为了将 key 相同的数据聚在一起， Hadoop 采用了基于排序的策略。由于各个 MapTask 已经实现对自己的处理结果进行了局部排序，因此， ReduceTask 只需对所有数据进行一次归并排序即可。</li>
<li><strong>Reduce阶段：</strong>reduce()函数将计算结果写到 HDFS 上。  </li>
</ol>
<h4 id="6-3-5-3-ReduceTask并行度决定机制"><a href="#6-3-5-3-ReduceTask并行度决定机制" class="headerlink" title="6.3.5.3 ReduceTask并行度决定机制"></a>6.3.5.3 ReduceTask并行度决定机制</h4><p><strong>回顾：</strong> MapTask 并行度由切片个数决定，切片个数由输入文件和切片规则决定。  </p>
<p><strong>思考：</strong>ReduceTask 并行度由谁决定？  </p>
<ol>
<li><p>设置 ReduceTask 并行度（个数）  </p>
<p>ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同， ReduceTask 数量的决定是可以直接手动设置：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 默认值是 1，手动设置为 4</span><br><span class="line">job.setNumReduceTasks(4);</span><br></pre></td></tr></table></figure>
</li>
<li><p>实验：测试 ReduceTask 多少合适  </p>
<ul>
<li><p>实验环境：1 个 Master 节点， 16 个 Slave 节点： CPU:8GHZ，内存: 2G  </p>
</li>
<li><p>实验结论：  </p>
<p><img src="https://i.loli.net/2021/06/30/ErW1H5ycFMgwK7m.png" alt="image-20210630195500638"></p>
</li>
<li><p><strong>注意事项：</strong></p>
<p><img src="https://i.loli.net/2021/06/30/6SJ8GyU4udPnhor.png" alt="image-20210630195523269"></p>
</li>
</ul>
</li>
</ol>
<h4 id="6-3-5-4-MapTask-amp-ReduceTask源码解析"><a href="#6-3-5-4-MapTask-amp-ReduceTask源码解析" class="headerlink" title="6.3.5.4 MapTask&amp;ReduceTask源码解析"></a>6.3.5.4 MapTask&amp;ReduceTask源码解析</h4><ol>
<li><p><strong>MapTask源码解析流程</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; MapTask &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">context.write(k, NullWritable.get()); &#x2F;&#x2F;自定义的 map 方法的写出，进入</span><br><span class="line">    output.write(key, value);</span><br><span class="line">    	&#x2F;&#x2F;MapTask727 行，收集方法，进入两次</span><br><span class="line">		collector.collect(key, value,partitioner.getPartition(key, value, partitions));</span><br><span class="line">			HashPartitioner(); &#x2F;&#x2F;默认分区器</span><br><span class="line">		collect() &#x2F;&#x2F;MapTask1082 行 map 端所有的 kv 全部写出后会走下面的 close 方法</span><br><span class="line">			close() &#x2F;&#x2F;MapTask732 行</span><br><span class="line">				collector.flush() &#x2F;&#x2F; 溢出刷写方法， MapTask735 行， 提前打个断点，进入</span><br><span class="line">					sortAndSpill() &#x2F;&#x2F;溢写排序， MapTask1505 行，进入</span><br><span class="line">						sorter.sort() QuickSort &#x2F;&#x2F;溢写排序方法， MapTask1625 行，进入</span><br><span class="line">					mergeParts(); &#x2F;&#x2F;合并文件， MapTask1527 行，进入，同时生成file.out和file.out.index</span><br><span class="line">				collector.close(); &#x2F;&#x2F;MapTask739 行,收集器关闭,即将进入 ReduceTask</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>ReduceTask源码解析流程</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; ReduceTask &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">if (isMapOrReduce()) &#x2F;&#x2F;reduceTask324 行， 提前打断点</span><br><span class="line">initialize() &#x2F;&#x2F; reduceTask333 行,进入</span><br><span class="line">init(shuffleContext); &#x2F;&#x2F; reduceTask375 行,走到这需要先给下面的打断点</span><br><span class="line">    totalMaps &#x3D; job.getNumMapTasks(); &#x2F;&#x2F; ShuffleSchedulerImpl 第 120 行，提前打断点</span><br><span class="line">    merger &#x3D; createMergeManager(context); &#x2F;&#x2F;合并方法， Shuffle 第 80 行</span><br><span class="line">        &#x2F;&#x2F; MergeManagerImpl 第 232 235 行，提前打断点</span><br><span class="line">        this.inMemoryMerger &#x3D; createInMemoryMerger(); &#x2F;&#x2F;内存合并</span><br><span class="line">        this.onDiskMerger &#x3D; new OnDiskMerger(this); &#x2F;&#x2F;磁盘合并</span><br><span class="line">rIter &#x3D; shuffleConsumerPlugin.run();</span><br><span class="line">    eventFetcher.start(); &#x2F;&#x2F;开始抓取数据， Shuffle 第 107 行，提前打断点</span><br><span class="line">    eventFetcher.shutDown(); &#x2F;&#x2F;抓取结束， Shuffle 第 141 行，提前打断点</span><br><span class="line">    copyPhase.complete(); &#x2F;&#x2F;copy 阶段完成， Shuffle 第 151 行</span><br><span class="line">    taskStatus.setPhase(TaskStatus.Phase.SORT); &#x2F;&#x2F;开始排序阶段， Shuffle 第 152 行</span><br><span class="line">sortPhase.complete(); &#x2F;&#x2F;排序阶段完成，即将进入 reduce 阶段 reduceTask382 行</span><br><span class="line">reduce(); &#x2F;&#x2F;reduce 阶段调用的就是我们自定义的 reduce 方法，会被调用多次</span><br><span class="line">	cleanup(context); &#x2F;&#x2F;reduce 完成之前，会最后调用一次 Reducer 里面的 cleanup 方法</span><br></pre></td></tr></table></figure>

<p><strong>注意：reduce()方法中往外写key和value时会调用其得toString()方法</strong></p>
</li>
</ol>
<h3 id="6-3-6-Join应用"><a href="#6-3-6-Join应用" class="headerlink" title="6.3.6 Join应用"></a>6.3.6 Join应用</h3><h4 id="6-3-6-1-Reduce-Join"><a href="#6-3-6-1-Reduce-Join" class="headerlink" title="6.3.6.1 Reduce Join"></a>6.3.6.1 Reduce Join</h4><ul>
<li>Map 端的主要工作：为来自不同表或文件的 key/value 对，<strong>打标签以区别不同来源的记录</strong>。然后<strong>用连接字段作为 key</strong>，其余部分和新加的标志作为 value，最后进行输出。  </li>
<li>Reduce 端的主要工作：在 Reduce 端<strong>以连接字段作为 key 的分组已经完成</strong>，我们只需要在每一个分组当中将那些来源于不同文件的记录（在 Map 阶段已经打标志） 分开，最后进行合并就 ok 了。  </li>
</ul>
<h4 id="6-3-6-2-Reduce-Join案例实操"><a href="#6-3-6-2-Reduce-Join案例实操" class="headerlink" title="6.3.6.2 Reduce Join案例实操"></a>6.3.6.2 Reduce Join案例实操</h4><ol>
<li><p>需求</p>
<p><img src="https://i.loli.net/2021/07/01/NqWIbn27sCwOPa9.png" alt="image-20210701165753730"></p>
<p><img src="https://i.loli.net/2021/07/01/hr4QMD572wYC8AE.png" alt="image-20210701165818113"></p>
<p>将商品信息表中数据根据商品 pid 合并到订单数据表中。</p>
<p><img src="https://i.loli.net/2021/07/01/sPrkOhI9Kg7bdoQ.png" alt="image-20210701165846172"></p>
</li>
<li><p>需求分析</p>
<p>通过将关联条件作为 Map 输出的 key，将两表满足 Join 条件的数据并携带数据所来源的文件信息，发往同一个 ReduceTask，在 Reduce 中进行数据的串联。</p>
<p><img src="https://i.loli.net/2021/07/01/KoTGBhXlREsk7Pq.png" alt="image-20210701180937491"></p>
</li>
<li><p>代码实现</p>
<ul>
<li><p>创建商品和订单合并后的 TableBean 类  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.reducejoin;</span><br><span class="line">import org.apache.hadoop.io.Writable;</span><br><span class="line">import java.io.DataInput;</span><br><span class="line">import java.io.DataOutput;</span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class TableBean implements Writable &#123;</span><br><span class="line">    private String id; &#x2F;&#x2F;订单 id</span><br><span class="line">    private String pid; &#x2F;&#x2F;产品 id</span><br><span class="line">    private int amount; &#x2F;&#x2F;产品数量</span><br><span class="line">    private String pname; &#x2F;&#x2F;产品名称</span><br><span class="line">    private String flag; &#x2F;&#x2F;判断是 order 表还是 pd 表的标志字段</span><br><span class="line">    </span><br><span class="line">    public TableBean() &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public String getId() &#123;</span><br><span class="line">    	return id;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public void setId(String id) &#123;</span><br><span class="line">    	this.id &#x3D; id;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public String getPid() &#123;</span><br><span class="line">    	return pid;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public void setPid(String pid) &#123;</span><br><span class="line">    	this.pid &#x3D; pid;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public int getAmount() &#123;</span><br><span class="line">    	return amount;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public void setAmount(int amount) &#123;</span><br><span class="line">    	this.amount &#x3D; amount;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public String getPname() &#123;</span><br><span class="line">    	return pname;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public void setPname(String pname) &#123;</span><br><span class="line">    	this.pname &#x3D; pname;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public String getFlag() &#123;</span><br><span class="line">    	return flag;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public void setFlag(String flag) &#123;</span><br><span class="line">    	this.flag &#x3D; flag;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">    	return id + &quot;\t&quot; + pname + &quot;\t&quot; + amount;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public void write(DataOutput out) throws IOException &#123;</span><br><span class="line">        out.writeUTF(id);</span><br><span class="line">        out.writeUTF(pid);</span><br><span class="line">        out.writeInt(amount);</span><br><span class="line">        out.writeUTF(pname);</span><br><span class="line">        out.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">        this.id &#x3D; in.readUTF();</span><br><span class="line">        this.pid &#x3D; in.readUTF();</span><br><span class="line">        this.amount &#x3D; in.readInt();</span><br><span class="line">        this.pname &#x3D; in.readUTF();</span><br><span class="line">        this.flag &#x3D; in.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写TableMapper类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.reducejoin;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class TableMapper extends Mapper&lt;LongWritable,Text,Text,TableBean&gt;</span><br><span class="line">&#123;</span><br><span class="line">    private String filename;</span><br><span class="line">    private Text outK &#x3D; new Text();</span><br><span class="line">    private TableBean outV &#x3D; new TableBean();</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    protected void setup(Context context) throws IOException,</span><br><span class="line">    InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F;获取对应文件名称</span><br><span class="line">        InputSplit split &#x3D; context.getInputSplit();</span><br><span class="line">        FileSplit fileSplit &#x3D; (FileSplit) split;</span><br><span class="line">        filename &#x3D; fileSplit.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">    throws IOException, InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F;获取一行</span><br><span class="line">        String line &#x3D; value.toString();</span><br><span class="line">        &#x2F;&#x2F;判断是哪个文件,然后针对文件进行不同的操作</span><br><span class="line">        if(filename.contains(&quot;order&quot;))&#123; &#x2F;&#x2F;订单表的处理</span><br><span class="line">            String[] split &#x3D; line.split(&quot;\t&quot;);</span><br><span class="line">            &#x2F;&#x2F;封装 outK</span><br><span class="line">            outK.set(split[1]);</span><br><span class="line">            &#x2F;&#x2F;封装 outV</span><br><span class="line">            outV.setId(split[0]);</span><br><span class="line">            outV.setPid(split[1]);</span><br><span class="line">            outV.setAmount(Integer.parseInt(split[2]));</span><br><span class="line">            outV.setPname(&quot;&quot;);</span><br><span class="line">            outV.setFlag(&quot;order&quot;);</span><br><span class="line">        &#125;else &#123; &#x2F;&#x2F;商品表的处理</span><br><span class="line">            String[] split &#x3D; line.split(&quot;\t&quot;);</span><br><span class="line">            &#x2F;&#x2F;封装 outK</span><br><span class="line">            outK.set(split[0]);</span><br><span class="line">            &#x2F;&#x2F;封装 outV</span><br><span class="line">            outV.setId(&quot;&quot;);</span><br><span class="line">            outV.setPid(split[0]);</span><br><span class="line">            outV.setAmount(0);</span><br><span class="line">            outV.setPname(split[1]);</span><br><span class="line">            outV.setFlag(&quot;pd&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F;写出 KV</span><br><span class="line">        context.write(outK,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写TableReducer类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.reducejoin;</span><br><span class="line">import org.apache.commons.beanutils.BeanUtils;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.lang.reflect.InvocationTargetException;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line"></span><br><span class="line">public class TableReducer extends Reducer&lt;Text,TableBean,TableBean,</span><br><span class="line">NullWritable&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context</span><br><span class="line">    context) throws IOException, InterruptedException &#123;</span><br><span class="line">        ArrayList&lt;TableBean&gt; orderBeans &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line">        TableBean pdBean &#x3D; new TableBean();</span><br><span class="line">        for (TableBean value : values) &#123;</span><br><span class="line">            &#x2F;&#x2F;判断数据来自哪个表</span><br><span class="line">            if(&quot;order&quot;.equals(value.getFlag()))&#123; &#x2F;&#x2F;订单表</span><br><span class="line">                &#x2F;&#x2F;创建一个临时 TableBean 对象接收 value</span><br><span class="line">                TableBean tmpOrderBean &#x3D; new TableBean();</span><br><span class="line">                try &#123;</span><br><span class="line">                	BeanUtils.copyProperties(tmpOrderBean,value);</span><br><span class="line">                &#125; catch (IllegalAccessException e) &#123;</span><br><span class="line">                	e.printStackTrace();</span><br><span class="line">                &#125; catch (InvocationTargetException e) &#123;</span><br><span class="line">                	e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                &#x2F;&#x2F;将临时 TableBean 对象添加到集合 orderBeans</span><br><span class="line">                orderBeans.add(tmpOrderBean);</span><br><span class="line">            &#125;else &#123; &#x2F;&#x2F;商品表</span><br><span class="line">                try &#123;</span><br><span class="line">                	BeanUtils.copyProperties(pdBean,value);</span><br><span class="line">                &#125; catch (IllegalAccessException e) &#123;</span><br><span class="line">                	e.printStackTrace();</span><br><span class="line">                &#125; catch (InvocationTargetException e) &#123;</span><br><span class="line">                	e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F;遍历集合 orderBeans,替换掉每个 orderBean 的 pid 为 pname,然后写出</span><br><span class="line">        for (TableBean orderBean : orderBeans) &#123;</span><br><span class="line">            orderBean.setPname(pdBean.getPname());</span><br><span class="line">            &#x2F;&#x2F;写出修改后的 orderBean 对象</span><br><span class="line">            context.write(orderBean,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写TableDriver类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.reducejoin;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class TableDriver &#123;</span><br><span class="line">    public static void main(String[] args) throws IOException,</span><br><span class="line">    ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        Job job &#x3D; Job.getInstance(new Configuration());</span><br><span class="line">        job.setJarByClass(TableDriver.class);</span><br><span class="line">        job.setMapperClass(TableMapper.class);</span><br><span class="line">        job.setReducerClass(TableReducer.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(TableBean.class);</span><br><span class="line">        job.setOutputKeyClass(TableBean.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        FileInputFormat.setInputPaths(job, new Path(&quot;D:\\input&quot;));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(&quot;D:\\output&quot;));</span><br><span class="line">        boolean b &#x3D; job.waitForCompletion(true);</span><br><span class="line">        System.exit(b ? 0 : 1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<p>运行程序查看结果  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1004 小米 4</span><br><span class="line">1001 小米 1</span><br><span class="line">1005 华为 5</span><br><span class="line">1002 华为 2</span><br><span class="line">1006 格力 6</span><br><span class="line">1003 格力 3</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>总结</strong></p>
<p><strong>缺点：</strong>这种方式中，合并的操作是在 Reduce 阶段完成， <strong>Reduce 端的处理压力太大</strong>， Map节点的运算负载则很低，资源利用率不高，且在 Reduce 阶段极易产生数据倾斜。  </p>
<p><strong>解决方案：Map端实现数据合并。</strong></p>
</li>
</ul>
</li>
</ol>
<h4 id="6-3-6-3-Map-Join"><a href="#6-3-6-3-Map-Join" class="headerlink" title="6.3.6.3 Map Join"></a>6.3.6.3 Map Join</h4><ol>
<li><p>使用场景</p>
<p>Map Join 适用于<strong>一张表十分小、一张表很大</strong>的场景。  </p>
</li>
<li><p>优点</p>
<p>思考： 在 Reduce 端处理过多的表， 非常容易产生数据倾斜。 怎么办？<br>在 Map 端缓存多张表，提前处理业务逻辑，这样增加 Map 端业务，减少 Reduce 端数据的压力，尽可能的减少数据倾斜。  </p>
</li>
<li><p><strong>具体办法：采用DistributedCache</strong></p>
<ol>
<li><p>在Mapper得setup()方法中，将文件读取到缓存集合中；</p>
</li>
<li><p>在Driver驱动类中记载缓存。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;缓存普通文件到 Task 运行节点。</span><br><span class="line">job.addCacheFile(new URI(&quot;file:&#x2F;&#x2F;&#x2F;e:&#x2F;cache&#x2F;pd.txt&quot;));</span><br><span class="line">&#x2F;&#x2F;如果是集群运行,需要设置 HDFS 路径</span><br><span class="line">job.addCacheFile(new URI(&quot;hdfs:&#x2F;&#x2F;hadoop102:8020&#x2F;cache&#x2F;pd.txt&quot;));</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h4 id="6-3-6-4-Map-Join案例实操"><a href="#6-3-6-4-Map-Join案例实操" class="headerlink" title="6.3.6.4 Map Join案例实操"></a>6.3.6.4 Map Join案例实操</h4><ol>
<li><p>需求分析</p>
<p><img src="https://i.loli.net/2021/07/01/WRtSHjMKh9uDBx1.png" alt="image-20210701212111781"></p>
</li>
<li><p>实现代码</p>
<ol>
<li><p>先在 MapJoinDriver 驱动类中添加缓存文件  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.mapjoin;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.net.URI;</span><br><span class="line">import java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line">public class MapJoinDriver &#123;</span><br><span class="line">    public static void main(String[] args) throws IOException,</span><br><span class="line">    URISyntaxException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F; 1 获取 job 信息</span><br><span class="line">        Configuration conf &#x3D; new Configuration();</span><br><span class="line">        Job job &#x3D; Job.getInstance(conf);</span><br><span class="line">        &#x2F;&#x2F; 2 设置加载 jar 包路径</span><br><span class="line">        job.setJarByClass(MapJoinDriver.class);</span><br><span class="line">        &#x2F;&#x2F; 3 关联 mapper</span><br><span class="line">        job.setMapperClass(MapJoinMapper.class);</span><br><span class="line">        &#x2F;&#x2F; 4 设置 Map 输出 KV 类型</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">        &#x2F;&#x2F; 5 设置最终输出 KV 类型</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        &#x2F;&#x2F; 加载缓存数据</span><br><span class="line">        job.addCacheFile(new URI(&quot;file:&#x2F;&#x2F;&#x2F;D:&#x2F;input&#x2F;tablecache&#x2F;pd.txt&quot;));</span><br><span class="line">        &#x2F;&#x2F; Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0</span><br><span class="line">        job.setNumReduceTasks(0);</span><br><span class="line">        &#x2F;&#x2F; 6 设置输入输出路径</span><br><span class="line">        FileInputFormat.setInputPaths(job, new Path(&quot;D:\\input&quot;));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(&quot;D:\\output&quot;));</span><br><span class="line">        &#x2F;&#x2F; 7 提交</span><br><span class="line">        boolean b &#x3D; job.waitForCompletion(true);</span><br><span class="line">        System.exit(b ? 0 : 1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 MapJoinMapper 类中的 setup 方法中读取缓存文件  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.mapjoin;</span><br><span class="line">import org.apache.commons.lang.StringUtils;</span><br><span class="line">import org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IOUtils;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import java.io.BufferedReader;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.io.InputStreamReader;</span><br><span class="line">import java.net.URI;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line">import java.util.Map;</span><br><span class="line"></span><br><span class="line">public class MapJoinMapper extends Mapper&lt;LongWritable, Text, Text,</span><br><span class="line">NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private Map&lt;String, String&gt; pdMap &#x3D; new HashMap&lt;&gt;();</span><br><span class="line">    </span><br><span class="line">    private Text text &#x3D; new Text();</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F;任务开始前将 pd 数据缓存进 pdMap</span><br><span class="line">    @Override</span><br><span class="line">    protected void setup(Context context) throws IOException,</span><br><span class="line">    InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F;通过缓存文件得到小表数据 pd.txt</span><br><span class="line">        URI[] cacheFiles &#x3D; context.getCacheFiles();</span><br><span class="line">        Path path &#x3D; new Path(cacheFiles[0]);</span><br><span class="line">        &#x2F;&#x2F;获取文件系统对象,并开流</span><br><span class="line">        FileSystem fs &#x3D; FileSystem.get(context.getConfiguration());</span><br><span class="line">        FSDataInputStream fis &#x3D; fs.open(path);</span><br><span class="line">        &#x2F;&#x2F;通过包装流转换为 reader,方便按行读取</span><br><span class="line">        BufferedReader reader &#x3D; new BufferedReader(new</span><br><span class="line">        InputStreamReader(fis, &quot;UTF-8&quot;));</span><br><span class="line">        &#x2F;&#x2F;逐行读取，按行处理</span><br><span class="line">        String line;</span><br><span class="line">        while (StringUtils.isNotEmpty(line &#x3D; reader.readLine())) &#123;</span><br><span class="line">            &#x2F;&#x2F;切割一行</span><br><span class="line">            &#x2F;&#x2F;01 小米</span><br><span class="line">            String[] split &#x3D; line.split(&quot;\t&quot;);</span><br><span class="line">            pdMap.put(split[0], split[1]);</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F;关流</span><br><span class="line">        IOUtils.closeStream(reader);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">    throws IOException, InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F;读取大表数据</span><br><span class="line">        &#x2F;&#x2F;1001 01 1</span><br><span class="line">        String[] fields &#x3D; value.toString().split(&quot;\t&quot;);</span><br><span class="line">        &#x2F;&#x2F;通过大表每行数据的 pid,去 pdMap 里面取出 pname</span><br><span class="line">        String pname &#x3D; pdMap.get(fields[1]);</span><br><span class="line">        &#x2F;&#x2F;将大表每行数据的 pid 替换为 pname</span><br><span class="line">        text.set(fields[0] + &quot;\t&quot; + pname + &quot;\t&quot; + fields[2]);</span><br><span class="line">        &#x2F;&#x2F;写出</span><br><span class="line">        context.write(text,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h3 id="6-3-7-数据清洗-ETL"><a href="#6-3-7-数据清洗-ETL" class="headerlink" title="6.3.7 数据清洗(ETL)"></a>6.3.7 数据清洗(ETL)</h3><p>“ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据<strong>从来源端经过抽取（Extract）、转换（Transform）、加载（Load）至目的端</strong>的过程。 ETL 一词较常用在数据仓库，但其对象并不限于数据仓库  </p>
<p>在运行核心业务 MapReduce 程序之前，往往要先对数据进行清洗，<strong>清理掉不符合用户要求的数据</strong>。 <strong>清理的过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序</strong>。  </p>
<ol>
<li><p>需求</p>
<p>去除日志中字段个数小于等于 11 的日志。  </p>
<ol>
<li><p>输入数据</p>
<p>web.log</p>
</li>
<li><p>期望输出数据</p>
<p>每行字段长度都大于 11。  </p>
</li>
</ol>
</li>
<li><p>需求分析</p>
<p>需要在 Map 阶段对输入的数据根据规则进行过滤清洗。  </p>
</li>
<li><p>实现代码</p>
<ol>
<li><p>编写WebLogMapper类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.weblog;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line">public class WebLogMapper extends Mapper&lt;LongWritable, Text, Text,</span><br><span class="line">NullWritable&gt;&#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">    throws IOException, InterruptedException &#123;</span><br><span class="line">        &#x2F;&#x2F; 1 获取 1 行数据</span><br><span class="line">        String line &#x3D; value.toString();</span><br><span class="line">        &#x2F;&#x2F; 2 解析日志</span><br><span class="line">        boolean result &#x3D; parseLog(line,context);</span><br><span class="line">        &#x2F;&#x2F; 3 日志不合法退出</span><br><span class="line">        if (!result) &#123;</span><br><span class="line">        return;</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F; 4 日志合法就直接写出</span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">    	&#x2F;&#x2F; 2 封装解析日志的方法</span><br><span class="line">    private boolean parseLog(String line, Context context) &#123;</span><br><span class="line">        &#x2F;&#x2F; 1 截取</span><br><span class="line">        String[] fields &#x3D; line.split(&quot; &quot;);</span><br><span class="line">        &#x2F;&#x2F; 2 日志长度大于 11 的为合法</span><br><span class="line">        if (fields.length &gt; 11) &#123;</span><br><span class="line">        return true;</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">        return false;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写WebLogDriver类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.weblog;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">public class WebLogDriver &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span><br><span class="line">        args &#x3D; new String[] &#123; &quot;D:&#x2F;input&#x2F;inputlog&quot;, &quot;D:&#x2F;output1&quot; &#125;;</span><br><span class="line">        &#x2F;&#x2F; 1 获取 job 信息</span><br><span class="line">        Configuration conf &#x3D; new Configuration();</span><br><span class="line">        Job job &#x3D; Job.getInstance(conf);</span><br><span class="line">        &#x2F;&#x2F; 2 加载 jar 包</span><br><span class="line">        job.setJarByClass(LogDriver.class);</span><br><span class="line">        &#x2F;&#x2F; 3 关联 map</span><br><span class="line">        job.setMapperClass(WebLogMapper.class);</span><br><span class="line">        &#x2F;&#x2F; 4 设置最终输出类型</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        &#x2F;&#x2F; 设置 reducetask 个数为 0</span><br><span class="line">        job.setNumReduceTasks(0);</span><br><span class="line">        &#x2F;&#x2F; 5 设置输入和输出路径</span><br><span class="line">        FileInputFormat.setInputPaths(job, new Path(args[0]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(args[1]));</span><br><span class="line">        &#x2F;&#x2F; 6 提交</span><br><span class="line">        boolean b &#x3D; job.waitForCompletion(true);</span><br><span class="line">        System.exit(b ? 0 : 1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h3 id="6-3-8-MapReduce开发总结"><a href="#6-3-8-MapReduce开发总结" class="headerlink" title="6.3.8 MapReduce开发总结"></a>6.3.8 MapReduce开发总结</h3><ol>
<li><p><strong>输入数据接口：InputFormat</strong></p>
<ul>
<li>默认使用的实现类是： <strong>TextInputFormat</strong>  </li>
<li>TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为 value 返回。 </li>
<li>CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。  </li>
</ul>
</li>
<li><p><strong>逻辑处理接口：Mapper</strong></p>
<ul>
<li>用户根据业务需求实现其中三个方法： map() setup() cleanup ()  </li>
</ul>
</li>
<li><p><strong>Partitioner分区</strong></p>
<ul>
<li>有默认实现 <strong>HashPartitioner</strong>，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号； <strong>key.hashCode()&amp;Integer.MAXVALUE % numReduces</strong>  </li>
<li>如果业务上有特别的需求，可以自定义分区。  </li>
</ul>
</li>
<li><p><strong>Comparable排序</strong></p>
<ul>
<li>当我们用自定义的对象作为 key 来输出时，就必须要实现 <strong>WritableComparable 接口</strong>，重写其中的 compareTo()方法。</li>
<li>部分排序：对最终输出的每一个文件进行内部排序。  </li>
<li>全排序：对所有数据进行排序，通常只有一个 Reduce。  </li>
<li>二次排序：排序的条件有两个。  </li>
</ul>
</li>
<li><p><strong>Combiner合并</strong></p>
<ul>
<li>Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的业务处理结果。  </li>
</ul>
</li>
<li><p><strong>逻辑处理接口：Reducer</strong></p>
<ul>
<li>用户根据业务需求实现其中三个方法： reduce() setup() cleanup ()  </li>
</ul>
</li>
<li><p><strong>输出数据接口：OutputFormat</strong></p>
<ul>
<li>默认实现类是 <strong>TextOutputFormat</strong>，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。  </li>
<li>用户还可以自定义 OutputFormat。  </li>
</ul>
</li>
</ol>
<h2 id="6-4-Hadoop数据压缩"><a href="#6-4-Hadoop数据压缩" class="headerlink" title="6.4 Hadoop数据压缩"></a>6.4 Hadoop数据压缩</h2><h3 id="6-4-1-概述"><a href="#6-4-1-概述" class="headerlink" title="6.4.1 概述"></a>6.4.1 概述</h3><ol>
<li>压缩的好处和坏处<ul>
<li>*<em>优点：以减少磁盘 IO、减少磁盘存储空间。  *</em></li>
<li>*<em>缺点：增加 CPU 开销。  *</em></li>
</ul>
</li>
<li>压缩原则<ul>
<li>运算密集型的 Job，少用压缩  </li>
<li>IO 密集型的 Job，多用压缩  </li>
</ul>
</li>
</ol>
<h3 id="6-4-2-MR支持的压缩编码"><a href="#6-4-2-MR支持的压缩编码" class="headerlink" title="6.4.2 MR支持的压缩编码"></a>6.4.2 MR支持的压缩编码</h3><ol>
<li><p>压缩算法比较介绍</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>Hadoop 自带？</th>
<th>算法</th>
<th>文件扩展 名</th>
<th>是否可 切片</th>
<th>换成压缩格式后，原来的 程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要 修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要 修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td><strong>是</strong></td>
<td>和文本处理一样，不需要 修改</td>
</tr>
<tr>
<td>LZO</td>
<td><strong>否，需要安装</strong></td>
<td>LZO</td>
<td>.lzo</td>
<td><strong>是</strong></td>
<td><strong>需要建索引，还需要指定 输入格式</strong></td>
</tr>
<tr>
<td>Snappy</td>
<td>是，直接使用</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要 修改</td>
</tr>
</tbody></table>
</li>
<li><p>压缩性能的比较</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
<blockquote>
<p><a href="http://google.github.io/snappy/" target="_blank" rel="noopener">http://google.github.io/snappy/</a><br><strong>Snappy</strong> is a compression/decompression library. <strong>It does not aim for maximum compression</strong>, or<br>compatibility with any other compression library; instead, it <strong>aims for very high speeds</strong> and<br>reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of<br>magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100%<br>bigger.On a single core of a Core i7 processor in 64-bit mode, Snappy <strong>compresses at about 250</strong><br><strong>MB/sec</strong> or more and <strong>decompresses at about 500 MB/sec</strong> or more.  </p>
</blockquote>
</li>
</ol>
<h3 id="6-4-3-压缩方式选择"><a href="#6-4-3-压缩方式选择" class="headerlink" title="6.4.3 压缩方式选择"></a>6.4.3 压缩方式选择</h3><p>压缩方式选择时重点考虑： <strong>压缩/解压缩速度</strong>、<strong>压缩率（压缩后存储大小）</strong>、压缩后<strong>是否可以支持切片</strong>。  </p>
<ol>
<li><strong>Gzip压缩</strong><ul>
<li>*<em>优点：压缩率比较高；  *</em></li>
<li>*<em>缺点：不支持 Split； 压缩/解压速度一般；  *</em></li>
</ul>
</li>
<li><strong>Bzip2压缩</strong><ul>
<li>*<em>优点：压缩率高； 支持 Split；  *</em></li>
<li><strong>缺点：压缩/解压速度慢。</strong>  </li>
</ul>
</li>
<li><strong>Lzo压缩</strong><ul>
<li>*<em>优点：压缩/解压速度比较快；支持 Split；  *</em></li>
<li><strong>缺点： 压缩率一般； 想支持切片需要额外创建索引。</strong></li>
</ul>
</li>
<li><strong>Snappy压缩</strong><ul>
<li>*<em>优点：压缩和解压缩速度快；  *</em></li>
<li>*<em>缺点：不支持 Split；压缩率一般；  *</em></li>
</ul>
</li>
</ol>
<p><strong>压缩位置选择：</strong></p>
<p>压缩可以在 MapReduce 作用的任意阶段启用。  </p>
<p><img src="https://i.loli.net/2021/07/01/dDaw9kp8cYIqbxy.png" alt="image-20210701221525865"></p>
<h3 id="6-4-4-压缩参数设置"><a href="#6-4-4-压缩参数设置" class="headerlink" title="6.4.4 压缩参数设置"></a>6.4.4 压缩参数设置</h3><ol>
<li><p>为了支持多种压缩/解压缩算法， Hadoop 引入了<strong>编码/解码器</strong></p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
</li>
<li><p>要在 Hadoop 中启用压缩，可以配置如下参数  </p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs （在 core-site.xml 中配置）</td>
<td>无，这个需要在命令行输入 hadoop checknative 查看</td>
<td>输入压缩</td>
<td>Hadoop 使用文件扩展 名判断是否支持某种 编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress（在 mapred-site.xml 中 配置）</td>
<td>false</td>
<td>mapper 输出</td>
<td>这个参数设为 true 启 用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec（在 mapred site.xml 中配置）</td>
<td>org.apache.hadoop.io.com press.DefaultCodec</td>
<td>mapper 输出</td>
<td>企业多使用 LZO 或 Snappy 编解码器在此 阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutpu tformat.compress（在 mapred-site.xml 中配置）</td>
<td>false</td>
<td>reducer 输出</td>
<td>这个参数设为 true 启 用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutpu tformat.compress.codec（在 mapred-site.xml 中配置）</td>
<td>org.apache.hadoop.io.com press.DefaultCodec</td>
<td>reducer 输出</td>
<td>使用标准工具或者编 解码器，如 gzip 和 bzip2</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="6-4-5-压缩案例实操"><a href="#6-4-5-压缩案例实操" class="headerlink" title="6.4.5 压缩案例实操"></a>6.4.5 压缩案例实操</h3><h4 id="6-4-5-1-Map输出端采用压缩"><a href="#6-4-5-1-Map输出端采用压缩" class="headerlink" title="6.4.5.1 Map输出端采用压缩"></a>6.4.5.1 Map输出端采用压缩</h4><p>即使你的 MapReduce 的输入输出文件都是未压缩的文件，你仍然可以对 Map 任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到 Reduce 节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置。  </p>
<ol>
<li><p>给大家提供的 Hadoop 源码支持的压缩格式有： <strong>BZip2Codec、 DefaultCodec</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.compress;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line">import org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line">import org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">public class WordCountDriver &#123;</span><br><span class="line">    public static void main(String[] args) throws IOException,</span><br><span class="line">    ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        Configuration conf &#x3D; new Configuration();</span><br><span class="line">        &#x2F;&#x2F; 开启 map 端输出压缩</span><br><span class="line">        conf.setBoolean(&quot;mapreduce.map.output.compress&quot;, true);</span><br><span class="line">        &#x2F;&#x2F; 设置 map 端输出压缩方式</span><br><span class="line">        conf.setClass(&quot;mapreduce.map.output.compress.codec&quot;,</span><br><span class="line">        BZip2Codec.class,CompressionCodec.class);</span><br><span class="line">        </span><br><span class="line">        Job job &#x3D; Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        FileInputFormat.setInputPaths(job, new Path(args[0]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(args[1]));</span><br><span class="line">        boolean result &#x3D; job.waitForCompletion(true);</span><br><span class="line">        System.exit(result ? 0 : 1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Mapper和Reducer保持不变</p>
</li>
</ol>
<h4 id="6-4-5-2-Reduce输出端采用压缩"><a href="#6-4-5-2-Reduce输出端采用压缩" class="headerlink" title="6.4.5.2 Reduce输出端采用压缩"></a>6.4.5.2 Reduce输出端采用压缩</h4><p>基于 WordCount 案例处理。  </p>
<ol>
<li><p>修改驱动</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.compress;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line">import org.apache.hadoop.io.compress.DefaultCodec;</span><br><span class="line">import org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line">import org.apache.hadoop.io.compress.Lz4Codec;</span><br><span class="line">import org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">public class WordCountDriver &#123;</span><br><span class="line">    public static void main(String[] args) throws IOException,</span><br><span class="line">    ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        Configuration conf &#x3D; new Configuration();</span><br><span class="line">        Job job &#x3D; Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        FileInputFormat.setInputPaths(job, new Path(args[0]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(args[1]));</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F; 设置 reduce 端输出压缩开启</span><br><span class="line">        FileOutputFormat.setCompressOutput(job, true);</span><br><span class="line">        &#x2F;&#x2F; 设置压缩的方式</span><br><span class="line">        FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span><br><span class="line">        &#x2F;&#x2F; FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);</span><br><span class="line">        &#x2F;&#x2F; FileOutputFormat.setOutputCompressorClass(job,</span><br><span class="line">        DefaultCodec.class);</span><br><span class="line">        </span><br><span class="line">        boolean result &#x3D; job.waitForCompletion(true);</span><br><span class="line">        System.exit(result?0:1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="6-5-常见错误及解决方案"><a href="#6-5-常见错误及解决方案" class="headerlink" title="6.5 常见错误及解决方案"></a>6.5 常见错误及解决方案</h2><ol>
<li><p>导包容易出错。尤其 Text 和 CombineTextInputFormat。  </p>
</li>
<li><p>Mapper 中第一个输入的参数必须是 LongWritable 或者 NullWritable， 不可以是 IntWritable。报的错误是类型转换异常。  </p>
</li>
<li><p>java.lang.Exception: java.io.IOException: Illegal partition for 13926435656 (4)， 说明 Partition和 ReduceTask 个数没对上，调整 ReduceTask 个数。</p>
</li>
<li><p>如果分区数不是 1， 但是 reducetask 为 1， 是否执行分区过程。答案是：不执行分区过程。因为在 MapTask 的源码中，执行分区的前提是先判断 ReduceNum 个数是否大于 1。 不大于1 肯定不执行。 </p>
</li>
<li><p>在 Windows 环境编译的 jar 包导入到 Linux 环境中运行，<code>hadoop jar wc.jar com.atguigu.mapreduce.wordcount.WordCountDriver /user/atguigu/ /user/atguigu/output</code>   报如下错误：</p>
<p><code>Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError:
com/atguigu/mapreduce/wordcount/WordCountDriver : Unsupported major.minor version 52.0</code></p>
<p>原因是 Windows 环境用的 jdk1.7， Linux 环境用的 jdk1.8。<br><strong>解决方案： 统一 jdk 版本。</strong>  </p>
</li>
<li><p>缓存 pd.txt 小文件案例中，报找不到 pd.txt 文件<br>原因：大部分为路径书写错误。还有就是要检查 pd.txt.txt 的问题。 还有个别电脑写相对路径<br>找不到 pd.txt， 可以修改为绝对路径。  </p>
</li>
<li><p>报类型转换异常。<br>通常都是在驱动函数中设置 Map 输出和最终输出时编写错误。<br><strong>Map 输出的 key 如果没有排序， 也会报类型转换异常。</strong>  </p>
</li>
<li><p>集群中运行 wc.jar 时出现了无法获得输入文件。<br>原因： WordCount 案例的输入文件不能放用 HDFS 集群的根目录。  </p>
</li>
<li><p>自定义 Outputformat 时，注意在 RecordWirter 中的 <strong>close 方法必须关闭流资源</strong>。否则输出的文件内容中数据为空。  </p>
</li>
</ol>
<h1 id="7-YARN"><a href="#7-YARN" class="headerlink" title="7.YARN"></a>7.YARN</h1><h2 id="7-1-Yarn资源调度器"><a href="#7-1-Yarn资源调度器" class="headerlink" title="7.1 Yarn资源调度器"></a>7.1 Yarn资源调度器</h2><p><strong>Yarn</strong>是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统，而MapReduce等运算程序则相当于运行于 <strong>操作系统之上的应用程序</strong>。</p>
<h3 id="7-1-1-Yarn基础架构"><a href="#7-1-1-Yarn基础架构" class="headerlink" title="7.1.1 Yarn基础架构"></a>7.1.1 Yarn基础架构</h3><p>YARN 主要由 ResourceManager、 NodeManager、 ApplicationMaster 和 Container 等组件构成。          </p>
<p><img src="https://i.loli.net/2021/08/10/Rk6dOwYzUE4eobZ.png" alt="image-20210810231029645"></p>
<h3 id="7-1-2-Yarn工作机制"><a href="#7-1-2-Yarn工作机制" class="headerlink" title="7.1.2 Yarn工作机制"></a>7.1.2 Yarn工作机制</h3><p><img src="https://i.loli.net/2021/08/28/borOxzqcZnf86Vh.png" alt="image-20210828161252267"></p>
<ol>
<li>MR 程序提交到客户端所在的节点。  </li>
<li>YarnRunner 向 ResourceManager 申请一个 Application。</li>
<li>RM 将该应用程序的资源路径返回给 YarnRunner。</li>
<li>该程序将运行所需资源提交到 HDFS 上。</li>
<li>程序资源提交完毕后，申请运行 mrAppMaster。</li>
<li>RM 将用户的请求初始化成一个 Task。</li>
<li>其中一个 NodeManager 领取到 Task 任务。</li>
<li>该 NodeManager 创建容器 Container， 并产生 MRAppmaster。  </li>
<li>Container 从 HDFS 上拷贝资源到本地。</li>
<li>MRAppmaster 向 RM 申请运行 MapTask 资源。</li>
<li>RM 将运行 MapTask 任务分配给另外两个 NodeManager， 另两个 NodeManager 分别领取任务并创建容器。</li>
<li>MR 向两个接收到任务的 NodeManager 发送程序启动脚本， 这两个 NodeManager分别启动 MapTask， MapTask 对数据分区排序。</li>
<li>MrAppMaster 等待所有 MapTask 运行完毕后，向 RM 申请容器， 运行 ReduceTask。</li>
<li>ReduceTask 向 MapTask 获取相应分区的数据。</li>
<li>程序运行完毕后， MR 会向 RM 申请注销自己。  </li>
</ol>
<h3 id="7-1-3-Yarn调度器和调度算法"><a href="#7-1-3-Yarn调度器和调度算法" class="headerlink" title="7.1.3 Yarn调度器和调度算法"></a>7.1.3 Yarn调度器和调度算法</h3><p>目前， Hadoop 作业调度器主要有三种： <strong>FIFO</strong>、 <strong>容量</strong>（Capacity Scheduler） 和<strong>公平</strong>（FairScheduler） 。 Apache Hadoop3.1.3 默认的资源调度器是 Capacity Scheduler。<br>CDH 框架默认调度器是 Fair Scheduler。  </p>
<h4 id="7-1-3-1-FIFO"><a href="#7-1-3-1-FIFO" class="headerlink" title="7.1.3.1 FIFO"></a>7.1.3.1 FIFO</h4><p>FIFO 调度器（First In First Out）：单队列，根据提交作业的先后顺序，先来先服务。  </p>
<h4 id="7-1-3-2-容量调度器"><a href="#7-1-3-2-容量调度器" class="headerlink" title="7.1.3.2 容量调度器"></a>7.1.3.2 容量调度器</h4><p>Capacity Scheduler 是 Yahoo 开发的多用户调度器。  </p>
<p><img src="https://i.loli.net/2021/08/28/6c8WQjK1NxuYEG3.png" alt="image-20210828162303146"></p>
<p><img src="https://i.loli.net/2021/08/28/5cWiDqfnAemXu6B.png" alt="image-20210828162640660"></p>
<h4 id="7-1-3-3-公平调度器"><a href="#7-1-3-3-公平调度器" class="headerlink" title="7.1.3.3 公平调度器"></a>7.1.3.3 公平调度器</h4><p>Fair Schedulere 是 Facebook 开发的多用户调度器。  </p>
<p><img src="https://i.loli.net/2021/08/28/jXWucapIbNlTz4V.png" alt="image-20210828163011144"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://extrali.com/2020/07/08/%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="黎达">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Extrali">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/08/%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">面试总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>

              <time title="Erstellt: 2020-07-08 16:03:17" itemprop="dateCreated datePublished" datetime="2020-07-08T16:03:17+08:00">2020-07-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Bearbeitet am</span>
                <time title="Geändert am: 2020-07-09 14:54:13" itemprop="dateModified" datetime="2020-07-09T14:54:13+08:00">2020-07-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">in</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%9D%A2%E8%AF%95/" itemprop="url" rel="index"><span itemprop="name">面试</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="7-06字节客户端一面二面三面"><a href="#7-06字节客户端一面二面三面" class="headerlink" title="7.06字节客户端一面二面三面"></a>7.06字节客户端一面二面三面</h1><p>一面二面三面一起的，一二面录音了，三面忘了，算法题放一起</p>
<p><strong>算法题：</strong></p>
<ol>
<li>旋转数组的查找</li>
<li>两个线程轮流打印奇数偶数</li>
<li>有序数组查找一个元素第一次出现的位置</li>
<li>单例</li>
<li>域名反转(<a href="http://www.toutiao.com-&gt;com.toutiao.www" target="_blank" rel="noopener">www.toutiao.com-&gt;com.toutiao.www</a>)</li>
<li>三面算法题在下面</li>
</ol>
<h2 id="一面-41min"><a href="#一面-41min" class="headerlink" title="一面(41min)"></a>一面(41min)</h2><ol>
<li>自我介绍</li>
<li>问性格，怎么不做算法？？？</li>
<li>计算机基础咋学的？</li>
<li>HTTP有哪些方法？</li>
<li>get和post的区别？有说到get是幂等的，然后面试官问如果在两个get期间数据库发生改变了那结果还是一样吗？(不一样)。那怎么是幂等的。说我是不是在网上哪随便看到的(对对对，不敢反驳)</li>
<li>比如我要去换一个头像是用get还是post？</li>
<li>HTTP和HTTPS</li>
<li>SSL握手过程，然后问我第一次握手是什么加密，第二次握手怎么加密</li>
<li>对称加密的安全性</li>
<li>操作系统层面的线程同步</li>
<li>信号量和互斥量的区别</li>
<li>synchronized介绍</li>
<li>两个线程轮流打印奇数和偶数(我讲了通过信号量去实现，可能没讲太清楚，面试官就让我写一下，说用信号量写也可以，但我更希望你用锁去实现，然后用锁写的)。</li>
<li>wait()和sleep()的区别</li>
<li>hashcode场景，然后问了hashMap</li>
<li>数据库</li>
<li>事务</li>
<li>脏读，怎么解决脏读(怎么加锁)</li>
<li>类加载过程</li>
<li>ClassLoader的作用，双亲委派</li>
<li>算法怎么样(果然认怂，基础一点的还行)</li>
<li>反问</li>
</ol>
<h2 id="二面-35min"><a href="#二面-35min" class="headerlink" title="二面(35min)"></a>二面(35min)</h2><ol>
<li>问项目，项目细节，项目背景</li>
<li>ArrayList和LinkedList的区别</li>
<li>HashMap的put()流程，扩容</li>
<li>设计模式知道哪些，讲一下单例，写一下(双重检查)</li>
<li>volatile的作用</li>
<li>双重检查的用处，假如不要第一次if判断会有什么问题</li>
<li>夸我讲的好！问我咋学的这些</li>
<li>快速排序讲一哈，时间复杂度，怎么使得快排避免O(n^2^)复杂度</li>
<li>写题，两道，自己写例子测试测试</li>
</ol>
<h2 id="三面-记得多少写多少吧"><a href="#三面-记得多少写多少吧" class="headerlink" title="三面(记得多少写多少吧)"></a>三面(记得多少写多少吧)</h2><p>三面面试官口头禅：好的OK了解</p>
<ol>
<li>前面都是聊天，问团队合作，实验室情况，问了实验室的项目</li>
<li>然后画风突转就问技术问题了，输入网址访问的过程，然后一点一点详细问</li>
<li>DNS解析过程，DNS劫持了解吗，浏览器渲染页面的过程(???)，HTML和JS讲一下(???)</li>
<li>然后就是一道算法题</li>
</ol>
<p><img src="https://i.loli.net/2020/07/08/Rk36PjlhVm1D94q.png" alt="image-20200708165103773.png"></p>
<p>给定一个只有数字的字符串，输出一共有多少种可能的结果</p>
<p>一直想着在整个字符串上dp</p>
<p>面试官提示1：不同的字符之间有什么关系(感觉面试官前面的提示都太隐晦了，在这浪费了一些时间)</p>
<p>拆分之后我给出了一个dp递推式，但是不太对，有重复。</p>
<p>面试官提示2：怎么解决重复(这里又卡了一会)</p>
<p>最后给出了dp关系式，但是没给多少时间写，没写完，只能写个大致，给出dp递推式后主要问题就是怎么去解决越界的问题，是每次都判断还是多申请几个空间这样，当时应该想一想再写的，有点慌。</p>
<p><strong>最后面试官评价：</strong></p>
<ol>
<li>这题本身有难度，你有在和我一起思考</li>
<li>代码能力需要加强。</li>
</ol>
<p><strong>算法思想：</strong></p>
<ul>
<li><p>首先第一点，字符串拆分成相同字符的集合，例如”2222333477”拆分成“2222”，“333”，“4”，“77”，则最终结果为这四个字符串的可能的乘积。</p>
</li>
<li><p>然后就是对于只有一种字符的字符串怎么计算结果，先只考虑3个的情况，以’2‘为例，则最后的结尾字符只能是A，B或C，如果以A结尾，则字符串可以看成前i-1个和最后一个的组合；如果以B结尾，则可以看成前i-2和最后两个的组合；如果以C结尾，则可以看成前i-3和最后三个的组合：</p>
<p>例如：“22222222”，可能的情况如果以A结尾，则为”2222222“+A；</p>
<p>如果以B结尾，则为”222222“+B；如果以C结尾，则为”22222“+C</p>
<p>所以有递推关系式dp[i]=dp[i-1]+dp[i-2]+dp[i-3]</p>
<p>那么对于四种字符的情况就是dp[i-1]+dp[i-2]+dp[i-3]+dp[i-4]</p>
</li>
<li><p>实现上来看，各种字符之前没有区别，结果只和长度有关，但是7和9例外，所以只需要确定长度和mode即可确定结果，在实现上用了dp_3表示三种字符的递推结果，dp_4表示四种字符，同时为了避免重复计算，用i_3计算当前dp_3计算到哪了，i_4表示当前dp_4计算到哪了</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">private static int[] dp_3;</span><br><span class="line"></span><br><span class="line">   private static int i_3;</span><br><span class="line"></span><br><span class="line">   private static int[] dp_4;</span><br><span class="line"></span><br><span class="line">   private static int i_4;</span><br><span class="line"></span><br><span class="line">   private static int convert(String str)&#123;</span><br><span class="line">       dp_3&#x3D;new int[Math.max(str.length(),3)];</span><br><span class="line">       dp_4&#x3D;new int[Math.max(str.length(),4)];</span><br><span class="line">       &#x2F;&#x2F;前几位直接给出</span><br><span class="line">       dp_3[0]&#x3D;1;</span><br><span class="line">       dp_3[1]&#x3D;2;</span><br><span class="line">       dp_3[2]&#x3D;4;</span><br><span class="line">       i_3&#x3D;3;</span><br><span class="line">       dp_4[0]&#x3D;1;</span><br><span class="line">       dp_4[1]&#x3D;2;</span><br><span class="line">       dp_4[2]&#x3D;4;</span><br><span class="line">       dp_4[3]&#x3D;8;</span><br><span class="line">       i_4&#x3D;4;</span><br><span class="line">       char[] chars&#x3D;str.toCharArray();</span><br><span class="line">       int l&#x3D;0,r&#x3D;0,res&#x3D;1;</span><br><span class="line">       while(r&lt;chars.length)&#123;</span><br><span class="line">       	&#x2F;&#x2F;寻找相同字符的区间</span><br><span class="line">           while(r&lt;chars.length&amp;&amp;chars[r]&#x3D;&#x3D;chars[l]) r++;</span><br><span class="line">           int num&#x3D;getDp(r-1-l,chars[l]&#x3D;&#x3D;&#39;7&#39;||chars[l]&#x3D;&#x3D;&#39;9&#39;?1:0);</span><br><span class="line">           res*&#x3D;num;</span><br><span class="line">           l&#x3D;r;</span><br><span class="line">       &#125;</span><br><span class="line">       return res;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;mode用来表示哪种情况，如果是&#39;7&#39;或&#39;9&#39;则为1</span><br><span class="line">   private static int getDp(int len,int mode)&#123;</span><br><span class="line">       if(mode&#x3D;&#x3D;0)&#123;</span><br><span class="line">           if(dp_3[len]!&#x3D;0) return dp_3[len];</span><br><span class="line">           &#x2F;&#x2F;从i_3开始向后算</span><br><span class="line">           for(int i&#x3D;i_3;i&lt;&#x3D;len;i++)&#123;</span><br><span class="line">               dp_3[i]&#x3D;dp_3[i-1]+dp_3[i-2]+dp_3[i-3];</span><br><span class="line">           &#125;</span><br><span class="line">           i_3&#x3D;len;</span><br><span class="line">           return dp_3[len];</span><br><span class="line">       &#125;else &#123;</span><br><span class="line">           if(dp_4[len]!&#x3D;0) return dp_4[len];</span><br><span class="line">           for(int i&#x3D;i_4;i&lt;&#x3D;len;i++)&#123;</span><br><span class="line">               dp_4[i]&#x3D;dp_4[i-1]+dp_4[i-2]+dp_4[i-3]+dp_4[i-4];</span><br><span class="line">           &#125;</span><br><span class="line">           i_4&#x3D;len;</span><br><span class="line">           return dp_4[len];</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://extrali.com/2020/06/26/%E8%93%84%E6%B0%B4%E6%B1%A0%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="黎达">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Extrali">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/26/%E8%93%84%E6%B0%B4%E6%B1%A0%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">蓄水池算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              

              <time title="Erstellt: 2020-06-26 09:35:04 / Geändert am: 09:58:19" itemprop="dateCreated datePublished" datetime="2020-06-26T09:35:04+08:00">2020-06-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">in</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="引论"><a href="#引论" class="headerlink" title="引论"></a>引论</h1><p>对于例如<strong>从100000 份调查报告中抽取 1000 份进行统计</strong>的抽样问题，如果要保持概论均等，那么只需要从<code>random.nextInt(100000)+1</code>中选出1000个即可，并且保持不重复。</p>
<blockquote>
<p>但是问题在于很多时候不知道总共有多少份报告，所以需要用到一下的蓄水池算法。</p>
</blockquote>
<h1 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h1><p>算法的过程：</p>
<ol>
<li><p>假设数据序列的规模为 n，需要采样的数量的为 k。</p>
</li>
<li><p>首先构建一个可容纳 k 个元素的数组，将序列的前 k 个元素放入数组中。</p>
</li>
<li><p>然后从第 k+1 个元素开始，以 k/n 的概率来决定该元素最后是否被留在数组中（每进来一个新的元素，数组中的每个旧元素被替换的概率是相同的）。 当遍历完所有元素之后，数组中剩下的元素即为所需采取的样本。</p>
</li>
</ol>
<h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public static int[] sampling(int[] nums,int K)&#123;</span><br><span class="line">        int[] pooling&#x3D;new int[K];</span><br><span class="line">        for(int i&#x3D;0;i&lt;K;i++)&#123;</span><br><span class="line">            pooling[i]&#x3D;nums[i];</span><br><span class="line">        &#125;</span><br><span class="line">        Random random&#x3D;new Random();</span><br><span class="line">        for(int i&#x3D;K;i&lt;nums.length;i++)&#123;</span><br><span class="line">            int index&#x3D;random.nextInt(i+1);</span><br><span class="line">            if(index&lt;K)&#123;</span><br><span class="line">                pooling[index]&#x3D;nums[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return pooling;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h1 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h1><p>分两步证明：</p>
<ol>
<li>对于前k个元素，最终被留下来的概率是<code>1*(1-1/(k+1))*(1-1/(k+2))*(1-1/(k+3))*.....*1/n=k/n</code>，其中首先前k个元素被选中的概率为1，然后不被第k+1个元素替换的概率为1-1/(k+1)，然后同理得出不被第k+2个元素替换，…….，相乘即可得出结果。</li>
<li>对于从第k+1个元素向后的任何一个元素M，首先其被选中的概率为k/M，然后不被第M+1替换，不被M+2替换，有<code>k/M*(1-1/(M+1))*(1-1/(M+2))*......*(1-1/n)=k/n</code></li>
<li>所以对于任何一个元素都有k/n的概率被选择</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://extrali.com/2020/06/18/Dubbo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="黎达">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Extrali">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/Dubbo/" class="post-title-link" itemprop="url">Dubbo</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              

              <time title="Erstellt: 2020-06-18 15:34:08 / Geändert am: 15:52:32" itemprop="dateCreated datePublished" datetime="2020-06-18T15:34:08+08:00">2020-06-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">in</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Dubbo/" itemprop="url" rel="index"><span itemprop="name">Dubbo</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="一重要的概念"><a href="#一重要的概念" class="headerlink" title="一重要的概念"></a>一重要的概念</h1><h2 id="1-1-什么是Dubbo"><a href="#1-1-什么是Dubbo" class="headerlink" title="1.1 什么是Dubbo"></a>1.1 什么是Dubbo</h2><p>Apache Dubbo (incubating) |ˈdʌbəʊ| 是一款高性能、轻量级的开源Java <strong>RPC 框架</strong>，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。简单来说 Dubbo 是一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案。</p>
<h2 id="1-2-什么是RPC？RPC原理是什么？"><a href="#1-2-什么是RPC？RPC原理是什么？" class="headerlink" title="1.2 什么是RPC？RPC原理是什么？"></a>1.2 什么是RPC？RPC原理是什么？</h2><p><strong>什么是 RPC？</strong></p>
<p>RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。<strong>比如两个不同的服务 A、B 部署在两台不同的机器上，那么服务 A 如果想要调用服务 B 中的某个方法</strong>该怎么办呢？使用 HTTP请求当然可以，但是可能会比较麻烦。 RPC 的出现就是为了让你调用远程方法像调用本地方法一样简单。</p>
<p><strong>RPC原理是什么？</strong></p>
<p>我这里这是简单的提一下。详细内容可以查看下面这篇文章：</p>
<p><img src="http://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-12-6/37345851.jpg" alt="RPC原理图"></p>
<ol>
<li>服务消费方（client）调用以本地调用方式调用服务；</li>
<li>client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；</li>
<li>client stub找到服务地址，并将消息发送到服务端；</li>
<li>server stub收到消息后进行解码；</li>
<li>server stub根据解码结果调用本地的服务；</li>
<li>本地服务执行并将结果返回给server stub；</li>
<li>server stub将返回结果打包成消息并发送至消费方；</li>
<li>client stub接收到消息，并进行解码；</li>
<li>服务消费方得到最终结果。</li>
</ol>
<p>下面再贴一个网上的时序图：</p>
<p><img src="http://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-12-6/32527396.jpg" alt="RPC原理时序图"></p>
<p><strong>说了这么多，我们为什么要用 Dubbo 呢？</strong></p>
<h2 id="1-3-为什么要用Dubbo"><a href="#1-3-为什么要用Dubbo" class="headerlink" title="1.3 为什么要用Dubbo"></a>1.3 为什么要用Dubbo</h2><p>Dubbo 的诞生和 SOA 分布式架构的流行有着莫大的关系。SOA 面向服务的架构（Service Oriented Architecture），也就是把工程按照业务逻辑拆分成服务层、表现层两个工程。服务层中包含业务逻辑，只需要对外提供服务即可。表现层只需要处理和页面的交互，业务逻辑都是调用服务层的服务来实现。SOA架构中有两个主要角色：服务提供者（Provider）和服务使用者（Consumer）。</p>
<p><img src="http://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-9-26/43050183.jpg" alt="为什么要用 Dubbo"></p>
<p><strong>如果你要开发分布式程序，你也可以直接基于 HTTP 接口进行通信，但是为什么要用 Dubbo呢？</strong></p>
<p>我觉得主要可以从 Dubbo 提供的下面四点特性来说为什么要用 Dubbo：</p>
<ol>
<li><strong>负载均衡</strong>——同一个服务部署在不同的机器时该调用那一台机器上的服务。</li>
<li><strong>服务调用链路生成</strong>——随着系统的发展，服务越来越多，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。Dubbo 可以为我们解决服务之间互相是如何调用的。</li>
<li><strong>服务访问压力以及时长统计、资源调度和治理</strong>——基于访问压力实时管理集群容量，提高集群利用率。</li>
<li><strong>服务降级</strong>——某个服务挂掉之后调用备用服务。</li>
</ol>
<p>另外，Dubbo 除了能够应用在分布式系统中，也可以应用在现在比较火的微服务系统中。不过，由于 Spring Cloud 在微服务中应用更加广泛，所以，我觉得一般我们提 Dubbo 的话，大部分是分布式系统的情况。</p>
<p><strong>我们刚刚提到了分布式这个概念，下面再给大家介绍一下什么是分布式？为什么要分布式？</strong></p>
<h2 id="1-4-什么是分布式？"><a href="#1-4-什么是分布式？" class="headerlink" title="1.4 什么是分布式？"></a>1.4 什么是分布式？</h2><p>分布式或者说 SOA 分布式重要的就是面向服务，说简单的分布式就是我们<strong>把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能</strong>。比如电商系统可以简单地拆分成订单系统、商品系统、登录系统等等，拆分之后的每个服务可以部署在不同的机器上，如果某一个服务的访问量比较大的话也可以将这个服务同时部署在多台机器上。</p>
<h2 id="1-5-为什么要分布式？"><a href="#1-5-为什么要分布式？" class="headerlink" title="1.5 为什么要分布式？"></a>1.5 为什么要分布式？</h2><p>从开发角度来讲单体应用的代码都集中在一起，而分布式系统的代码根据业务被拆分。所以，每个团队可以负责一个服务的开发，这样提升了开发效率。另外，代码根据业务拆分之后更加便于维护和扩展。</p>
<p>另外，我觉得将系统拆分成分布式之后不光便于系统扩展和维护，更能提高整个系统的性能。你想一想嘛？把整个系统拆分成不同的服务/系统，然后每个服务/系统 单独部署在一台服务器上，是不是很大程度上提高了系统性能呢？</p>
<h1 id="二Dubbo的架构"><a href="#二Dubbo的架构" class="headerlink" title="二Dubbo的架构"></a>二Dubbo的架构</h1><h2 id="2-1-Dubbo的架构图解"><a href="#2-1-Dubbo的架构图解" class="headerlink" title="2.1 Dubbo的架构图解"></a>2.1 Dubbo的架构图解</h2><p><img src="http://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-9-26/46816446.jpg" alt="Dubbo 架构"></p>
<p><strong>上述节点简单说明：</strong></p>
<ul>
<li><strong>Provider：</strong> 暴露服务的服务提供方</li>
<li><strong>Consumer：</strong> 调用远程服务的服务消费方</li>
<li><strong>Registry：</strong> 服务注册与发现的注册中心</li>
<li><strong>Monitor：</strong> 统计服务的调用次数和调用时间的监控中心</li>
<li><strong>Container：</strong> 服务运行容器</li>
</ul>
<p><strong>调用关系说明：</strong></p>
<ol>
<li>服务容器负责启动，加载，运行服务提供者。</li>
<li>服务提供者在启动时，向注册中心注册自己提供的服务。</li>
<li>服务消费者在启动时，向注册中心订阅自己所需的服务。</li>
<li>注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。</li>
<li>服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。</li>
<li>服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。</li>
</ol>
<p><strong>重要知识点总结：</strong></p>
<ul>
<li><strong>注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小</strong></li>
<li><strong>监控中心负责统计各服务调用次数，调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以报表展示</strong></li>
<li><strong>注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外</strong></li>
<li><strong>注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者</strong></li>
<li><strong>注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表</strong></li>
<li><strong>注册中心和监控中心都是可选的，服务消费者可以直连服务提供者</strong></li>
<li><strong>服务提供者无状态，任意一台宕掉后，不影响使用</strong></li>
<li><strong>服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复</strong></li>
</ul>
<h1 id="三Dubbo的负载均衡策略"><a href="#三Dubbo的负载均衡策略" class="headerlink" title="三Dubbo的负载均衡策略"></a>三Dubbo的负载均衡策略</h1><h2 id="3-1-什么是负载均衡"><a href="#3-1-什么是负载均衡" class="headerlink" title="3.1 什么是负载均衡"></a>3.1 什么是负载均衡</h2><p>比如我们的系统中的某个服务的访问量特别大，我们将这个服务部署在了多台服务器上，当客户端发起请求的时候，多台服务器都可以处理这个请求。那么，如何正确选择处理该请求的服务器就很关键。假如，你就要一台服务器来处理该服务的请求，那该服务部署在多台服务器的意义就不复存在了。负载均衡就是为了避免单个服务器响应同一请求，容易造成服务器宕机、崩溃等问题，我们从负载均衡的这四个字就能明显感受到它的意义。</p>
<h2 id="3-2-Dubbo提供的负载均衡策略"><a href="#3-2-Dubbo提供的负载均衡策略" class="headerlink" title="3.2 Dubbo提供的负载均衡策略"></a>3.2 Dubbo提供的负载均衡策略</h2><p>在集群负载均衡时，Dubbo 提供了多种均衡策略，默认为 <code>random</code> 随机调用。</p>
<h3 id="3-2-1-Random-LoadBalance"><a href="#3-2-1-Random-LoadBalance" class="headerlink" title="3.2.1 Random LoadBalance"></a>3.2.1 Random LoadBalance</h3><ul>
<li><strong>随机，按权重设置随机概率。</strong></li>
<li>在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。</li>
</ul>
<p><img src="http://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-12-7/77722327.jpg" alt="基于权重的随机负载均衡机制"></p>
<h3 id="3-2-2-RoundRobin-LoadBalance"><a href="#3-2-2-RoundRobin-LoadBalance" class="headerlink" title="3.2.2 RoundRobin LoadBalance"></a>3.2.2 RoundRobin LoadBalance</h3><ul>
<li>轮循，按公约后的权重设置轮循比率。</li>
<li>存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。</li>
</ul>
<p><img src="http://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-12-7/97933247.jpg" alt="基于权重的轮询负载均衡机制"></p>
<h3 id="3-2-3-LeastActive-LoadBalance"><a href="#3-2-3-LeastActive-LoadBalance" class="headerlink" title="3.2.3 LeastActive LoadBalance"></a>3.2.3 LeastActive LoadBalance</h3><ul>
<li>最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。</li>
<li>使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。</li>
</ul>
<h3 id="3-2-4-ConsistentHash-LoadBalance"><a href="#3-2-4-ConsistentHash-LoadBalance" class="headerlink" title="3.2.4 ConsistentHash LoadBalance"></a>3.2.4 ConsistentHash LoadBalance</h3><ul>
<li><strong>一致性 Hash，相同参数的请求总是发到同一提供者。(如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性hash策略。)</strong></li>
<li>当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Nächste Seite"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Inhaltsverzeichnis
        </li>
        <li class="sidebar-nav-overview">
          Übersicht
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">黎达</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">42</span>
          <span class="site-state-item-name">Artikel</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">Kategorien</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">35</span>
        <span class="site-state-item-name">schlagwörter</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">黎达</span>
</div>
  <div class="powered-by">Erstellt mit  <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
